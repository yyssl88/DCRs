#!/usr/bin/env python3
"""
å¤šæ•°æ®é›†ç²¾ç¡®å™ªéŸ³æ§åˆ¶è„šæœ¬
æ”¯æŒ Amazonã€FakeDDITã€Goodreads ä¸‰ä¸ªæ•°æ®é›†
åŒ…å«å›¾ç‰‡å¯¹åº”æ£€æŸ¥å’ŒéªŒè¯åŠŸèƒ½
"""

import os
import random
import pandas as pd
import numpy as np
import shutil
import json
from PIL import Image

# ================== é…ç½®åŒº ==================
SEED = 42
NOISE_RATIO = 0.1  # 10%å™ªéŸ³æ¯”ä¾‹ï¼ˆæŒ‰è¡Œæ•°æ¯”ä¾‹ï¼‰
NOISE_CELLS = 100  # ç²¾ç¡®æ§åˆ¶å™ªéŸ³å•å…ƒæ ¼æ•°é‡ï¼ˆä¼˜å…ˆçº§é«˜äºNOISE_RATIOï¼‰
USE_EXACT_CELL_COUNT = True  # æ˜¯å¦ä½¿ç”¨ç²¾ç¡®å•å…ƒæ ¼æ•°é‡æ§åˆ¶

# å¤šæ•°æ®é›†é…ç½®
DATASET_CONFIG = {
    'amazon': {
        'data_dir': '/data_nas/DCR/data_her/data/amazon',
        'relation_file': 'amazon_com_best_sellers_2025_01_27.csv',
        'img_dir': 'imgs',
        'sep': ',',
        'output_dir': '/data_nas/DCR/split_addnoise/amazon_test',
        'best_img_dict': 'best_img_dict.json',
        'her_map': 'amazon_her_map.json',
    },
    'fakeddit': {
        'data_dir': '/data_nas/DCR/data_her/data/fakeddit',
        'relation_file': 'all_train.tsv',
        'img_dir': 'imgs',
        'sep': '\t',
        'output_dir': '/data_nas/DCR/split_addnoise/fakeddit_test',
        'best_img_dict': 'best_img_dict.json',
        'her_map': 'fakeddit_her_map.json',
    },
    'goodreads': {
        'data_dir': '/data_nas/DCR/data_her/data/goodreads',
        'relation_file': 'GoodReads_100k_books.csv',
        'img_dir': 'imgs',
        'sep': ',',
        'output_dir': '/data_nas/DCR/split_addnoise/goodreads_test',
        'best_img_dict': 'best_img_dict.json',
        'her_map': 'goodreads_her_map.json',
    },
    'ml25m': {
        'data_dir': '/data_nas/DCR/data_her/data/ml25m',
        'relation_file': 'movie_wide_table.csv',
        'img_dir': 'covers/covers/',
        'sep': ',',
        'output_dir': '/data_nas/DCR/split_addnoise/ml25m_test',
        'best_img_dict': 'best_img_dict.json',
        'her_map': 'ml25m_her_map.json',
    },
}

# ================ è¾…åŠ©å‡½æ•° =================
def set_seed(seed):
    random.seed(seed)
    np.random.seed(seed)

def split_df(df, ratios=(0.8, 0.1, 0.1), seed=42):
    """æ‹†åˆ†æ•°æ®é›†ä¸ºè®­ç»ƒã€éªŒè¯ã€æµ‹è¯•é›†"""
    idx = np.arange(len(df))
    np.random.shuffle(idx)
    n = len(df)
    n_train = int(n * ratios[0])
    n_valid = int(n * ratios[1])
    train_idx = idx[:n_train]
    valid_idx = idx[n_train:n_train + n_valid]
    test_idx = idx[n_train + n_valid:]
    return df.iloc[train_idx], df.iloc[valid_idx], df.iloc[test_idx]

def add_precise_noise(df, noise_ratio=0.1, noise_cells=None, use_exact_cell_count=False, seed=42):
    """
    ç²¾ç¡®æ§åˆ¶å™ªéŸ³æ·»åŠ ï¼šæ”¯æŒæŒ‰è¡Œæ•°æ¯”ä¾‹æˆ–ç²¾ç¡®å•å…ƒæ ¼æ•°é‡æ·»åŠ å™ªéŸ³
    æ’é™¤img_pathåˆ—å’Œæ–‡æœ¬æè¿°åˆ—
    
    Args:
        df: è¾“å…¥æ•°æ®æ¡†
        noise_ratio: æŒ‰è¡Œæ•°æ¯”ä¾‹æ·»åŠ å™ªéŸ³ï¼ˆå½“use_exact_cell_count=Falseæ—¶ä½¿ç”¨ï¼‰
        noise_cells: ç²¾ç¡®çš„å™ªéŸ³å•å…ƒæ ¼æ•°é‡ï¼ˆå½“use_exact_cell_count=Trueæ—¶ä½¿ç”¨ï¼‰
        use_exact_cell_count: æ˜¯å¦ä½¿ç”¨ç²¾ç¡®å•å…ƒæ ¼æ•°é‡æ§åˆ¶
        seed: éšæœºç§å­
    """
    noisy_df = df.copy()
    n = len(df)
    
    print(f"ğŸ“Š æ•°æ®é›†ä¿¡æ¯:")
    print(f"  - æ€»è¡Œæ•°: {n}")
    print(f"  - æ€»åˆ—æ•°: {len(df.columns)}")
    
    if use_exact_cell_count and noise_cells is not None:
        print(f"  - å™ªéŸ³æ§åˆ¶æ¨¡å¼: ç²¾ç¡®å•å…ƒæ ¼æ•°é‡")
        print(f"  - ç›®æ ‡å™ªéŸ³å•å…ƒæ ¼æ•°: {noise_cells}")
        print(f"  - å™ªéŸ³ç±»å‹: ä»…æšä¸¾å‹åˆ—")
    else:
        n_noise_rows = max(1, int(n * noise_ratio))  # è‡³å°‘1è¡Œ
        print(f"  - å™ªéŸ³æ§åˆ¶æ¨¡å¼: æŒ‰è¡Œæ•°æ¯”ä¾‹")
        print(f"  - å™ªéŸ³è¡Œæ•°: {n_noise_rows} ({noise_ratio*100:.1f}%)")
        print(f"  - å™ªéŸ³ç±»å‹: ä»…æšä¸¾å‹åˆ—")
    
    # å®šä¹‰éœ€è¦æ’é™¤çš„åˆ—
    excluded_cols = {
        'img_path',  # å›¾ç‰‡è·¯å¾„åˆ—
        'description', 'descriptionRaw', 'name', 'title',  # æ–‡æœ¬æè¿°åˆ—
        'features', 'breadcrumbs', 'additionalProperties',  # è¯¦ç»†æè¿°åˆ—
        'url', 'imageUrls', 'variants',  # URLå’Œå›¾ç‰‡ç›¸å…³åˆ—
        'scrapedDate', 'new_path', 'nodeName',  # å…ƒæ•°æ®åˆ—
        'gtin', 'mpn', 'sku', 'style'  # äº§å“æ ‡è¯†åˆ—
    }
    
    # è¯†åˆ«æ•°å€¼å‹å’Œæšä¸¾ç±»å‹åˆ—ï¼ˆæ’é™¤æŒ‡å®šåˆ—ï¼‰
    numeric_cols = []
    enum_cols = []
    
    for col in noisy_df.columns:
        # è·³è¿‡æ’é™¤çš„åˆ—
        if col in excluded_cols:
            continue
            
        if pd.api.types.is_numeric_dtype(noisy_df[col]):
            numeric_cols.append(col)
        elif noisy_df[col].dtype == 'object':
            # æ£€æŸ¥æ˜¯å¦ä¸ºæšä¸¾ç±»å‹ï¼ˆå”¯ä¸€å€¼æ•°é‡ç›¸å¯¹è¾ƒå°‘ï¼‰
            unique_ratio = noisy_df[col].nunique() / len(noisy_df[col].dropna())
            if unique_ratio < 0.5 and noisy_df[col].nunique() > 1:  # å”¯ä¸€å€¼æ¯”ä¾‹å°äº50%ä¸”å¤šäº1ä¸ª
                enum_cols.append(col)
    
    print(f"ğŸ”§ åˆ—ç±»å‹è¯†åˆ«:")
    print(f"  - æ’é™¤çš„åˆ—: {sorted(excluded_cols.intersection(set(df.columns)))}")
    print(f"  - æ•°å€¼å‹åˆ— ({len(numeric_cols)}): {numeric_cols} (ä¸æ·»åŠ å™ªéŸ³)")
    print(f"  - æšä¸¾å‹åˆ— ({len(enum_cols)}): {enum_cols} (æ·»åŠ å™ªéŸ³)")
    print(f"  - å…¶ä»–åˆ— ({len(df.columns) - len(numeric_cols) - len(enum_cols) - len(excluded_cols.intersection(set(df.columns)))}): {[col for col in df.columns if col not in numeric_cols and col not in enum_cols and col not in excluded_cols]}")
    
    # è®¾ç½®éšæœºç§å­
    np.random.seed(seed)
    random.seed(seed)
    
    # è®°å½•ä¿®æ”¹ç»Ÿè®¡
    modification_stats = {
        'selected_rows': set(),
        'modified_rows': set(),
        'total_changes': 0,
        'changes_by_column': {}
    }
    
    if use_exact_cell_count and noise_cells is not None:
        # ç²¾ç¡®å•å…ƒæ ¼æ•°é‡æ¨¡å¼
        print(f"ğŸ”§ ç²¾ç¡®å•å…ƒæ ¼æ•°é‡æ¨¡å¼: ç›®æ ‡ {noise_cells} ä¸ªå•å…ƒæ ¼")
        
        # åˆ›å»ºæ‰€æœ‰å¯ä¿®æ”¹çš„å•å…ƒæ ¼åˆ—è¡¨ï¼ˆåªåŒ…å«æšä¸¾å‹åˆ—ï¼‰
        modifiable_cells = []
        for row_idx in noisy_df.index:
            for col in enum_cols:  # åªå¯¹æšä¸¾å‹åˆ—æ·»åŠ å™ªéŸ³
                if pd.notna(noisy_df.at[row_idx, col]):
                    modifiable_cells.append((row_idx, col))
        
        print(f"ğŸ“Š å¯ä¿®æ”¹çš„å•å…ƒæ ¼æ€»æ•°: {len(modifiable_cells)}")
        
        if len(modifiable_cells) < noise_cells:
            print(f"âš ï¸  å¯ä¿®æ”¹å•å…ƒæ ¼æ•°é‡ ({len(modifiable_cells)}) å°‘äºç›®æ ‡æ•°é‡ ({noise_cells})")
            noise_cells = len(modifiable_cells)
            print(f"ğŸ“Š è°ƒæ•´ä¸ºå®é™…å¯ä¿®æ”¹æ•°é‡: {noise_cells}")
        
        # éšæœºé€‰æ‹©è¦ä¿®æ”¹çš„å•å…ƒæ ¼
        selected_cells = random.sample(modifiable_cells, noise_cells)
        print(f"ğŸ“Š é€‰ä¸­çš„å•å…ƒæ ¼æ•°é‡: {len(selected_cells)}")
        
        # å¯¹é€‰ä¸­çš„å•å…ƒæ ¼æ·»åŠ å™ªéŸ³ï¼ˆåªå¤„ç†æšä¸¾å‹åˆ—ï¼‰
        for i, (row_idx, col) in enumerate(selected_cells):
            if i % 10 == 0:  # æ¯10ä¸ªå•å…ƒæ ¼æ‰“å°ä¸€æ¬¡è¿›åº¦
                print(f"ğŸ”§ å¤„ç†è¿›åº¦: {i+1}/{len(selected_cells)}")
            
            original_val = noisy_df.at[row_idx, col]
            
            # åªå¤„ç†æšä¸¾å‹åˆ—
            if col in enum_cols:
                # æšä¸¾å‹åˆ—å¤„ç†
                # è·å–è¯¥åˆ—çš„æ‰€æœ‰å”¯ä¸€å€¼
                unique_values = noisy_df[col].dropna().unique().tolist()
                
                if len(unique_values) > 1:
                    # éšæœºé€‰æ‹©å…¶ä»–å€¼
                    other_values = [v for v in unique_values if v != original_val]
                    if other_values:
                        new_val = random.choice(other_values)
                        noisy_df.at[row_idx, col] = new_val
                        print(f"  ğŸ“Š æšä¸¾åˆ— {col}: {original_val} -> {new_val}")
                    else:
                        continue  # å¦‚æœæ²¡æœ‰å…¶ä»–å€¼å¯é€‰ï¼Œè·³è¿‡
                else:
                    continue  # å¦‚æœåªæœ‰ä¸€ä¸ªå”¯ä¸€å€¼ï¼Œè·³è¿‡
            else:
                # å¦‚æœä¸æ˜¯æšä¸¾å‹åˆ—ï¼Œè·³è¿‡
                continue
            
            # è®°å½•ç»Ÿè®¡ä¿¡æ¯
            modification_stats['selected_rows'].add(row_idx)
            modification_stats['modified_rows'].add(row_idx)
            modification_stats['total_changes'] += 1
            
            if col not in modification_stats['changes_by_column']:
                modification_stats['changes_by_column'][col] = 0
            modification_stats['changes_by_column'][col] += 1
        
        print(f"ğŸ”§ å¤„ç†å®Œæˆ: {len(selected_cells)} ä¸ªå•å…ƒæ ¼")
        
    else:
        # æŒ‰è¡Œæ•°æ¯”ä¾‹æ¨¡å¼ï¼ˆåªå¯¹æšä¸¾å‹åˆ—æ·»åŠ å™ªéŸ³ï¼‰
        n_noise_rows = max(1, int(n * noise_ratio))  # è‡³å°‘1è¡Œ
        noise_row_indices = np.random.choice(noisy_df.index, n_noise_rows, replace=False)
        print(f"ğŸ“Š é€‰ä¸­çš„è¡Œç´¢å¼•: {sorted(noise_row_indices.tolist())}")
        
        modification_stats['selected_rows'] = set(noise_row_indices)
        
        # å¯¹é€‰ä¸­çš„è¡Œæ·»åŠ å™ªéŸ³ï¼ˆåªå¯¹æšä¸¾å‹åˆ—ï¼‰
        for row_idx in noise_row_indices:
            print(f"\nğŸ”§ å¤„ç†è¡Œ {row_idx}:")
            row_changes = 0
            
            # åªå¯¹æšä¸¾å‹åˆ—æ·»åŠ å™ªéŸ³
            for col in enum_cols:
                if pd.notna(noisy_df.at[row_idx, col]):
                    # è·å–è¯¥åˆ—çš„æ‰€æœ‰å”¯ä¸€å€¼
                    unique_values = noisy_df[col].dropna().unique().tolist()
                    
                    if len(unique_values) > 1:
                        original_val = noisy_df.at[row_idx, col]
                        
                        # éšæœºé€‰æ‹©å…¶ä»–å€¼
                        other_values = [v for v in unique_values if v != original_val]
                        if other_values:
                            new_val = random.choice(other_values)
                            noisy_df.at[row_idx, col] = new_val
                            print(f"  ğŸ“Š æšä¸¾åˆ— {col}: {original_val} -> {new_val}")
                            row_changes += 1
                            
                            # è®°å½•åˆ—å˜åŒ–ç»Ÿè®¡
                            if col not in modification_stats['changes_by_column']:
                                modification_stats['changes_by_column'][col] = 0
                            modification_stats['changes_by_column'][col] += 1
            
            if row_changes > 0:
                modification_stats['modified_rows'].add(row_idx)
                modification_stats['total_changes'] += row_changes
                print(f"  ğŸ“ˆ è¡Œ {row_idx} æ€»è®¡ä¿®æ”¹: {row_changes} ä¸ªå•å…ƒæ ¼")
    
    # è¯¦ç»†ç»Ÿè®¡æŠ¥å‘Š
    print(f"\nğŸ“Š è¯¦ç»†ç»Ÿè®¡æŠ¥å‘Š:")
    print(f"  - é€‰ä¸­çš„è¡Œæ•°: {len(modification_stats['selected_rows'])}")
    print(f"  - å®é™…ä¿®æ”¹çš„è¡Œæ•°: {len(modification_stats['modified_rows'])}")
    print(f"  - æ€»ä¿®æ”¹å•å…ƒæ ¼æ•°: {modification_stats['total_changes']}")
    
    if len(modification_stats['selected_rows']) > 0:
        print(f"  - å¹³å‡æ¯è¡Œä¿®æ”¹: {modification_stats['total_changes']/len(modification_stats['selected_rows']):.1f} ä¸ªå•å…ƒæ ¼")
    
    print(f"\nğŸ“Š å„åˆ—ä¿®æ”¹ç»Ÿè®¡:")
    for col, changes in modification_stats['changes_by_column'].items():
        print(f"  - {col}: {changes} ä¸ªå˜åŒ–")
    
    # éªŒè¯ä¿®æ”¹æ¯”ä¾‹
    total_cells = len(df) * len(df.columns)
    change_ratio = modification_stats['total_changes'] / total_cells * 100
    print(f"\nğŸ“Š æ€»ä½“ä¿®æ”¹æ¯”ä¾‹:")
    print(f"  - ä¿®æ”¹å•å…ƒæ ¼æ¯”ä¾‹: {change_ratio:.2f}%")
    print(f"  - ä¿®æ”¹è¡Œæ¯”ä¾‹: {len(modification_stats['modified_rows'])/n*100:.1f}%")
    
    if use_exact_cell_count and noise_cells is not None:
        print(f"  - ç›®æ ‡å•å…ƒæ ¼æ•°: {noise_cells}")
        print(f"  - å®é™…ä¿®æ”¹å•å…ƒæ ¼æ•°: {modification_stats['total_changes']}")
        if modification_stats['total_changes'] == noise_cells:
            print(f"  âœ… ç²¾ç¡®è¾¾åˆ°ç›®æ ‡å•å…ƒæ ¼æ•°é‡")
        else:
            print(f"  âš ï¸  å®é™…ä¿®æ”¹æ•°é‡ä¸ç›®æ ‡æ•°é‡ä¸ç¬¦")
    
    return noisy_df

def load_best_img_dict(cfg):
    """åŠ è½½best_img_dict.jsonæ–‡ä»¶"""
    best_img_path = os.path.join(cfg['data_dir'], cfg['best_img_dict'])
    if not os.path.exists(best_img_path):
        print(f"âŒ best_img_dictæ–‡ä»¶ä¸å­˜åœ¨: {best_img_path}")
        return None
    
    try:
        with open(best_img_path, 'r', encoding='utf-8') as f:
            best_img_dict = json.load(f)
        print(f"âœ… æˆåŠŸåŠ è½½best_img_dict: {len(best_img_dict)} ä¸ªæœ‰æ•ˆæ ·æœ¬")
        return best_img_dict
    except Exception as e:
        print(f"âŒ åŠ è½½best_img_dictå¤±è´¥: {e}")
        return None

def filter_data_by_best_img(df, best_img_dict):
    """æ ¹æ®best_img_dictè¿‡æ»¤æ•°æ®å¹¶æ·»åŠ å›¾ç‰‡ä¿¡æ¯"""
    if best_img_dict is None:
        return df
    
    # è·å–æœ‰æ•ˆçš„è¡Œå·ï¼ˆbest_img_dictçš„keyï¼‰
    valid_indices = list(best_img_dict.keys())
    # è½¬æ¢ä¸ºæ•´æ•°ç´¢å¼•
    valid_indices = [int(idx) for idx in valid_indices]
    
    # è¿‡æ»¤æ‰è¶…å‡ºæ•°æ®èŒƒå›´çš„ç´¢å¼•
    max_valid_index = len(df) - 1
    valid_indices = [idx for idx in valid_indices if 0 <= idx <= max_valid_index]
    
    if len(valid_indices) == 0:
        print(f"âš ï¸  æ‰€æœ‰ç´¢å¼•éƒ½è¶…å‡ºæ•°æ®èŒƒå›´ï¼Œä½¿ç”¨åŸå§‹æ•°æ®")
        return df
    
    print(f"ğŸ“Š ç´¢å¼•è¿‡æ»¤: åŸå§‹ç´¢å¼•æ•° {len(list(best_img_dict.keys()))}, æœ‰æ•ˆç´¢å¼•æ•° {len(valid_indices)}")
    
    # è¿‡æ»¤æ•°æ®
    filtered_df = df.iloc[valid_indices].copy()
    
    # æ·»åŠ å›¾ç‰‡è·¯å¾„ä¿¡æ¯ï¼ˆåªå¯¹åº”æœ‰æ•ˆçš„ç´¢å¼•ï¼‰
    img_paths = [best_img_dict[str(idx)] for idx in valid_indices]
    filtered_df['img_path'] = img_paths
    
    print(f"ğŸ“Š è¿‡æ»¤åæ ·æœ¬æ•°: {len(filtered_df)} (åŸå§‹: {len(df)})")
    return filtered_df

def check_image_correspondence(df, cfg):
    """æ£€æŸ¥å›¾ç‰‡å¯¹åº”æ˜¯å¦æ­£ç¡®"""
    print("ğŸ” æ£€æŸ¥å›¾ç‰‡å¯¹åº”å…³ç³»...")
    
    if 'img_path' not in df.columns:
        print("âš ï¸  æ•°æ®ä¸­æ²¡æœ‰img_pathåˆ—")
        return False
    
    img_dir = os.path.join(cfg['data_dir'], cfg['img_dir'])
    if not os.path.exists(img_dir):
        print(f"âŒ å›¾ç‰‡ç›®å½•ä¸å­˜åœ¨: {img_dir}")
        return False
    
    # ç»Ÿè®¡ä¿¡æ¯
    total_images = len(df)
    existing_images = 0
    missing_images = 0
    corrupted_images = 0
    valid_images = 0
    
    print(f"ğŸ“ å›¾ç‰‡ç›®å½•: {img_dir}")
    print(f"ğŸ“Š æ£€æŸ¥ {total_images} ä¸ªå›¾ç‰‡è·¯å¾„...")
    
    for idx, row in df.iterrows():
        img_path = row['img_path']
        if pd.isna(img_path):
            missing_images += 1
            continue
        
        # æ„å»ºå®Œæ•´è·¯å¾„
        full_img_path = os.path.join(img_dir, img_path)
        
        if not os.path.exists(full_img_path):
            missing_images += 1
            print(f"  âŒ å›¾ç‰‡ä¸å­˜åœ¨: {img_path}")
            continue
        
        existing_images += 1
        
        # å°è¯•æ‰“å¼€å›¾ç‰‡éªŒè¯å®Œæ•´æ€§
        try:
            with Image.open(full_img_path) as img:
                img.verify()  # éªŒè¯å›¾ç‰‡å®Œæ•´æ€§
            valid_images += 1
        except Exception as e:
            corrupted_images += 1
            print(f"  âš ï¸  å›¾ç‰‡æŸå: {img_path} - {e}")
    
    # è¾“å‡ºç»Ÿè®¡ç»“æœ
    print(f"\nğŸ“Š å›¾ç‰‡å¯¹åº”æ£€æŸ¥ç»“æœ:")
    print(f"  - æ€»å›¾ç‰‡æ•°: {total_images}")
    print(f"  - å­˜åœ¨å›¾ç‰‡: {existing_images} ({existing_images/total_images*100:.1f}%)")
    print(f"  - ç¼ºå¤±å›¾ç‰‡: {missing_images} ({missing_images/total_images*100:.1f}%)")
    print(f"  - æŸåå›¾ç‰‡: {corrupted_images} ({corrupted_images/total_images*100:.1f}%)")
    print(f"  - æœ‰æ•ˆå›¾ç‰‡: {valid_images} ({valid_images/total_images*100:.1f}%)")
    
    # åˆ¤æ–­æ˜¯å¦é€šè¿‡æ£€æŸ¥
    success_rate = valid_images / total_images
    if success_rate >= 0.9:  # 90%ä»¥ä¸Šå›¾ç‰‡æœ‰æ•ˆ
        print(f"âœ… å›¾ç‰‡å¯¹åº”æ£€æŸ¥é€šè¿‡ (æˆåŠŸç‡: {success_rate*100:.1f}%)")
        return True
    elif success_rate >= 0.7:  # 70%ä»¥ä¸Šå›¾ç‰‡æœ‰æ•ˆ
        print(f"âš ï¸  å›¾ç‰‡å¯¹åº”æ£€æŸ¥è­¦å‘Š (æˆåŠŸç‡: {success_rate*100:.1f}%)")
        return True
    else:
        print(f"âŒ å›¾ç‰‡å¯¹åº”æ£€æŸ¥å¤±è´¥ (æˆåŠŸç‡: {success_rate*100:.1f}%)")
        return False

def process_dataset(dataset_name, cfg):
    """å¤„ç†å•ä¸ªæ•°æ®é›†"""
    print(f"\n{'='*50}")
    print(f"å¤„ç†æ•°æ®é›†: {dataset_name}")
    print(f"{'='*50}")
    
    data_path = os.path.join(cfg['data_dir'], cfg['relation_file'])
    sep = cfg['sep']
    out_dir = cfg['output_dir']
    
    # æ£€æŸ¥æ•°æ®æ–‡ä»¶æ˜¯å¦å­˜åœ¨
    if not os.path.exists(data_path):
        print(f"âŒ æ•°æ®æ–‡ä»¶ä¸å­˜åœ¨: {data_path}")
        return False
    
    # åˆ›å»ºè¾“å‡ºç›®å½•
    os.makedirs(out_dir, exist_ok=True)
    print(f"ğŸ“ è¾“å‡ºç›®å½•: {out_dir}")
    
    # åŠ è½½best_img_dict
    best_img_dict = load_best_img_dict(cfg)
    
    print(f"ğŸ“ è¯»å–æ•°æ®: {data_path}")
    try:
        # å°è¯•ä¸åŒçš„ç¼–ç æ ¼å¼
        encodings = ['utf-8', 'latin-1', 'cp1252', 'iso-8859-1']
        df = None
        
        for encoding in encodings:
            try:
                df = pd.read_csv(data_path, sep=sep, encoding=encoding)
                print(f"âœ… æˆåŠŸä½¿ç”¨ç¼–ç : {encoding}")
                break
            except UnicodeDecodeError:
                continue
        
        if df is None:
            print(f"âŒ æ‰€æœ‰ç¼–ç æ ¼å¼éƒ½æ— æ³•è¯»å–æ–‡ä»¶")
            return False
            
        print(f"ğŸ“Š åŸå§‹æ ·æœ¬æ•°: {len(df)}")
        print(f"ğŸ“Š åŸå§‹åˆ—æ•°: {len(df.columns)}")
        print(f"ğŸ“Š åŸå§‹åˆ—å: {list(df.columns)}")
    except Exception as e:
        print(f"âŒ è¯»å–æ•°æ®å¤±è´¥: {e}")
        return False
    
    # æ ¹æ®best_img_dictè¿‡æ»¤æ•°æ®
    df = filter_data_by_best_img(df, best_img_dict)
    
    # æ£€æŸ¥å›¾ç‰‡å¯¹åº”å…³ç³»
    if not check_image_correspondence(df, cfg):
        print("âš ï¸  å›¾ç‰‡å¯¹åº”æ£€æŸ¥å¤±è´¥ï¼Œä½†ç»§ç»­å¤„ç†...")

    # æ‹†åˆ†
    train_df, valid_df, test_df = split_df(df, seed=SEED)
    print(f"ğŸ“ˆ æ‹†åˆ†ç»“æœ - train: {len(train_df)}, valid: {len(valid_df)}, test: {len(test_df)}")
    
    # å¤åˆ¶HERåŒ¹é…ç»“æœæ–‡ä»¶ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
    her_map_path = os.path.join(cfg['data_dir'], cfg['her_map'])
    if os.path.exists(her_map_path):
        her_map_dst = os.path.join(out_dir, cfg['her_map'])
        try:
            # ç›´æ¥è¯»å–å†…å®¹å¹¶å†™å…¥ï¼Œé¿å…æƒé™é—®é¢˜
            with open(her_map_path, 'r', encoding='utf-8') as f:
                content = f.read()
            with open(her_map_dst, 'w', encoding='utf-8') as f:
                f.write(content)
            print(f"âœ… å·²å¤åˆ¶HERåŒ¹é…ç»“æœ: {cfg['her_map']}")
        except Exception as e:
            print(f"âŒ å¤åˆ¶HERåŒ¹é…ç»“æœå¤±è´¥: {e}")
    else:
        print(f"âš ï¸  HERåŒ¹é…ç»“æœæ–‡ä»¶ä¸å­˜åœ¨: {her_map_path}")
    
    # ä¿å­˜cleanæ•°æ®é›†
    def save_csv_with_retry(df, filepath, max_retries=3):
        """å¸¦é‡è¯•æœºåˆ¶çš„CSVä¿å­˜"""
        for attempt in range(max_retries):
            try:
                df.to_csv(filepath, index=False)
                return True
            except OSError as e:
                if attempt < max_retries - 1:
                    print(f"âš ï¸  ä¿å­˜å¤±è´¥ï¼Œé‡è¯• {attempt + 1}/{max_retries}: {filepath}")
                    import time
                    time.sleep(1)  # ç­‰å¾…1ç§’åé‡è¯•
                else:
                    print(f"âŒ ä¿å­˜å¤±è´¥: {filepath}, é”™è¯¯: {e}")
                    return False
        return False
    
    # ä¿å­˜cleanæ•°æ®é›†
    save_success = True
    save_success &= save_csv_with_retry(train_df, os.path.join(out_dir, 'train_clean.csv'))
    save_success &= save_csv_with_retry(valid_df, os.path.join(out_dir, 'valid_clean.csv'))
    save_success &= save_csv_with_retry(test_df, os.path.join(out_dir, 'test_clean.csv'))
    
    if save_success:
        print("âœ… å·²ä¿å­˜cleanæ•°æ®é›†")
    else:
        print("âŒ ä¿å­˜cleanæ•°æ®é›†å¤±è´¥")
        return False

    # valid/teståŠ å™ªéŸ³
    print("\nğŸ”§ ä¸ºvalidæ•°æ®é›†æ·»åŠ å™ªéŸ³...")
    if USE_EXACT_CELL_COUNT:
        valid_dirty = add_precise_noise(valid_df, noise_cells=NOISE_CELLS, use_exact_cell_count=True, seed=SEED)
    else:
        valid_dirty = add_precise_noise(valid_df, noise_ratio=NOISE_RATIO, seed=SEED)
    
    print("\nğŸ”§ ä¸ºtestæ•°æ®é›†æ·»åŠ å™ªéŸ³...")
    if USE_EXACT_CELL_COUNT:
        test_dirty = add_precise_noise(test_df, noise_cells=NOISE_CELLS, use_exact_cell_count=True, seed=SEED)
    else:
        test_dirty = add_precise_noise(test_df, noise_ratio=NOISE_RATIO, seed=SEED)
    
    # ä¿å­˜dirtyæ•°æ®é›†
    save_success = True
    save_success &= save_csv_with_retry(valid_dirty, os.path.join(out_dir, 'valid_dirty.csv'))
    save_success &= save_csv_with_retry(test_dirty, os.path.join(out_dir, 'test_dirty.csv'))
    
    if save_success:
        print("âœ… å·²ä¿å­˜dirtyæ•°æ®é›†")
    else:
        print("âŒ ä¿å­˜dirtyæ•°æ®é›†å¤±è´¥")
        return False
    
    # å¤åˆ¶å›¾ç‰‡æ–‡ä»¶å¤¹
    img_src_dir = os.path.join(cfg['data_dir'], cfg['img_dir'])
    img_dst_dir = os.path.join(out_dir, cfg['img_dir'])
    if os.path.exists(img_src_dir):
        if os.path.exists(img_dst_dir):
            shutil.rmtree(img_dst_dir)  # å¦‚æœç›®æ ‡ç›®å½•å­˜åœ¨ï¼Œå…ˆåˆ é™¤
        try:
            # ä½¿ç”¨è‡ªå®šä¹‰å¤åˆ¶å‡½æ•°ï¼Œé¿å…æƒé™é—®é¢˜
            def copy_tree_without_metadata(src, dst):
                if not os.path.exists(dst):
                    os.makedirs(dst)
                for item in os.listdir(src):
                    s = os.path.join(src, item)
                    d = os.path.join(dst, item)
                    if os.path.isdir(s):
                        copy_tree_without_metadata(s, d)
                    else:
                        # ç›´æ¥è¯»å–å†…å®¹å¹¶å†™å…¥ï¼Œé¿å…æƒé™é—®é¢˜
                        try:
                            with open(s, 'rb') as f_src:
                                content = f_src.read()
                            with open(d, 'wb') as f_dst:
                                f_dst.write(content)
                        except Exception as e:
                            print(f"âš ï¸  å¤åˆ¶æ–‡ä»¶å¤±è´¥ {s}: {e}")
            
            copy_tree_without_metadata(img_src_dir, img_dst_dir)
            print(f"âœ… å·²å¤åˆ¶å›¾ç‰‡æ–‡ä»¶å¤¹: {img_src_dir} -> {img_dst_dir}")
        except Exception as e:
            print(f"âŒ å¤åˆ¶å›¾ç‰‡æ–‡ä»¶å¤¹å¤±è´¥: {e}")
    else:
        print(f"âš ï¸  å›¾ç‰‡æ–‡ä»¶å¤¹ä¸å­˜åœ¨: {img_src_dir}")
    
    # æ‰“å°cleanå’Œdirtyæ•°æ®çš„å‰5è¡Œ
    print("\nğŸ“Š Cleanæ•°æ®å‰5è¡Œ:")
    print("=" * 50)
    print("Train Clean:")
    print(train_df.head())
    print("\nValid Clean:")
    print(valid_df.head())
    print("\nTest Clean:")
    print(test_df.head())
    
    print("\nğŸ“Š Dirtyæ•°æ®å‰5è¡Œ:")
    print("=" * 50)
    print("Valid Dirty:")
    print(valid_dirty.head())
    print("\nTest Dirty:")
    print(test_dirty.head())
    
    return True

def main():
    """ä¸»å‡½æ•° - å¤„ç†æ‰€æœ‰æ•°æ®é›†"""
    set_seed(SEED)
    print("ğŸš€ å¼€å§‹å¤„ç†æ‰€æœ‰æ•°æ®é›†")
    if USE_EXACT_CELL_COUNT:
        print(f"ğŸ”§ é…ç½® - SEED: {SEED}, NOISE_CELLS: {NOISE_CELLS} (ç²¾ç¡®å•å…ƒæ ¼æ•°é‡æ¨¡å¼)")
    else:
        print(f"ğŸ”§ é…ç½® - SEED: {SEED}, NOISE_RATIO: {NOISE_RATIO} (æŒ‰è¡Œæ•°æ¯”ä¾‹æ¨¡å¼)")
    
    success_count = 0
    total_count = len(DATASET_CONFIG)
    
    for dataset_name, cfg in DATASET_CONFIG.items():
        if process_dataset(dataset_name, cfg):
            success_count += 1
    
    print(f"\n{'='*50}")
    print(f"ğŸ‰ å¤„ç†å®Œæˆ! æˆåŠŸ: {success_count}/{total_count}")
    print(f"{'='*50}")

if __name__ == '__main__':
    main() 