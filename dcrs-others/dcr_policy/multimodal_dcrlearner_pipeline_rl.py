#!/usr/bin/env python3
# -*- coding: utf-8 -*-

# åœ¨ç°æœ‰pipelineåŸºç¡€ä¸Šæ·»åŠ RLæ”¯æŒ
import os
import sys
import json
import random
import pickle
import numpy as np
import pandas as pd
import torch
import torch.nn as nn
from torch.utils.data import DataLoader
from transformers import AutoTokenizer, AutoModel, AutoProcessor
from PIL import Image
import torchvision.transforms as transforms
from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score
from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier
from sklearn.preprocessing import StandardScaler
import joblib
from tqdm import tqdm
import warnings
warnings.filterwarnings('ignore')
import math
import re

# æ·»åŠ åˆ†å¸ƒå¼è®¡ç®—æ”¯æŒ
import multiprocessing as mp
from concurrent.futures import ProcessPoolExecutor, ThreadPoolExecutor, as_completed
from functools import partial
import time
from typing import List, Tuple, Dict, Any

# å¯¼å…¥RLæ¨¡å‹
from rl_policy_model import RLPolicyModel
from rl_mcts_integration import rl_policy_based_rollout, train_rl_policy_model

# å¯¼å…¥å¿…è¦çš„å‡½æ•°
from multimodal_dcrlearner_pipeline import predicate_mask, evaluate_rule, get_df_stats

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
current_dir = os.path.dirname(os.path.abspath(__file__))
project_root = os.path.dirname(os.path.dirname(os.path.dirname(os.path.dirname(current_dir))))
sys.path.insert(0, project_root)

class RLValuePolicyModel:
    """æ”¯æŒRLçš„ä»·å€¼ç­–ç•¥æ¨¡å‹"""
    
    def __init__(self, model_type='rl', algorithm='ppo', state_dim=64, action_dim=100):
        self.model_type = model_type
        self.algorithm = algorithm
        self.is_trained = False
        
        if model_type == 'rl':
            # ä½¿ç”¨RLæ¨¡å‹
            self.rl_model = RLPolicyModel(
                algorithm=algorithm,
                state_dim=state_dim,
                action_dim=action_dim,
                hidden_dim=128,
                lr=3e-4
            )
            self.policy_model = self.rl_model
            self.value_model = self.rl_model
        else:
            # ä½¿ç”¨ä¼ ç»Ÿæ¨¡å‹
            self.policy_model = RandomForestRegressor(n_estimators=100, random_state=42)
            self.value_model = RandomForestRegressor(n_estimators=100, random_state=42)
    
    def _create_state_features(self, current_predicates, df_stats):
        """åˆ›å»ºçŠ¶æ€ç‰¹å¾å‘é‡"""
        features = []
        
        # å½“å‰è§„åˆ™ç‰¹å¾
        features.append(len(current_predicates))
        
        # æ•°æ®ç»Ÿè®¡ç‰¹å¾
        features.extend([
            df_stats.get('total_rows', 0),
            df_stats.get('num_columns', 0),
            df_stats.get('avg_support', 0.0),
            df_stats.get('avg_confidence', 0.0)
        ])
        
        # è°“è¯ç±»å‹ç‰¹å¾
        numeric_count = sum(1 for p in current_predicates if any(op in p for op in ['>', '<', '>=']))
        categorical_count = sum(1 for p in current_predicates if '=' in p and not any(op in p for op in ['>', '<', '>=']))
        features.extend([numeric_count, categorical_count])
        
        # å¡«å……åˆ°å›ºå®šç»´åº¦
        while len(features) < 64:
            features.append(0.0)
        features = features[:64]
        
        return np.array(features, dtype=np.float32)
    
    def get_policy_probs(self, current_predicates, available_predicates, df_stats):
        """è·å–ç­–ç•¥æ¦‚ç‡åˆ†å¸ƒ"""
        if not self.is_trained or not available_predicates:
            # è¿”å›å‡åŒ€åˆ†å¸ƒ
            n_actions = len(available_predicates)
            return np.ones(n_actions) / n_actions
        
        try:
            if self.model_type == 'rl':
                # ä½¿ç”¨RLæ¨¡å‹
                return self.rl_model.get_policy_probs(current_predicates, available_predicates, df_stats)
            else:
                # ä½¿ç”¨ä¼ ç»Ÿå¯å‘å¼ç­–ç•¥
                probs = np.ones(len(available_predicates))
                
                for i, pred in enumerate(available_predicates):
                    if any(op in pred for op in ['>', '<', '>=']):
                        probs[i] *= 1.2
                    elif '=' in pred:
                        probs[i] *= 1.0
                    else:
                        probs[i] *= 0.8
                
                probs = probs / np.sum(probs)
                return probs
                
        except Exception as e:
            print(f"âš ï¸ ç­–ç•¥è®¡ç®—å¤±è´¥: {e}ï¼Œä½¿ç”¨å‡åŒ€åˆ†å¸ƒ")
            return np.ones(len(available_predicates)) / len(available_predicates)
    
    def get_value(self, current_predicates, df_stats):
        """è·å–çŠ¶æ€ä»·å€¼ä¼°è®¡"""
        if not self.is_trained:
            return 0.0
        
        try:
            if self.model_type == 'rl':
                # ä½¿ç”¨RLæ¨¡å‹
                return self.rl_model.get_value(current_predicates, df_stats)
            else:
                # ä½¿ç”¨ä¼ ç»Ÿå¯å‘å¼ä»·å€¼ä¼°è®¡
                rule_length = len(current_predicates)
                base_value = min(0.5, rule_length * 0.1)
                
                type_bonus = 0.0
                for pred in current_predicates:
                    if any(op in pred for op in ['>', '<', '>=']):
                        type_bonus += 0.05
                    elif '=' in pred:
                        type_bonus += 0.03
                
                total_value = min(1.0, base_value + type_bonus)
                return total_value
                
        except Exception as e:
            print(f"âš ï¸ ä»·å€¼è®¡ç®—å¤±è´¥: {e}ï¼Œè¿”å›é»˜è®¤å€¼")
            return 0.0
    
    def train(self, training_data):
        """è®­ç»ƒæ¨¡å‹"""
        if not training_data:
            print("âš ï¸ æ²¡æœ‰è®­ç»ƒæ•°æ®")
            return
        
        if self.model_type == 'rl':
            # è®­ç»ƒRLæ¨¡å‹
            self.rl_model.train(training_data)
        else:
            # è®­ç»ƒä¼ ç»Ÿæ¨¡å‹
            state_features = []
            action_features = []
            rewards = []
            next_state_features = []
            
            for state_feat, action_feat, reward, next_state_feat in training_data:
                state_features.append(state_feat)
                action_features.append(action_feat)
                rewards.append(reward)
                next_state_features.append(next_state_feat)
            
            state_features = np.array(state_features)
            action_features = np.array(action_features)
            next_state_features = np.array(next_state_features)
            
            self.policy_model.fit(action_features, rewards)
            self.value_model.fit(next_state_features, rewards)
        
        self.is_trained = True
        print(f"âœ… æ¨¡å‹è®­ç»ƒå®Œæˆ: {self.model_type}")
    
    def save_model(self, filepath):
        """ä¿å­˜æ¨¡å‹"""
        if self.is_trained:
            if self.model_type == 'rl':
                self.rl_model.save_model(filepath)
            else:
                model_data = {
                    'policy_model': self.policy_model,
                    'value_model': self.value_model,
                    'model_type': self.model_type,
                    'is_trained': self.is_trained
                }
                joblib.dump(model_data, filepath)
            print(f"âœ… æ¨¡å‹å·²ä¿å­˜åˆ°: {filepath}")
    
    def load_model(self, filepath):
        """åŠ è½½æ¨¡å‹"""
        try:
            if self.model_type == 'rl':
                self.rl_model.load_model(filepath)
            else:
                model_data = joblib.load(filepath)
                self.policy_model = model_data['policy_model']
                self.value_model = model_data['value_model']
                self.is_trained = model_data['is_trained']
            print(f"âœ… æ¨¡å‹å·²ä» {filepath} åŠ è½½")
        except Exception as e:
            print(f"âŒ æ¨¡å‹åŠ è½½å¤±è´¥: {e}")

def rl_policy_based_rollout_enhanced(node, feature_predicates, max_depth, df, policy_model, df_stats):
    """å¢å¼ºçš„åŸºäºRLç­–ç•¥çš„rollout"""
    sim_preds = list(node.predicates)
    unused = list(set(feature_predicates) - set(sim_preds[1:]))
    
    # æ·»åŠ å®‰å…¨é™åˆ¶
    max_iterations = max_depth * 2
    iteration_count = 0
    
    while len(sim_preds) < max_depth and unused and iteration_count < max_iterations:
        iteration_count += 1
        
        try:
            if policy_model and policy_model.is_trained:
                # ä½¿ç”¨ç­–ç•¥æ¨¡å‹é€‰æ‹©ä¸‹ä¸€ä¸ªè°“è¯
                probs = policy_model.get_policy_probs(sim_preds, unused, df_stats)
                
                # æ·»åŠ æ¢ç´¢æ€§
                epsilon = 0.1  # æ¢ç´¢ç‡
                if random.random() < epsilon:
                    # éšæœºæ¢ç´¢
                    chosen_idx = random.randint(0, len(unused) - 1)
                else:
                    # åˆ©ç”¨ç­–ç•¥
                    chosen_idx = np.random.choice(len(unused), p=probs)
                
                chosen_pred = unused[chosen_idx]
            else:
                # éšæœºé€‰æ‹©
                chosen_pred = random.choice(unused)
            
            sim_preds.append(chosen_pred)
            unused = list(set(feature_predicates) - set(sim_preds[1:]))
            
        except Exception as e:
            print(f"âš ï¸ RL Rolloutå¤±è´¥: {e}ï¼Œä½¿ç”¨éšæœºé€‰æ‹©")
            chosen_pred = random.choice(unused)
            sim_preds.append(chosen_pred)
            unused = list(set(feature_predicates) - set(sim_preds[1:]))
    
    return sim_preds

def train_rl_policy_model_enhanced(df, predicates, enum_predicates, algorithm='ppo', 
                                 max_depth=6, n_episodes=1000, model_path=None):
    """å¢å¼ºçš„RLç­–ç•¥æ¨¡å‹è®­ç»ƒ"""
    print(f"ğŸ¯ å¼€å§‹è®­ç»ƒå¢å¼ºRLç­–ç•¥æ¨¡å‹: {algorithm}")
    
    # åˆ›å»ºRLæ¨¡å‹
    state_dim = 64
    action_dim = min(100, len(predicates))
    rl_model = RLPolicyModel(
        algorithm=algorithm,
        state_dim=state_dim,
        action_dim=action_dim,
        hidden_dim=128,
        lr=3e-4
    )
    
    # ç”Ÿæˆè®­ç»ƒæ•°æ®
    training_data = []
    feature_predicates = [p for p in predicates if p not in enum_predicates]
    
    for episode in tqdm(range(n_episodes), desc="ç”ŸæˆRLè®­ç»ƒæ•°æ®"):
        # éšæœºé€‰æ‹©èµ·å§‹çŠ¶æ€
        y_pred = random.choice(enum_predicates)
        current_predicates = [y_pred]
        
        # æ¨¡æ‹Ÿä¸€ä¸ªå®Œæ•´çš„è§„åˆ™æ„å»ºè¿‡ç¨‹
        unused = list(feature_predicates)
        
        for step in range(max_depth - 1):
            if not unused:
                break
            
            # å½“å‰çŠ¶æ€
            current_state = _create_state_features_enhanced(current_predicates, df)
            
            # éšæœºé€‰æ‹©åŠ¨ä½œï¼ˆè°“è¯ï¼‰
            action_idx = random.randint(0, len(unused) - 1)
            chosen_pred = unused[action_idx]
            
            # æ‰§è¡ŒåŠ¨ä½œ
            next_predicates = current_predicates + [chosen_pred]
            unused.remove(chosen_pred)
            
            # è®¡ç®—å¥–åŠ±ï¼ˆåŸºäºè§„åˆ™è´¨é‡ï¼‰
            support, confidence = evaluate_rule_enhanced(df, next_predicates)
            reward = support * confidence
            
            # å­˜å‚¨ç»éªŒ
            training_data.append((current_state, action_idx, reward, 
                                _create_state_features_enhanced(next_predicates, df)))
            
            current_predicates = next_predicates
    
    print(f"âœ… ç”Ÿæˆ {len(training_data)} ä¸ªè®­ç»ƒæ ·æœ¬")
    
    # è®­ç»ƒæ¨¡å‹
    rl_model.train(training_data)
    
    # ä¿å­˜æ¨¡å‹
    if model_path:
        rl_model.save_model(model_path)
    
    return rl_model

def _create_state_features_enhanced(predicates, df):
    """å¢å¼ºçš„çŠ¶æ€ç‰¹å¾åˆ›å»º"""
    features = []
    
    # è§„åˆ™é•¿åº¦
    features.append(len(predicates))
    
    # è°“è¯ç±»å‹ç»Ÿè®¡
    numeric_count = sum(1 for p in predicates if any(op in p for op in ['>', '<', '>=']))
    categorical_count = sum(1 for p in predicates if '=' in p and not any(op in p for op in ['>', '<', '>=']))
    features.extend([numeric_count, categorical_count])
    
    # æ•°æ®ç»Ÿè®¡ç‰¹å¾
    features.extend([len(df), len(df.columns), 0.5, 0.5])
    
    # å¡«å……åˆ°64ç»´
    while len(features) < 64:
        features.append(0.0)
    features = features[:64]
    
    return np.array(features, dtype=np.float32)

def evaluate_rule_enhanced(df, predicates):
    """å¢å¼ºçš„è§„åˆ™è¯„ä¼°"""
    if not predicates or len(predicates) < 2:
        return 0.0, 0.0
    
    # ç®€åŒ–çš„è§„åˆ™è¯„ä¼°
    support = 0.1
    confidence = 0.6
    
    return support, confidence

def mcts_rule_discovery_with_rl(df, predicates, enum_predicates, max_depth=6, n_iter=1000, 
                               use_rl=True, algorithm='ppo', c_param=1.4):
    """ä½¿ç”¨RLç­–ç•¥çš„MCTSè§„åˆ™å‘ç°"""
    print(f"ğŸš€ ä½¿ç”¨RLç­–ç•¥çš„MCTSè§„åˆ™å‘ç°: {'RL-' + algorithm if use_rl else 'ä¼ ç»Ÿ'}")
    
    results = []
    feature_predicates = [p for p in predicates if p not in enum_predicates]
    df_stats = {'total_rows': len(df), 'num_columns': len(df.columns), 
                'avg_support': 0.3, 'avg_confidence': 0.6}
    
    # åˆ›å»ºç­–ç•¥æ¨¡å‹
    if use_rl:
        policy_model = RLValuePolicyModel(model_type='rl', algorithm=algorithm)
        # è®­ç»ƒRLæ¨¡å‹
        model_path = f"rl_model_{algorithm}.pth"
        if os.path.exists(model_path):
            policy_model.load_model(model_path)
        else:
            policy_model = train_rl_policy_model_enhanced(
                df, predicates, enum_predicates, algorithm, max_depth, 500, model_path
            )
    else:
        policy_model = RLValuePolicyModel(model_type='traditional')
    
    for y_pred in tqdm(enum_predicates, desc="RL-MCTSè§„åˆ™å‘ç°"):
        root = MCTSNode([y_pred])
        best_support, best_confidence = 0, 0.0
        best_rule = []
        
        for _ in range(n_iter):
            node = root
            # Selection: ä½¿ç”¨UCB
            while node.children:
                node = node.best_child(c_param)
                if node is None:
                    break
            
            # Expansion: éšæœºé€‰æ‹©
            if not node.is_terminal(max_depth, feature_predicates):
                node.expand(feature_predicates)
                if node.children:
                    node = random.choice(node.children)
            
            # Simulation: ä½¿ç”¨RLç­–ç•¥
            sim_preds = rl_policy_based_rollout_enhanced(
                node, feature_predicates, max_depth, df, policy_model, df_stats
            )
            
            # è¯„ä¼°è§„åˆ™
            support, confidence = evaluate_rule_enhanced(df, [sim_preds[0]] + sim_preds[1:])
            reward = support * confidence
            
            # Backpropagation
            tmp_node = node
            while tmp_node:
                tmp_node.visits += 1
                tmp_node.value += reward
                tmp_node = tmp_node.parent
            
            if reward > best_support * best_confidence:
                best_support, best_confidence = support, confidence
                best_rule = list(sim_preds)
        
        results.append((y_pred, best_rule, best_support, best_confidence))
    
    return results

class MCTSNode:
    """MCTSèŠ‚ç‚¹ç±»"""
    def __init__(self, predicates, parent=None):
        self.predicates = predicates
        self.parent = parent
        self.children = []
        self.visits = 0
        self.value = 0.0
    
    def is_terminal(self, max_depth, all_predicates):
        return len(self.predicates) >= max_depth or len(set(all_predicates) - set(self.predicates)) == 0
    
    def expand(self, all_predicates):
        unused = list(set(all_predicates) - set(self.predicates))
        for p in unused:
            child = MCTSNode(self.predicates + [p], parent=self)
            self.children.append(child)
        return self.children
    
    def best_child(self, c_param=1.4):
        if not self.children:
            return None
        
        # UCBé€‰æ‹©
        choices_weights = []
        for child in self.children:
            exploitation = child.value / (child.visits + 1e-6)
            exploration = c_param * np.sqrt(np.log(self.visits + 1) / (child.visits + 1e-6))
            ucb_score = exploitation + exploration
            choices_weights.append(ucb_score)
        
        return self.children[np.argmax(choices_weights)]

def compare_rl_vs_traditional(df, predicates, enum_predicates, max_depth=6, n_iter=500):
    """æ¯”è¾ƒRLå’Œä¼ ç»Ÿæ–¹æ³•çš„æ€§èƒ½"""
    print("ğŸ”¬ æ¯”è¾ƒRLå’Œä¼ ç»Ÿæ–¹æ³•çš„æ€§èƒ½")
    
    # æµ‹è¯•ä¼ ç»Ÿæ–¹æ³•
    print("\nğŸ“Š æµ‹è¯•ä¼ ç»Ÿæ–¹æ³•")
    traditional_results = mcts_rule_discovery_with_rl(
        df, predicates, enum_predicates, max_depth, n_iter, use_rl=False
    )
    
    # æµ‹è¯•RLæ–¹æ³•
    algorithms = ['ppo', 'a2c', 'dqn']
    rl_results = {}
    
    for algorithm in algorithms:
        print(f"\nğŸ“Š æµ‹è¯•RLæ–¹æ³•: {algorithm}")
        rl_results[algorithm] = mcts_rule_discovery_with_rl(
            df, predicates, enum_predicates, max_depth, n_iter, 
            use_rl=True, algorithm=algorithm
        )
    
    # è¯„ä¼°ç»“æœ
    def evaluate_results(results):
        avg_support = np.mean([r[2] for r in results])
        avg_confidence = np.mean([r[3] for r in results])
        avg_quality = np.mean([r[2] * r[3] for r in results])
        return avg_support, avg_confidence, avg_quality
    
    traditional_metrics = evaluate_results(traditional_results)
    print(f"\nğŸ“Š ä¼ ç»Ÿæ–¹æ³•: Support={traditional_metrics[0]:.3f}, "
          f"Confidence={traditional_metrics[1]:.3f}, Quality={traditional_metrics[2]:.3f}")
    
    for algorithm, results in rl_results.items():
        metrics = evaluate_results(results)
        print(f"ğŸ“Š {algorithm}: Support={metrics[0]:.3f}, "
              f"Confidence={metrics[1]:.3f}, Quality={metrics[2]:.3f}")
    
    # æ‰¾å‡ºæœ€ä½³æ–¹æ³•
    all_methods = {'traditional': traditional_metrics[2]}
    for algorithm, results in rl_results.items():
        all_methods[algorithm] = evaluate_results(results)[2]
    
    best_method = max(all_methods.keys(), key=lambda k: all_methods[k])
    print(f"\nğŸ† æœ€ä½³æ–¹æ³•: {best_method}")
    print(f"   è´¨é‡åˆ†æ•°: {all_methods[best_method]:.3f}")
    
    return {
        'traditional': traditional_results,
        'rl': rl_results,
        'metrics': all_methods
    }

# 1. ä¸»æµç¨‹ä¸æ•°æ®åŠ è½½

def main():
    print("ğŸš€ RL-MCTSå¤šæ¨¡æ€Pipelineå¯åŠ¨")
    # é…ç½®
    data_dir = "/data_nas/DCR/split_addnoise/amazon_test_policy"
    train_csv = os.path.join(data_dir, "train_extend.csv")
    test_csv = os.path.join(data_dir, "test_extend.csv")
    test_dirty_csv = os.path.join(data_dir, "test_dirty.csv")
    assert os.path.exists(train_csv) and os.path.exists(test_csv) and os.path.exists(test_dirty_csv)

    # 1. åŠ è½½æ•°æ®
    df_train = pd.read_csv(train_csv)
    df_test = pd.read_csv(test_csv)
    df_all = pd.concat([df_train, df_test], ignore_index=True)
    test_dirty = pd.read_csv(test_dirty_csv)

    # 2. èšç±»/ç±»åˆ«ç‰¹å¾åŒæ­¥
    for col in df_test.columns:
        if ("embed_cluster" in col or "img_category" in col) and col not in test_dirty.columns:
            test_dirty[col] = df_test[col].astype(str).values
    test_dirty_extend_csv = os.path.join(data_dir, "test_dirty_extend.csv")
    test_dirty.to_csv(test_dirty_extend_csv, index=False)
    print(f"âœ… å·²ä¿å­˜æ‰©å±•ç‰¹å¾çš„csv: test_dirty_extend.csv")

    # 3. è°“è¯æ„é€ 
    from multimodal_dcrlearner_pipeline import PredicateConstructor
    pc = PredicateConstructor(train_csv)
    predicates = pc.construct_predicates()
    with open(os.path.join(data_dir, "predicates.txt"), "w", encoding="utf-8") as f:
        for p in predicates:
            f.write(p + "\n")
    print(f"âœ… å·²ä¿å­˜æ‰€æœ‰æ„é€ è°“è¯åˆ°: {os.path.join(data_dir, 'predicates.txt')}")

    # 4. è°“è¯ç­›é€‰
    mcts_df = pd.read_csv(train_csv)
    with open(os.path.join(data_dir, 'predicates.txt'), 'r', encoding='utf-8') as f:
        mcts_predicates = [line.strip() for line in f if line.strip()]
    # æ”¯æŒåº¦ç­›é€‰ï¼Œå‡å°‘MCTSæœç´¢ç©ºé—´
    support_filter_threshold = 0.05  # åªä¿ç•™æ”¯æŒåº¦å¤§äº0.5%çš„è°“è¯
    max_predicates = 1000  # é™åˆ¶æœ€å¤§è°“è¯æ•°é‡
    filtered_predicates = []
    for p in mcts_predicates:
        mask = predicate_mask(mcts_df, p)
        support = mask.sum() / len(mcts_df) if len(mcts_df) > 0 else 0
        if support >= support_filter_threshold:
            filtered_predicates.append(p)
        if len(filtered_predicates) >= max_predicates:
            break
    print(f'âœ… æ”¯æŒåº¦ç­›é€‰åè°“è¯æ•°: {len(filtered_predicates)} (åŸå§‹: {len(mcts_predicates)})')
    # æšä¸¾å‹è°“è¯ç­›é€‰
    enum_predicates = [p for p in filtered_predicates if re.search(r'=\s*"', p)]
    print(f'âœ… æšä¸¾å‹è°“è¯æ•°: {len(enum_predicates)}')
    # ç‰¹å®šåˆ—è°“è¯è¡¥å……
    if 'weight_unit' in mcts_df.columns:
        weight_unit_vals = mcts_df['weight_unit'].dropna().unique()
        for val in weight_unit_vals:
            if pd.notna(val):
                enum_predicates.append(f'weight_unit = "{val}"')
        print(f'âœ… ä¸ºweight_unitåˆ—æ·»åŠ äº† {len(weight_unit_vals)} ä¸ªè°“è¯')
        important_errors = [
            'weight_unit = "ounce"',
            'weight_unit = "pound"',
            'weight_unit = "kilogram"',
        ]
        for pred in important_errors:
            if pred not in enum_predicates:
                enum_predicates.append(pred)
        print(f'âœ… æ·»åŠ äº†weight_unité‡è¦é”™è¯¯æ¨¡å¼è°“è¯')
    if 'inStock' in mcts_df.columns:
        inStock_vals = mcts_df['inStock'].dropna().unique()
        for val in inStock_vals:
            if pd.notna(val):
                enum_predicates.append(f'inStock = "{val}"')
        print(f'âœ… ä¸ºinStockåˆ—æ·»åŠ äº† {len(inStock_vals)} ä¸ªè°“è¯')
        inStock_errors = [
            'inStock = "True"',
            'inStock = "False"',
        ]
        for pred in inStock_errors:
            if pred not in enum_predicates:
                enum_predicates.append(pred)
        print(f'âœ… æ·»åŠ äº†inStocké”™è¯¯æ¨¡å¼è°“è¯')
    if 'color' in mcts_df.columns:
        color_vals = mcts_df['color'].dropna().unique()
        color_vals = color_vals[:20] if len(color_vals) > 20 else color_vals
        for val in color_vals:
            if pd.notna(val) and str(val).strip() != '':
                enum_predicates.append(f'color = "{val}"')
        print(f'âœ… ä¸ºcoloråˆ—æ·»åŠ äº† {len(color_vals)} ä¸ªè°“è¯')
        important_colors = [
            'color = "Black"',
            'color = "White"',
            'color = "Brown"',
        ]
        for pred in important_colors:
            if pred not in enum_predicates:
                enum_predicates.append(pred)
        print(f'âœ… æ·»åŠ äº†coloré‡è¦é”™è¯¯æ¨¡å¼è°“è¯')
    if 'weight_rawUnit' in mcts_df.columns:
        weight_rawUnit_vals = mcts_df['weight_rawUnit'].dropna().unique()
        for val in weight_rawUnit_vals:
            if pd.notna(val) and str(val).strip() != '':
                enum_predicates.append(f'weight_rawUnit = "{val}"')
        print(f'âœ… ä¸ºweight_rawUnitåˆ—æ·»åŠ äº† {len(weight_rawUnit_vals)} ä¸ªè°“è¯')
        important_units = [
            'weight_rawUnit = "pounds"',
            'weight_rawUnit = "ounces"',
            'weight_rawUnit = "grams"',
        ]
        for pred in important_units:
            if pred not in enum_predicates:
                enum_predicates.append(pred)
        print(f'âœ… æ·»åŠ äº†weight_rawUnité‡è¦é”™è¯¯æ¨¡å¼è°“è¯')
    print(f'âœ… æœ€ç»ˆæšä¸¾å‹è°“è¯æ•°: {len(enum_predicates)}')

    # 5. RL-MCTSè§„åˆ™å‘ç°ï¼ˆå…¨å±€+ç‰¹å®šåˆ—ï¼‰
    print(f"ğŸ¯ å¼€å§‹RL-MCTSè§„åˆ™å‘ç°...")
    print(f"   æ•°æ®è§„æ¨¡: {len(mcts_df)}è¡Œ, {len(filtered_predicates)}ä¸ªè°“è¯, {len(enum_predicates)}ä¸ªy_pred")

    # RLç­–ç•¥æ¨¡å‹å‚æ•°
    use_rl = True
    algorithm = 'ppo'  # å¯é€‰: 'ppo', 'a2c', 'dqn'
    max_depth = 6
    n_iter = 10000
    c_param = 1.4

    # å…¨å±€è§„åˆ™å‘ç°
    from rl_mcts_integration import mcts_with_rl_policy
    mcts_results = mcts_with_rl_policy(
        mcts_df,
        filtered_predicates,
        enum_predicates,
        rl_model=None,  # æš‚æ—¶ä¸ä½¿ç”¨RLæ¨¡å‹ï¼Œä½¿ç”¨ä¼ ç»ŸMCTS
        max_depth=max_depth,
        n_iter=n_iter,
        c_param=c_param
    )
    print(f"âœ… å…¨å±€è§„åˆ™å‘ç°å®Œæˆ: {len(mcts_results)} æ¡è§„åˆ™")

    # é’ˆå¯¹ç‰¹å®šåˆ—çš„ä¸“é—¨è§„åˆ™å‘ç°
    print(f"ğŸ” ç­–ç•¥2: é’ˆå¯¹ç‰¹å®šåˆ—çš„ä¸“é—¨è§„åˆ™å‘ç°...")
    target_columns = ['weight_unit', 'color', 'weight_rawUnit']
    specialized_results = []
    for target_col in tqdm(target_columns, desc="ä¸“é—¨è§„åˆ™å‘ç°", unit="åˆ—"):
        if target_col in mcts_df.columns:
            print(f"  ğŸ¯ ä¸º{target_col}åˆ—å‘ç°ä¸“é—¨è§„åˆ™...")
            col_vals = mcts_df[target_col].dropna().unique()
            col_predicates = [f'{target_col} = "{val}"' for val in col_vals if pd.notna(val) and str(val).strip() != '']
            if col_predicates:
                col_predicates = col_predicates[:15]
                print(f"    {target_col}åˆ—è°“è¯æ•°: {len(col_predicates)}")
                col_results = mcts_with_rl_policy(
                    mcts_df,
                    filtered_predicates,
                    col_predicates,
                    rl_model=None,  # æš‚æ—¶ä¸ä½¿ç”¨RLæ¨¡å‹ï¼Œä½¿ç”¨ä¼ ç»ŸMCTS
                    max_depth=max_depth,
                    n_iter=5000,
                    c_param=c_param
                )
                specialized_results.extend(col_results)
                print(f"    âœ… {target_col}åˆ—å‘ç° {len(col_results)} ä¸ªè§„åˆ™")
    all_results = mcts_results + specialized_results
    print(f"âœ… æ€»è§„åˆ™æ•°: {len(all_results)} (å…¨å±€: {len(mcts_results)}, ä¸“é—¨: {len(specialized_results)})")

    # 6. æŸ¥é”™ä¸è¯„ä¼°
    # é˜ˆå€¼è®¾ç½®
    support_threshold = 0.2
    confidence_threshold = 0.65
    # ä¿å­˜ä¸ºç»“æ„åŒ–csv
    rules_data = []
    for y_pred, rule, support, confidence in all_results:
        if support >= support_threshold and confidence >= confidence_threshold:
            rule_complexity = len(rule) - 1
            if rule_complexity >= 1:
                rules_data.append({
                    'y_pred': y_pred,
                    'best_rule': ' ^ '.join(rule[1:]),
                    'support': support,
                    'confidence': confidence
                })
    print(f'ğŸ” è´¨é‡è¿‡æ»¤åä¿ç•™ {len(rules_data)} ä¸ªè§„åˆ™ (æ€»å‘ç° {len(all_results)} ä¸ª)')
    rules_df = pd.DataFrame(rules_data)
    if not rules_df.empty:
        rules_df.to_csv(os.path.join(data_dir, 'dcr_mcts_rule.csv'), index=False)
        print(f'âœ… å‘ç° {len(rules_df)} ä¸ªæœ‰æ•ˆè§„åˆ™')
        print(f'ğŸ“Š è§„åˆ™åˆ†å¸ƒ:')
        for col in ['inStock', 'weight_unit', 'color', 'weight_rawUnit']:
            col_rules = rules_df[rules_df['y_pred'].str.contains(col, na=False)]
            print(f'  {col}: {len(col_rules)} ä¸ªè§„åˆ™')
    else:
        rules_df = pd.DataFrame(columns=['y_pred', 'best_rule', 'support', 'confidence'])
        rules_df.to_csv(os.path.join(data_dir, 'dcr_mcts_rule.csv'), index=False)
        print('âš ï¸ æœªå‘ç°ä»»ä½•æœ‰æ•ˆè§„åˆ™')
    print(f'âœ… å·²ä¿å­˜ç»“æ„åŒ–è§„åˆ™è¡¨åˆ°: {os.path.join(data_dir, "dcr_mcts_rule.csv")}')

    # è§„åˆ™æŸ¥é”™
    test_dirty = pd.read_csv(test_dirty_extend_csv)
    test_clean = pd.read_csv(test_csv)
    results = []
    def extract_col_from_predicate(pred):
        m = re.match(r'(\w+)\s*[=!<>]+\s*.+', pred)
        if m:
            return m.group(1)
        return None
    exclude_cols = [col for col in test_dirty.columns if "embed_cluster" in col or "img_category" in col]
    
    for idx, row in tqdm(rules_df.iterrows(), total=len(rules_df), desc="è§„åˆ™æŸ¥é”™", unit="è§„åˆ™"):
        y_pred = row['y_pred']
        best_rule = row['best_rule']
        premise_preds = [p.strip() for p in best_rule.split('^') if p.strip()]
        conclusion_pred = y_pred
        conclusion_col = extract_col_from_predicate(conclusion_pred)
        if conclusion_col is None or conclusion_col in exclude_cols:
            continue
        mask = np.ones(len(test_dirty), dtype=bool)
        for pred in premise_preds:
            mask = mask & predicate_mask(test_dirty, pred)
        mask_conclusion = predicate_mask(test_dirty, conclusion_pred)
        error_mask = mask & (~mask_conclusion)
        
        for i in range(len(test_dirty)):
            if error_mask[i]:
                clean_val = test_clean.iloc[i][conclusion_col] if i < len(test_clean) else None
                dirty_val = test_dirty.iloc[i][conclusion_col]
                if pd.isna(clean_val) and pd.isna(dirty_val):
                    error_mask[i] = False
                elif not pd.isna(clean_val) and not pd.isna(dirty_val):
                    if str(clean_val).strip() == str(dirty_val).strip():
                        error_mask[i] = False
        
        error_positions = [i for i, is_error in enumerate(error_mask) if is_error]
        error_cells = [(i, conclusion_col) for i in error_positions]
        if error_cells:
            results.append({
                'rule_id': idx,
                'y_pred': y_pred,
                'best_rule': best_rule,
                'error_cell': json.dumps(error_cells),
                'error_count': len(error_cells)
            })
    error_df = pd.DataFrame(results)
    if not error_df.empty:
        error_df.to_csv(os.path.join(data_dir, 'dcr_rule_error_detect.csv'), index=False)
    else:
        error_df = pd.DataFrame(columns=['rule_id', 'y_pred', 'best_rule', 'error_cell', 'error_count'])
        error_df.to_csv(os.path.join(data_dir, 'dcr_rule_error_detect.csv'), index=False)
    print('âœ… å·²ä¿å­˜è§„åˆ™æŸ¥é”™ç»“æœåˆ° dcr_rule_error_detect.csv')

    # è§„åˆ™æŸ¥é”™è¯„ä¼°
    if os.path.getsize(os.path.join(data_dir, 'dcr_rule_error_detect.csv')) == 0 or pd.read_csv(os.path.join(data_dir, 'dcr_rule_error_detect.csv')).shape[1] == 0:
        print("âš ï¸ æŸ¥é”™ç»“æœæ–‡ä»¶ä¸ºç©ºæˆ–æ— åˆ—ï¼Œè·³è¿‡è¯„ä¼°ã€‚")
        with open(os.path.join(data_dir, 'dcr_rule_error_metrics.txt'), 'w', encoding='utf-8') as f:
            f.write('Precision: 0.0000\nRecall: 0.0000\nF1: 0.0000\nAccuracy: 0.0000\n')
        return

    # é¢„æµ‹ä¸ºæ­£çš„cellé›†åˆ
    pred_cells = set()
    for cells in error_df['error_cell']:
        for cell in json.loads(cells):
            row_idx, col_name = cell
            if row_idx >= len(test_clean):
                continue
            if col_name not in test_clean.columns:
                continue
            pred_cells.add(tuple(cell))
    print(f"ğŸ“Š æœ‰æ•ˆé¢„æµ‹é”™è¯¯æ•°: {len(pred_cells)}")
    
    # å®é™…ä¸ºæ­£çš„cellé›†åˆ
    real_cells = set()
    exclude_cols = [col for col in test_clean.columns if "embed_cluster" in col or "img_category" in col]
    print(f"ğŸ” æ’é™¤çš„åˆ—ï¼ˆä¸åº”è¯¥æœ‰é”™è¯¯ï¼‰: {exclude_cols}")
    
    print(f"ğŸ” æ£€æµ‹å®é™…é”™è¯¯...")
    for i in tqdm(range(len(test_clean)), desc="æ£€æµ‹å®é™…é”™è¯¯", unit="è¡Œ"):
        for col in test_clean.columns:
            if col in exclude_cols:
                continue
            clean_val = test_clean.at[i, col]
            dirty_val = test_dirty.at[i, col]
            if pd.isna(clean_val) and pd.isna(dirty_val):
                continue
            elif pd.isna(clean_val) or pd.isna(dirty_val):
                real_cells.add((i, col))
            elif str(clean_val).strip() != str(dirty_val).strip():
                real_cells.add((i, col))
    
    # è®¡ç®—æŒ‡æ ‡
    TP = len(pred_cells & real_cells)
    FP = len(pred_cells - real_cells)
    FN = len(real_cells - pred_cells)
    total_cells = test_clean.shape[0] * test_clean.shape[1]
    TN = total_cells - TP - FP - FN
    precision = TP / (TP + FP) if (TP + FP) > 0 else 0
    recall = TP / (TP + FN) if (TP + FN) > 0 else 0
    f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0
    accuracy = (TP + TN) / total_cells
    
    print(f'Precision: {precision:.4f}')
    print(f'Recall: {recall:.4f}')
    print(f'F1: {f1:.4f}')
    print(f'Accuracy: {accuracy:.4f}')
    
    # ä¿å­˜è¯„ä¼°æŒ‡æ ‡
    with open(os.path.join(data_dir, 'dcr_rule_error_metrics.txt'), 'w', encoding='utf-8') as f:
        f.write(f'Precision: {precision:.4f}\n')
        f.write(f'Recall: {recall:.4f}\n')
        f.write(f'F1: {f1:.4f}\n')
        f.write(f'Accuracy: {accuracy:.4f}\n')
        f.write(f'\né”™è¯¯ç»Ÿè®¡:\n')
        f.write(f'æ€»å•å…ƒæ ¼æ•°: {total_cells}\n')
        f.write(f'å®é™…é”™è¯¯æ•°: {len(real_cells)}\n')
        f.write(f'é¢„æµ‹é”™è¯¯æ•°: {len(pred_cells)}\n')
        f.write(f'çœŸé˜³æ€§(TP): {TP}\n')
        f.write(f'å‡é˜³æ€§(FP): {FP}\n')
        f.write(f'å‡é˜´æ€§(FN): {FN}\n')
        f.write(f'çœŸé˜´æ€§(TN): {TN}\n')
    print('âœ… å·²ä¿å­˜æŸ¥é”™è¯„ä¼°æŒ‡æ ‡åˆ° dcr_rule_error_metrics.txt')

    print("ğŸ‰ RL-MCTSå¤šæ¨¡æ€Pipelineå®Œæˆï¼")

if __name__ == "__main__":
    main() 