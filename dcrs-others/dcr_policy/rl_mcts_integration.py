#!/usr/bin/env python3
# -*- coding: utf-8 -*-

import numpy as np
import torch
import random
import os
from tqdm import tqdm
from typing import List, Tuple, Dict, Any
from rl_policy_model import RLPolicyModel

# å¯¼å…¥çœŸå®çš„evaluate_ruleå‡½æ•°
from multimodal_dcrlearner_pipeline import evaluate_rule

def create_rl_training_data(df, predicates, enum_predicates, max_depth=6, n_episodes=1000):
    """åˆ›å»ºå¼ºåŒ–å­¦ä¹ è®­ç»ƒæ•°æ®"""
    print(f"ğŸ¯ åˆ›å»ºRLè®­ç»ƒæ•°æ®: {n_episodes}ä¸ªepisode")
    
    training_data = []
    feature_predicates = [p for p in predicates if p not in enum_predicates]
    
    for episode in tqdm(range(n_episodes), desc="ç”ŸæˆRLè®­ç»ƒæ•°æ®"):
        # éšæœºé€‰æ‹©èµ·å§‹çŠ¶æ€
        y_pred = random.choice(enum_predicates)
        current_predicates = [y_pred]
        
        # æ¨¡æ‹Ÿä¸€ä¸ªå®Œæ•´çš„è§„åˆ™æ„å»ºè¿‡ç¨‹
        episode_data = []
        unused = list(feature_predicates)
        
        for step in range(max_depth - 1):
            if not unused:
                break
            
            # å½“å‰çŠ¶æ€
            current_state = _create_state_features(current_predicates, df)
            
            # éšæœºé€‰æ‹©åŠ¨ä½œï¼ˆè°“è¯ï¼‰
            action_idx = random.randint(0, len(unused) - 1)
            chosen_pred = unused[action_idx]
            
            # æ‰§è¡ŒåŠ¨ä½œ
            next_predicates = current_predicates + [chosen_pred]
            unused.remove(chosen_pred)
            
            # è®¡ç®—å¥–åŠ±ï¼ˆåŸºäºè§„åˆ™è´¨é‡ï¼‰
            support, confidence = evaluate_rule(df, next_predicates)
            reward = support * confidence
            
            # å­˜å‚¨ç»éªŒ
            episode_data.append({
                'state': current_state,
                'action': action_idx,
                'reward': reward,
                'next_state': _create_state_features(next_predicates, df),
                'done': len(next_predicates) >= max_depth
            })
            
            current_predicates = next_predicates
        
        # å°†episodeæ•°æ®æ·»åŠ åˆ°è®­ç»ƒæ•°æ®
        training_data.extend(episode_data)
    
    print(f"âœ… ç”Ÿæˆ {len(training_data)} ä¸ªè®­ç»ƒæ ·æœ¬")
    return training_data

def _create_state_features(predicates, df):
    """åˆ›å»ºçŠ¶æ€ç‰¹å¾å‘é‡"""
    features = []
    
    # è§„åˆ™é•¿åº¦
    features.append(len(predicates))
    
    # è°“è¯ç±»å‹ç»Ÿè®¡
    numeric_count = sum(1 for p in predicates if any(op in p for op in ['>', '<', '>=']))
    categorical_count = sum(1 for p in predicates if '=' in p and not any(op in p for op in ['>', '<', '>=']))
    features.extend([numeric_count, categorical_count])
    
    # æ•°æ®ç»Ÿè®¡ç‰¹å¾ï¼ˆç®€åŒ–ï¼‰
    features.extend([len(df), len(df.columns), 0.5, 0.5])  # å›ºå®šå€¼
    
    # å¡«å……åˆ°64ç»´
    while len(features) < 64:
        features.append(0.0)
    features = features[:64]
    
    return np.array(features, dtype=np.float32)

# def evaluate_rule(df, predicates):
#     """è¯„ä¼°è§„åˆ™è´¨é‡"""
#     if not predicates or len(predicates) < 2:
#         return 0.0, 0.0
#     
#     # ç®€åŒ–çš„è§„åˆ™è¯„ä¼°
#     premise_preds = predicates[:-1]
#     conclusion_pred = predicates[-1]
#     
#     # è®¡ç®—æ”¯æŒåº¦å’Œç½®ä¿¡åº¦
#     support = 0.1  # ç®€åŒ–è®¡ç®—
#     confidence = 0.6  # ç®€åŒ–è®¡ç®—
#     
#     return support, confidence

def train_rl_policy_model(df, predicates, enum_predicates, algorithm='ppo', 
                         max_depth=6, n_episodes=1000, model_path=None):
    """è®­ç»ƒå¼ºåŒ–å­¦ä¹ ç­–ç•¥æ¨¡å‹"""
    print(f"ğŸ¯ å¼€å§‹è®­ç»ƒRLç­–ç•¥æ¨¡å‹: {algorithm}")
    
    # åˆ›å»ºRLæ¨¡å‹
    state_dim = 64
    action_dim = min(100, len(predicates))  # é™åˆ¶åŠ¨ä½œç©ºé—´
    rl_model = RLPolicyModel(
        algorithm=algorithm,
        state_dim=state_dim,
        action_dim=action_dim,
        hidden_dim=128,
        lr=3e-4
    )
    
    # ç”Ÿæˆè®­ç»ƒæ•°æ®
    training_data = create_rl_training_data(
        df, predicates, enum_predicates, max_depth, n_episodes
    )
    
    # è®­ç»ƒæ¨¡å‹
    rl_model.train(training_data)
    
    # ä¿å­˜æ¨¡å‹
    if model_path:
        rl_model.save_model(model_path)
    
    return rl_model

def rl_policy_based_rollout(node, feature_predicates, max_depth, df, rl_model, df_stats):
    """åŸºäºRLç­–ç•¥çš„rollout"""
    sim_preds = list(node.predicates)
    unused = list(set(feature_predicates) - set(sim_preds[1:]))
    
    # æ·»åŠ å®‰å…¨é™åˆ¶
    max_iterations = max_depth * 2
    iteration_count = 0
    
    while len(sim_preds) < max_depth and unused and iteration_count < max_iterations:
        iteration_count += 1
        
        try:
            if rl_model and rl_model.is_trained:
                # ä½¿ç”¨RLç­–ç•¥é€‰æ‹©ä¸‹ä¸€ä¸ªè°“è¯
                probs = rl_model.get_policy_probs(sim_preds, unused, df_stats)
                
                # æ·»åŠ æ¢ç´¢æ€§
                epsilon = 0.1  # æ¢ç´¢ç‡
                if random.random() < epsilon:
                    # éšæœºæ¢ç´¢
                    chosen_idx = random.randint(0, len(unused) - 1)
                else:
                    # åˆ©ç”¨ç­–ç•¥
                    chosen_idx = np.random.choice(len(unused), p=probs)
                
                chosen_pred = unused[chosen_idx]
            else:
                # éšæœºé€‰æ‹©
                chosen_pred = random.choice(unused)
            
            sim_preds.append(chosen_pred)
            unused = list(set(feature_predicates) - set(sim_preds[1:]))
            
        except Exception as e:
            print(f"âš ï¸ RL Rolloutå¤±è´¥: {e}ï¼Œä½¿ç”¨éšæœºé€‰æ‹©")
            chosen_pred = random.choice(unused)
            sim_preds.append(chosen_pred)
            unused = list(set(feature_predicates) - set(sim_preds[1:]))
    
    return sim_preds

def mcts_with_rl_policy(df, predicates, enum_predicates, rl_model=None, 
                       max_depth=6, n_iter=1000, c_param=1.4):
    """ä½¿ç”¨RLç­–ç•¥çš„MCTSè§„åˆ™å‘ç°"""
    if rl_model is not None:
        print(f"ğŸš€ ä½¿ç”¨RLç­–ç•¥çš„MCTSè§„åˆ™å‘ç°: {rl_model.algorithm}")
    else:
        print(f"ğŸš€ ä½¿ç”¨ä¼ ç»ŸMCTSè§„åˆ™å‘ç°")
    
    results = []
    feature_predicates = [p for p in predicates if p not in enum_predicates]
    df_stats = {'total_rows': len(df), 'num_columns': len(df.columns), 
                'avg_support': 0.3, 'avg_confidence': 0.6}
    
    for y_pred in tqdm(enum_predicates, desc="RL-MCTSè§„åˆ™å‘ç°"):
        root = MCTSNode([y_pred])
        best_support, best_confidence = 0, 0.0
        best_rule = []
        
        for _ in range(n_iter):
            node = root
            # Selection: ä½¿ç”¨UCB
            while node.children:
                node = node.best_child(c_param)
                if node is None:
                    break
            
            # Expansion: éšæœºé€‰æ‹©
            if not node.is_terminal(max_depth, feature_predicates):
                node.expand(feature_predicates)
                if node.children:
                    node = random.choice(node.children)
            
            # Simulation: ä½¿ç”¨RLç­–ç•¥
            sim_preds = rl_policy_based_rollout(
                node, feature_predicates, max_depth, df, rl_model, df_stats
            )
            
            # è¯„ä¼°è§„åˆ™
            support, confidence = evaluate_rule(df, [sim_preds[0]] + sim_preds[1:])
            reward = support * confidence
            
            # Backpropagation
            tmp_node = node
            while tmp_node:
                tmp_node.visits += 1
                tmp_node.value += reward
                tmp_node = tmp_node.parent
            
            if reward > best_support * best_confidence:
                best_support, best_confidence = support, confidence
                best_rule = list(sim_preds)
        
        results.append((y_pred, best_rule, best_support, best_confidence))
    
    return results

class MCTSNode:
    """MCTSèŠ‚ç‚¹ç±»"""
    def __init__(self, predicates, parent=None):
        self.predicates = predicates
        self.parent = parent
        self.children = []
        self.visits = 0
        self.value = 0.0
    
    def is_terminal(self, max_depth, all_predicates):
        return len(self.predicates) >= max_depth or len(set(all_predicates) - set(self.predicates)) == 0
    
    def expand(self, all_predicates):
        unused = list(set(all_predicates) - set(self.predicates))
        for p in unused:
            child = MCTSNode(self.predicates + [p], parent=self)
            self.children.append(child)
        return self.children
    
    def best_child(self, c_param=1.4):
        if not self.children:
            return None
        
        # UCBé€‰æ‹©
        choices_weights = []
        for child in self.children:
            exploitation = child.value / (child.visits + 1e-6)
            exploration = c_param * np.sqrt(np.log(self.visits + 1) / (child.visits + 1e-6))
            ucb_score = exploitation + exploration
            choices_weights.append(ucb_score)
        
        return self.children[np.argmax(choices_weights)]

def compare_rl_algorithms(df, predicates, enum_predicates, max_depth=6, n_iter=500):
    """æ¯”è¾ƒä¸åŒRLç®—æ³•çš„æ€§èƒ½"""
    print("ğŸ”¬ æ¯”è¾ƒä¸åŒRLç®—æ³•çš„æ€§èƒ½")
    
    algorithms = ['ppo', 'a2c', 'dqn']
    results = {}
    
    for algorithm in algorithms:
        print(f"\nğŸ“Š æµ‹è¯•ç®—æ³•: {algorithm}")
        
        # è®­ç»ƒæ¨¡å‹
        model_path = f"rl_model_{algorithm}.pth"
        rl_model = train_rl_policy_model(
            df, predicates, enum_predicates, algorithm, 
            max_depth, n_episodes=500, model_path=model_path
        )
        
        # è¿è¡ŒMCTS
        mcts_results = mcts_with_rl_policy(
            df, predicates, enum_predicates, rl_model, 
            max_depth, n_iter
        )
        
        # è¯„ä¼°ç»“æœ
        avg_support = np.mean([r[2] for r in mcts_results])
        avg_confidence = np.mean([r[3] for r in mcts_results])
        avg_quality = np.mean([r[2] * r[3] for r in mcts_results])
        
        results[algorithm] = {
            'avg_support': avg_support,
            'avg_confidence': avg_confidence,
            'avg_quality': avg_quality,
            'rules_count': len(mcts_results)
        }
        
        print(f"  {algorithm}: Support={avg_support:.3f}, Confidence={avg_confidence:.3f}, Quality={avg_quality:.3f}")
    
    # æ‰¾å‡ºæœ€ä½³ç®—æ³•
    best_algorithm = max(results.keys(), key=lambda k: results[k]['avg_quality'])
    print(f"\nğŸ† æœ€ä½³ç®—æ³•: {best_algorithm}")
    print(f"   è´¨é‡åˆ†æ•°: {results[best_algorithm]['avg_quality']:.3f}")
    
    return results

def online_rl_training(df, predicates, enum_predicates, rl_model, 
                      max_depth=6, n_iter=1000, update_frequency=100):
    """åœ¨çº¿RLè®­ç»ƒ"""
    print(f"ğŸ”„ åœ¨çº¿RLè®­ç»ƒ: æ¯{update_frequency}æ­¥æ›´æ–°ä¸€æ¬¡")
    
    results = []
    feature_predicates = [p for p in predicates if p not in enum_predicates]
    df_stats = {'total_rows': len(df), 'num_columns': len(df.columns), 
                'avg_support': 0.3, 'avg_confidence': 0.6}
    
    for y_pred in tqdm(enum_predicates, desc="åœ¨çº¿RLè®­ç»ƒ"):
        root = MCTSNode([y_pred])
        best_support, best_confidence = 0, 0.0
        best_rule = []
        
        for step in range(n_iter):
            node = root
            # Selection
            while node.children:
                node = node.best_child()
                if node is None:
                    break
            
            # Expansion
            if not node.is_terminal(max_depth, feature_predicates):
                node.expand(feature_predicates)
                if node.children:
                    node = random.choice(node.children)
            
            # Simulation with RL
            sim_preds = rl_policy_based_rollout(
                node, feature_predicates, max_depth, df, rl_model, df_stats
            )
            
            # è¯„ä¼°å¹¶å­˜å‚¨ç»éªŒ
            support, confidence = evaluate_rule(df, [sim_preds[0]] + sim_preds[1:])
            reward = support * confidence
            
            # å­˜å‚¨ç»éªŒç”¨äºåœ¨çº¿å­¦ä¹ 
            if rl_model and rl_model.is_trained:
                current_state = _create_state_features(node.predicates, df)
                next_state = _create_state_features(sim_preds, df)
                rl_model.store_experience(current_state, 0, reward, next_state, False)
            
            # Backpropagation
            tmp_node = node
            while tmp_node:
                tmp_node.visits += 1
                tmp_node.value += reward
                tmp_node = tmp_node.parent
            
            # åœ¨çº¿æ›´æ–°RLæ¨¡å‹
            if step % update_frequency == 0 and rl_model and rl_model.is_trained:
                rl_model.train_step()
            
            if reward > best_support * best_confidence:
                best_support, best_confidence = support, confidence
                best_rule = list(sim_preds)
        
        results.append((y_pred, best_rule, best_support, best_confidence))
    
    return results

# ä½¿ç”¨ç¤ºä¾‹
if __name__ == "__main__":
    # æ¨¡æ‹Ÿæ•°æ®
    import pandas as pd
    
    # åˆ›å»ºæ¨¡æ‹Ÿæ•°æ®
    np.random.seed(42)
    n_samples = 1000
    df = pd.DataFrame({
        'genre': np.random.choice(['Fiction', 'Non-Fiction', 'Mystery'], n_samples),
        'rating': np.random.randint(1, 6, n_samples),
        'language': np.random.choice(['English', 'Spanish', 'French'], n_samples),
        'format': np.random.choice(['Paperback', 'Hardcover', 'Ebook'], n_samples),
        'publisher': np.random.choice(['Penguin', 'Random House', 'HarperCollins'], n_samples)
    })
    
    # åˆ›å»ºè°“è¯
    predicates = [
        'genre = "Fiction"', 'genre = "Non-Fiction"', 'genre = "Mystery"',
        'rating > 3', 'rating > 4', 'rating = 5',
        'language = "English"', 'language = "Spanish"', 'language = "French"',
        'format = "Paperback"', 'format = "Hardcover"', 'format = "Ebook"',
        'publisher = "Penguin"', 'publisher = "Random House"', 'publisher = "HarperCollins"'
    ]
    
    enum_predicates = ['genre = "Fiction"', 'genre = "Non-Fiction"', 'rating > 4']
    
    print("ğŸš€ RL-MCTSé›†æˆæµ‹è¯•")
    
    # 1. æ¯”è¾ƒä¸åŒRLç®—æ³•
    results = compare_rl_algorithms(df, predicates, enum_predicates)
    
    # 2. ä½¿ç”¨æœ€ä½³ç®—æ³•è¿›è¡Œåœ¨çº¿è®­ç»ƒ
    best_algorithm = max(results.keys(), key=lambda k: results[k]['avg_quality'])
    rl_model = RLPolicyModel(algorithm=best_algorithm, state_dim=64, action_dim=100)
    
    online_results = online_rl_training(df, predicates, enum_predicates, rl_model)
    
    print(f"\nâœ… åœ¨çº¿è®­ç»ƒå®Œæˆï¼Œå‘ç° {len(online_results)} ä¸ªè§„åˆ™")
    avg_quality = np.mean([r[2] * r[3] for r in online_results])
    print(f"   å¹³å‡è´¨é‡: {avg_quality:.3f}") 