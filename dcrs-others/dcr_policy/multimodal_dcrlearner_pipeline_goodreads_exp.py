#!/usr/bin/env python3
# -*- coding: utf-8 -*-


import os
# è®¾ç½®åªä½¿ç”¨5å·GPUå¡
os.environ['CUDA_VISIBLE_DEVICES'] = '0'

import sys
import json
import random
import pickle
import numpy as np
import pandas as pd
import torch
import torch.nn as nn
from torch.utils.data import DataLoader
from transformers import AutoTokenizer, AutoModel, AutoProcessor
from PIL import Image
import torchvision.transforms as transforms
from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score
from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier
from sklearn.preprocessing import StandardScaler
import joblib
from tqdm import tqdm
import warnings
warnings.filterwarnings('ignore')
import math
import re

# æ·»åŠ åˆ†å¸ƒå¼è®¡ç®—æ”¯æŒ
import multiprocessing as mp
from concurrent.futures import ProcessPoolExecutor, ThreadPoolExecutor, as_completed
from functools import partial
import time
from typing import List, Tuple, Dict, Any

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
current_dir = os.path.dirname(os.path.abspath(__file__))
project_root = os.path.dirname(os.path.dirname(os.path.dirname(os.path.dirname(current_dir))))
sys.path.insert(0, project_root)


class MultimodalModels:
    """å¤šæ¨¡æ€æ¨¡å‹é›†åˆ - External Models Mğ‘ˆ"""
    
    def __init__(self, device='cuda'):
        self.device = torch.device(device if torch.cuda.is_available() else 'cpu')
        self._init_models()
    
    def _init_models(self):
        """åˆå§‹åŒ–å¤šæ¨¡æ€æ¨¡å‹"""
        print("ğŸ”§ Initializing Multimodal Models Mğ‘ˆ...")
        
        # æ–‡æ¡£ç±»æ¨¡å‹
        self.bert_mrc = self._create_bert_mrc()
        
        # å¤šæ¨¡æ€æ¨¡å‹
        self.qwen_vl = self._create_qwen_vl()

        
        print("âœ… Multimodal Models Mğ‘ˆ initialized")
    
    def _create_bert_mrc(self):
        """åˆ›å»ºBert-MRC [43] - å®ä½“æå–æ¨¡å‹ï¼Œä½¿ç”¨BGE-M3"""
        # ä½¿ç”¨æœ¬åœ°BGE-M3æ¨¡å‹
        model_path = "/data_nas/model_hub/bge-m3"
        
        print(f"ğŸ“ Loading BGE-M3 from local path: {model_path}")
        self.bert_tokenizer = AutoTokenizer.from_pretrained(model_path, local_files_only=True)
        self.bert_model = AutoModel.from_pretrained(model_path, local_files_only=True)
        self.bert_model.to(self.device)
        self.bert_model.eval()
        print("âœ… BGE-M3 [43] initialized")
        return True
    

    
    def _create_qwen_vl(self):
        """åˆ›å»ºQwen-2.5-VL [73] - å›¾åƒ/è§†é¢‘å¤„ç†æ¨¡å‹ï¼Œä½¿ç”¨Qwen2.5-VL-7B-Instruct"""
        # ä½¿ç”¨æœ¬åœ°Qwen2.5-VL-7B-Instructæ¨¡å‹
        model_path = "/data_nas/model_hub/Qwen2.5-VL-7B-Instruct"
        
        print(f"ğŸ“ Loading Qwen2.5-VL-7B-Instruct from local path: {model_path}")
        self.qwen_processor = AutoProcessor.from_pretrained(model_path, local_files_only=True)
        
        # åŠ è½½æ”¯æŒç”Ÿæˆçš„æ¨¡å‹ç±»
        from transformers import Qwen2_5_VLForConditionalGeneration
        self.qwen_model = Qwen2_5_VLForConditionalGeneration.from_pretrained(model_path, local_files_only=True)
        self.qwen_model.to(self.device)
        self.qwen_model.eval()
        print("âœ… Qwen2.5-VL-7B-Instruct [73] initialized (with generation support)")
        return True
    
    def _load_and_preprocess_image(self, image_path):
        """åŠ è½½å’Œé¢„å¤„ç†å›¾åƒ"""
        try:
            from PIL import Image
            import torchvision.transforms as transforms
            
            # åŠ è½½å›¾åƒ
            image = Image.open(image_path).convert('RGB')
            
            # å®šä¹‰é¢„å¤„ç†å˜æ¢
            transform = transforms.Compose([
                transforms.Resize((224, 224)),  # è°ƒæ•´å¤§å°
                transforms.ToTensor(),  # è½¬æ¢ä¸ºå¼ é‡
                transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])  # æ ‡å‡†åŒ–
            ])
            
            # åº”ç”¨å˜æ¢
            image_tensor = transform(image)
            
            return image_tensor
            
        except Exception as e:
            print(f"âŒ Error loading image {image_path}: {e}")
            # è¿”å›ä¸€ä¸ªé»˜è®¤çš„å›¾åƒå¼ é‡
            return torch.zeros(3, 224, 224)
    
    def extract_image_features(self, image_path):
        """ä»…æå–å›¾åƒç‰¹å¾embeddingï¼Œä¿ç•™åŸå§‹ç»´åº¦"""
        if not os.path.exists(image_path):
            print(f"âŒ Image file not found: {image_path}")
            return None
        try:
            from PIL import Image
            image = Image.open(image_path).convert('RGB')
            print(f"âœ… Loaded image: {image.size}")
            with torch.no_grad():
                prompt = "Describe this image."
                inputs = self.qwen_processor(
                    text=prompt,
                    images=image, 
                    return_tensors="pt"
                )
                if 'pixel_values' in inputs:
                    inputs['images'] = inputs.pop('pixel_values')
                model_inputs = {}
                for key in ['input_ids', 'attention_mask', 'pixel_values']:
                    if key in inputs:
                        model_inputs[key] = inputs[key]
                model_inputs = {k: v.to(self.device) for k, v in model_inputs.items()}
                outputs = self.qwen_model(**model_inputs)
                if hasattr(outputs, 'image_embeds'):
                    img_feat = outputs.image_embeds
                elif hasattr(outputs, 'last_hidden_state'):
                    img_feat = outputs.last_hidden_state[:, 0, :]
                elif hasattr(outputs, 'logits'):
                    img_feat = outputs.logits.mean(dim=1)
                else:
                    img_feat = torch.zeros(1, 1).to(self.device)
                return img_feat
        except Exception as e:
            print(f"âŒ Error extracting image features from {image_path}: {e}")
            import traceback
            traceback.print_exc()
            return None

    def extract_image_category(self, image_path):
        """ä»…è¯†åˆ«å›¾åƒç±»åˆ«æˆ–æ‰€å±label"""
        if not os.path.exists(image_path):
            print(f"âŒ Image file not found: {image_path}")
            return None
        try:
            from PIL import Image
            image = Image.open(image_path).convert('RGB')
            print(f"âœ… Loaded image: {image.size}")
            with torch.no_grad():
                category_prompt = "è¯·è¯†åˆ«å›¾ç‰‡çš„ä¸»è¦ç±»åˆ«æˆ–æ ‡ç­¾"
                text_inputs = self.qwen_processor(
                    text=category_prompt,
                    images=image,
                    return_tensors="pt"
                )
                if 'pixel_values' in text_inputs:
                    text_inputs['images'] = text_inputs.pop('pixel_values')
                model_text_inputs = {}
                for key in ['input_ids', 'attention_mask', 'pixel_values']:
                    if key in text_inputs:
                        model_text_inputs[key] = text_inputs[key]
                model_text_inputs = {k: v.to(self.device) for k, v in model_text_inputs.items()}
                generated_ids = self.qwen_model.generate(
                    **model_text_inputs,
                    max_new_tokens=20,
                    do_sample=True,
                    temperature=0.3,
                    pad_token_id=self.qwen_processor.tokenizer.eos_token_id
                )
                category = self.qwen_processor.tokenizer.decode(generated_ids[0], skip_special_tokens=True)
                category = category.replace(category_prompt, "").strip()
                if not category or len(category) < 2:
                    category = "æœªçŸ¥ç±»åˆ«"
                return category
        except Exception as e:
            print(f"âŒ Error extracting image category from {image_path}: {e}")
            import traceback
            traceback.print_exc()
            return None

    
    def extract_text_features(self, text):
        """æå–å•ä¸ªæ–‡æœ¬çš„embeddingï¼Œä¿ç•™åŸå§‹ç»´åº¦"""
        if not text or not isinstance(text, str):
            return torch.zeros(1).to(self.device)
        inputs = self.bert_tokenizer(text, return_tensors="pt", padding=True, truncation=True, max_length=512)
        inputs = {k: v.to(self.device) for k, v in inputs.items()}
        with torch.no_grad():
            outputs = self.bert_model(**inputs)
            text_feat = outputs.last_hidden_state[:, 0, :]  # [1, hidden_size]
            return text_feat.squeeze(0)
    

class PredicateConstructor:
    def __init__(self, csv_path):
        self.df = pd.read_csv(csv_path)
        self.col_types = self._infer_col_types()
        self.row_ids = [f"t{i}" for i in range(len(self.df))]

    def _infer_col_types(self):
        col_types = {}
        for col in self.df.columns:
            if "img_path" in col:
                col_types[col] = "img_path"
            elif self.df[col].dtype in ['float64', 'int64']:
                col_types[col] = "numeric"
            elif self.df[col].dtype == 'object':
                unique_count = self.df[col].nunique()
                avg_len = self.df[col].astype(str).apply(len).mean()
                if unique_count < 20:
                    col_types[col] = "enum"
                elif avg_len > 20:
                    col_types[col] = "text"
                else:
                    col_types[col] = "enum"
            else:
                col_types[col] = "other"
        return col_types

    def construct_predicates(self):
        predicates = []
        for col, typ in self.col_types.items():
            if typ == "enum":
                try:
                    values = self.df[col].dropna().astype(str)
                    if len(values) == 0:
                        continue
                    # å¢åŠ è°“è¯å¤šæ ·æ€§ï¼Œä½†ä¿æŒç²¾ç¡®æ€§
                    top_modes = values.value_counts().index[:5]  # å¢åŠ æ›´å¤šä¼—æ•°
                    for val in top_modes:
                        predicates.append(f'{col} = "{val}"')

                except Exception as e:
                    print(f"âš ï¸ æšä¸¾å‹åˆ— {col} æ„é€ è°“è¯å‡ºé”™: {e}")
            elif typ == "numeric" and str(self.df[col].dtype).startswith("int"):
                try:
                    values = self.df[col].dropna().astype(int)
                    if len(values) == 0:
                        continue
                    mean_val = int(values.mean())
                    min_val = int(values.min())
                    max_val = int(values.max())
                    median_val = int(values.median())
                    q25_val = int(values.quantile(0.25))
                    q75_val = int(values.quantile(0.75))
                    for val in set([mean_val, min_val, max_val, median_val, q25_val, q75_val]):
                        predicates.append(f'{col} = {val}')
                        # predicates.append(f'{col} != {val}')  # æ³¨é‡Šæ‰ä¸ç­‰äºè°“è¯
                        predicates.append(f'{col} > {val}')   # æ³¨é‡Šæ‰å¤§äºè°“è¯
                        predicates.append(f'{col} < {val}')   # æ³¨é‡Šæ‰å°äºè°“è¯
                except Exception as e:
                    print(f"âš ï¸ intæ•°å€¼å‹åˆ— {col} æ„é€ è°“è¯å‡ºé”™: {e}")
            elif typ == "numeric" and str(self.df[col].dtype).startswith("float"):
                try:
                    values = self.df[col].dropna().astype(float)
                    if len(values) == 0:
                        continue
                    mean_val = float(values.mean())
                    min_val = float(values.min())
                    max_val = float(values.max())
                    median_val = float(values.median())
                    q25_val = float(values.quantile(0.25))
                    q75_val = float(values.quantile(0.75))
                    for val in [mean_val, min_val, max_val, median_val, q25_val, q75_val]:
                        predicates.append(f'{col} = {val}')  # é‡æ–°å¯ç”¨floatåˆ—çš„ = è°“è¯
                        predicates.append(f'{col} > {val}')   # æ³¨é‡Šæ‰å¤§äºè°“è¯
                        predicates.append(f'{col} < {val}')   # æ³¨é‡Šæ‰å°äºè°“è¯
                except Exception as e:
                    print(f"âš ï¸ floatæ•°å€¼å‹åˆ— {col} æ„é€ è°“è¯å‡ºé”™: {e}")
            # ä¸ºembeddingèšç±»åˆ—æ„é€ è°“è¯
            elif "embed_cluster" in col or "img_category" in col:
                try:
                    values = self.df[col].dropna()
                    if len(values) == 0:
                        continue
                    unique_vals = values.unique()
                    for val in unique_vals[:10]:  # é™åˆ¶æœ€å¤š10ä¸ªèšç±»
                        if pd.notna(val):
                            predicates.append(f'{col} = {val}')
                except Exception as e:
                    print(f"âš ï¸ èšç±»åˆ— {col} æ„é€ è°“è¯å‡ºé”™: {e}")
        return predicates


class ValuePolicyModel:
    """ä»·å€¼ç­–ç•¥æ¨¡å‹ï¼Œç”¨äºMCTSçš„policy-based rollout"""
    
    def __init__(self, model_type='random_forest'):
        self.model_type = model_type
        self.policy_model = None  # ç­–ç•¥æ¨¡å‹ï¼šé¢„æµ‹é€‰æ‹©æ¯ä¸ªè°“è¯çš„æ¦‚ç‡
        self.value_model = None   # ä»·å€¼æ¨¡å‹ï¼šé¢„æµ‹çŠ¶æ€çš„ä»·å€¼
        self.scaler = StandardScaler()
        self.is_trained = False
        
    def _create_feature_vector(self, current_predicates, available_predicates, df_stats):
        """åˆ›å»ºç‰¹å¾å‘é‡"""
        features = []
        
        # å½“å‰è°“è¯ç»„åˆçš„ç‰¹å¾
        features.append(len(current_predicates))  # å½“å‰è§„åˆ™é•¿åº¦
        
        # å¯ç”¨è°“è¯çš„ç‰¹å¾
        features.append(len(available_predicates))  # å¯ç”¨è°“è¯æ•°é‡
        
        # æ•°æ®ç»Ÿè®¡ç‰¹å¾
        features.extend([
            df_stats.get('total_rows', 0),
            df_stats.get('num_columns', 0),
            df_stats.get('avg_support', 0.0),
            df_stats.get('avg_confidence', 0.0)
        ])
        
        # è°“è¯ç±»å‹ç‰¹å¾ï¼ˆç®€åŒ–ç‰ˆï¼‰
        numeric_count = sum(1 for p in available_predicates if any(op in p for op in ['>', '<', '>=']))
        categorical_count = sum(1 for p in available_predicates if '=' in p and not any(op in p for op in ['>', '<', '>=']))
        features.extend([numeric_count, categorical_count])
        
        return np.array(features)
    
    def _create_state_features(self, current_predicates, df_stats):
        """åˆ›å»ºçŠ¶æ€ç‰¹å¾å‘é‡"""
        features = []
        
        # å½“å‰è§„åˆ™ç‰¹å¾
        features.append(len(current_predicates))
        
        # æ•°æ®ç»Ÿè®¡ç‰¹å¾
        features.extend([
            df_stats.get('total_rows', 0),
            df_stats.get('num_columns', 0),
            df_stats.get('avg_support', 0.0),
            df_stats.get('avg_confidence', 0.0)
        ])
        
        return np.array(features)
    
    def train(self, training_data):
        """è®­ç»ƒç­–ç•¥å’Œä»·å€¼æ¨¡å‹
        
        Args:
            training_data: [(state_features, action_features, reward, next_state_features), ...]
        """
        if not training_data:
            print("âš ï¸ æ²¡æœ‰è®­ç»ƒæ•°æ®ï¼Œä½¿ç”¨éšæœºç­–ç•¥")
            return
            
        # åˆ†ç¦»æ•°æ®
        state_features = []
        action_features = []
        rewards = []
        next_state_features = []
        
        for state_feat, action_feat, reward, next_state_feat in training_data:
            state_features.append(state_feat)
            action_features.append(action_feat)
            rewards.append(reward)
            next_state_features.append(next_state_feat)
        
        # æ ‡å‡†åŒ–ç‰¹å¾
        state_features = np.array(state_features)
        action_features = np.array(action_features)
        next_state_features = np.array(next_state_features)
        
        # è®­ç»ƒç­–ç•¥æ¨¡å‹ï¼ˆå›å½’å™¨ï¼‰
        if self.model_type == 'random_forest':
            self.policy_model = RandomForestRegressor(n_estimators=100, random_state=42)
            # ä½¿ç”¨rewardä½œä¸ºç›®æ ‡å€¼ï¼Œè®­ç»ƒä¸€ä¸ªå›å½’æ¨¡å‹æ¥é¢„æµ‹åŠ¨ä½œä»·å€¼
            if len(action_features) > 0:
                self.policy_model.fit(action_features, rewards)
            else:
                # å¦‚æœæ²¡æœ‰æœ‰æ•ˆæ•°æ®ï¼Œåˆ›å»ºä¸€ä¸ªç®€å•çš„æ¨¡å‹
                self.policy_model = RandomForestRegressor(n_estimators=10, random_state=42)
                dummy_features = np.zeros((1, 10))
                dummy_labels = np.array([0.0])
                self.policy_model.fit(dummy_features, dummy_labels)
        
        # è®­ç»ƒä»·å€¼æ¨¡å‹ï¼ˆå›å½’å™¨ï¼‰
        if self.model_type == 'random_forest':
            self.value_model = RandomForestRegressor(n_estimators=100, random_state=42)
            self.value_model.fit(next_state_features, rewards)
        
        self.is_trained = True
        print(f"âœ… ç­–ç•¥å’Œä»·å€¼æ¨¡å‹è®­ç»ƒå®Œæˆï¼Œä½¿ç”¨ {len(training_data)} ä¸ªæ ·æœ¬")
    
    def get_policy_probs(self, current_predicates, available_predicates, df_stats):
        """è·å–ç­–ç•¥æ¦‚ç‡åˆ†å¸ƒ"""
        if not self.is_trained or not available_predicates:
            # è¿”å›å‡åŒ€åˆ†å¸ƒ
            n_actions = len(available_predicates)
            return np.ones(n_actions) / n_actions
        
        # ç®€åŒ–ç­–ç•¥ï¼šç›´æ¥è¿”å›å‡åŒ€åˆ†å¸ƒï¼Œé¿å…å¤æ‚çš„ç‰¹å¾è®¡ç®—
        # åŸæ¥çš„æ–¹æ³•ä¼šå¯¼è‡´ç‰¹å¾é‡å¤è®¡ç®—å’Œå¯èƒ½çš„æ— é™å¾ªç¯
        try:
            # ä½¿ç”¨ç®€å•çš„å¯å‘å¼ç­–ç•¥
            probs = np.ones(len(available_predicates))
            
            # æ ¹æ®è°“è¯ç±»å‹ç»™äºˆä¸åŒæƒé‡
            for i, pred in enumerate(available_predicates):
                # æ•°å€¼å‹è°“è¯æƒé‡ç¨é«˜
                if any(op in pred for op in ['>', '<', '>=']):
                    probs[i] *= 1.2
                # åˆ†ç±»å‹è°“è¯æƒé‡é€‚ä¸­
                elif '=' in pred:
                    probs[i] *= 1.0
                # å…¶ä»–è°“è¯æƒé‡ç¨ä½
                else:
                    probs[i] *= 0.8
            
            # å½’ä¸€åŒ–
            probs = probs / np.sum(probs)
            return probs
            
        except Exception as e:
            # å¦‚æœè®¡ç®—å¤±è´¥ï¼Œè¿”å›å‡åŒ€åˆ†å¸ƒ
            print(f"âš ï¸ ç­–ç•¥è®¡ç®—å¤±è´¥: {e}ï¼Œä½¿ç”¨å‡åŒ€åˆ†å¸ƒ")
            return np.ones(len(available_predicates)) / len(available_predicates)
    
    def get_value(self, current_predicates, df_stats):
        """è·å–çŠ¶æ€ä»·å€¼ä¼°è®¡"""
        if not self.is_trained:
            return 0.0
        
        try:
            # ç®€åŒ–ä»·å€¼ä¼°è®¡ï¼šåŸºäºè§„åˆ™é•¿åº¦å’Œè°“è¯ç±»å‹
            rule_length = len(current_predicates)
            
            # åŸºç¡€ä»·å€¼ï¼šè§„åˆ™è¶Šé•¿ä»·å€¼è¶Šé«˜ï¼ˆä½†æœ‰é™åˆ¶ï¼‰
            base_value = min(0.5, rule_length * 0.1)
            
            # æ ¹æ®è°“è¯ç±»å‹è°ƒæ•´ä»·å€¼
            type_bonus = 0.0
            for pred in current_predicates:
                if any(op in pred for op in ['>', '<', '>=']):
                    type_bonus += 0.05  # æ•°å€¼å‹è°“è¯åŠ åˆ†
                elif '=' in pred:
                    type_bonus += 0.03  # åˆ†ç±»å‹è°“è¯åŠ åˆ†
            
            total_value = min(1.0, base_value + type_bonus)
            return total_value
            
        except Exception as e:
            print(f"âš ï¸ ä»·å€¼è®¡ç®—å¤±è´¥: {e}ï¼Œè¿”å›é»˜è®¤å€¼")
            return 0.0
    
    def save_model(self, filepath):
        """ä¿å­˜æ¨¡å‹"""
        if self.is_trained:
            model_data = {
                'policy_model': self.policy_model,
                'value_model': self.value_model,
                'model_type': self.model_type,
                'is_trained': self.is_trained
            }
            joblib.dump(model_data, filepath)
            print(f"âœ… æ¨¡å‹å·²ä¿å­˜åˆ°: {filepath}")
    
    def load_model(self, filepath):
        """åŠ è½½æ¨¡å‹"""
        try:
            model_data = joblib.load(filepath)
            self.policy_model = model_data['policy_model']
            self.value_model = model_data['value_model']
            self.model_type = model_data['model_type']
            self.is_trained = model_data['is_trained']
            print(f"âœ… æ¨¡å‹å·²ä» {filepath} åŠ è½½")
        except Exception as e:
            print(f"âŒ æ¨¡å‹åŠ è½½å¤±è´¥: {e}")


class MCTSNode:
    def __init__(self, predicates, parent=None):
        self.predicates = predicates  # å½“å‰èŠ‚ç‚¹çš„è°“è¯ç»„åˆï¼ˆlistï¼‰
        self.parent = parent
        self.children = []
        self.visits = 0
        self.value = 0.0
        self.prior_value = 0.0  # å…ˆéªŒä»·å€¼ï¼ˆæ¥è‡ªç­–ç•¥æ¨¡å‹ï¼‰

    def is_terminal(self, max_depth, all_predicates):
        return len(self.predicates) >= max_depth or len(set(all_predicates) - set(self.predicates)) == 0

    def expand(self, all_predicates):
        unused = list(set(all_predicates) - set(self.predicates))
        for p in unused:
            child = MCTSNode(self.predicates + [p], parent=self)
            self.children.append(child)
        return self.children

    def best_child(self, c_param=1.4, alpha=1.0):
        import numpy as np
        if not self.children:
            return None
            
        # ä½¿ç”¨UCBè¿›è¡ŒèŠ‚ç‚¹é€‰æ‹©ï¼Œalphaå‚æ•°å½±å“æ¢ç´¢é¡¹
        choices_weights = []
        for child in self.children:
            # UCBå…¬å¼ï¼šexploitation + alpha * exploration
            exploitation = child.value / (child.visits + 1e-6)
            exploration = c_param * math.sqrt(math.log(self.visits + 1) / (child.visits + 1e-6))
            ucb_score = exploitation + alpha * exploration
            choices_weights.append(ucb_score)
        
        return self.children[np.argmax(choices_weights)]

# ä¿®æ”¹predicate_maskæ”¯æŒcol op valæ ¼å¼

def predicate_mask(df, pred):
    import re
    import numpy as np
    m = re.match(r'(\w+)\s*([=!<>]+)\s*(.+)', pred)
    if not m:
        return np.ones(len(df), dtype=bool)
    col, op, val = m.groups()
    val = val.strip('"')
    if col not in df.columns:
        return np.ones(len(df), dtype=bool)
    if op == '=':
        return df[col].astype(str) == val
    elif op == '!=':
        return df[col].astype(str) != val
    elif op == '>':
        return df[col].astype(float) > float(val)
    elif op == '<':
        return df[col].astype(float) < float(val)
    else:
        return np.ones(len(df), dtype=bool)

def evaluate_rule(df, predicates, target_col='y'):
    import numpy as np
    if not predicates or len(predicates) < 2:
        return 0, 0.0
    premise_preds = predicates[:-1]
    conclusion_pred = predicates[-1]
    # å‰æmask
    mask = np.ones(len(df), dtype=bool)
    for pred in premise_preds:
        mask = mask & predicate_mask(df, pred)
    support_count = mask.sum()
    support = support_count / len(df) if len(df) > 0 else 0
    if support_count == 0:
        return 0, 0.0
    # ç»“è®ºmask
    mask_conclusion = mask & predicate_mask(df, conclusion_pred)
    confidence = mask_conclusion.sum() / support_count
    return support, confidence


def get_df_stats(df, predicates):
    """è·å–æ•°æ®ç»Ÿè®¡ä¿¡æ¯ï¼Œç”¨äºç­–ç•¥æ¨¡å‹ç‰¹å¾"""
    stats = {
        'total_rows': len(df),
        'num_columns': len(df.columns),
        'avg_support': 0.0,
        'avg_confidence': 0.0
    }
    
    # è®¡ç®—å¹³å‡æ”¯æŒåº¦å’Œç½®ä¿¡åº¦
    total_support = 0.0
    total_confidence = 0.0
    count = 0
    
    for pred in predicates[:min(100, len(predicates))]:  # é™åˆ¶è®¡ç®—é‡
        try:
            support, confidence = evaluate_rule(df, [pred])
            total_support += support
            total_confidence += confidence
            count += 1
        except:
            continue
    
    if count > 0:
        stats['avg_support'] = total_support / count
        stats['avg_confidence'] = total_confidence / count
    
    return stats

def policy_based_rollout(node, feature_predicates, max_depth, df, policy_model, df_stats):
    """åŸºäºç­–ç•¥çš„rollout"""
    sim_preds = list(node.predicates)
    unused = list(set(feature_predicates) - set(sim_preds[1:]))
    
    # æ·»åŠ å®‰å…¨é™åˆ¶ï¼Œé¿å…æ— é™å¾ªç¯
    max_iterations = max_depth * 2
    iteration_count = 0
    
    while len(sim_preds) < max_depth and unused and iteration_count < max_iterations:
        iteration_count += 1
        
        try:
            if policy_model and policy_model.is_trained:
                # ä½¿ç”¨ç­–ç•¥æ¨¡å‹é€‰æ‹©ä¸‹ä¸€ä¸ªè°“è¯
                probs = policy_model.get_policy_probs(sim_preds, unused, df_stats)
                # æ·»åŠ ä¸€äº›éšæœºæ€§
                probs = probs * 0.8 + np.random.uniform(0, 0.2, len(probs))
                probs = probs / probs.sum()  # é‡æ–°å½’ä¸€åŒ–
                
                chosen_idx = np.random.choice(len(unused), p=probs)
                chosen_pred = unused[chosen_idx]
            else:
                # éšæœºé€‰æ‹©
                chosen_pred = random.choice(unused)
            
            sim_preds.append(chosen_pred)
            unused = list(set(feature_predicates) - set(sim_preds[1:]))
            
        except Exception as e:
            # å¦‚æœç­–ç•¥é€‰æ‹©å¤±è´¥ï¼Œä½¿ç”¨éšæœºé€‰æ‹©
            print(f"âš ï¸ Rolloutç­–ç•¥é€‰æ‹©å¤±è´¥: {e}ï¼Œä½¿ç”¨éšæœºé€‰æ‹©")
            chosen_pred = random.choice(unused)
            sim_preds.append(chosen_pred)
            unused = list(set(feature_predicates) - set(sim_preds[1:]))
    
    return sim_preds


def mcts_rule_discovery(df, predicates, enum_predicates, max_depth=3, n_iter=100, 
                       use_policy=True, policy_model=None, c_param=1.4):
    """å¢å¼ºçš„MCTSè§„åˆ™å‘ç°ï¼Œæ”¯æŒç­–ç•¥æ¨¡å‹"""
    results = []
    feature_predicates = [p for p in predicates if p not in enum_predicates]
    
    # è·å–æ•°æ®ç»Ÿè®¡ä¿¡æ¯
    df_stats = get_df_stats(df, predicates)
    
    for y_pred in tqdm(enum_predicates, desc="MCTSè§„åˆ™å‘ç°", unit="y_pred"):
        root = MCTSNode([y_pred])
        best_support, best_confidence = 0, 0.0
        best_rule = []
        
        for _ in range(n_iter):
            node = root
            # Selection
            while node.children:
                node = node.best_child(c_param)
                if node is None:
                    break
            
            # Expansion
            if not node.is_terminal(max_depth, feature_predicates):
                node.expand(feature_predicates)
                if node.children:
                    # ä¸ºå­èŠ‚ç‚¹è®¾ç½®å…ˆéªŒä»·å€¼
                    if policy_model and policy_model.is_trained:
                        for child in node.children:
                            child.prior_value = policy_model.get_value(child.predicates, df_stats)
                    
                    # é€‰æ‹©æ–°æ‰©å±•çš„èŠ‚ç‚¹
                    if use_policy and policy_model and policy_model.is_trained:
                        # åŸºäºç­–ç•¥é€‰æ‹©
                        probs = policy_model.get_policy_probs(node.predicates, 
                                                            [c.predicates[-1] for c in node.children], df_stats)
                        node = np.random.choice(node.children, p=probs)
                    else:
                        # éšæœºé€‰æ‹©
                        node = random.choice(node.children)
            
            # Simulation (Policy-based rollout)
            sim_preds = policy_based_rollout(node, feature_predicates, max_depth, df, policy_model, df_stats)
            
            # è¯„ä¼°ï¼šå‰æ=sim_preds[1:], ç»“è®º=sim_preds[0]
            support, confidence = evaluate_rule(df, [sim_preds[0]] + sim_preds[1:])
            reward = support * confidence
            
            # Backpropagation
            tmp_node = node
            while tmp_node:
                tmp_node.visits += 1
                tmp_node.value += reward
                tmp_node = tmp_node.parent
            
            if reward > best_support * best_confidence:
                best_support, best_confidence = support, confidence
                best_rule = list(sim_preds)
        
        results.append((y_pred, best_rule, best_support, best_confidence))
    
    return results

def mcts_rule_discovery_single_y_pred(args):
    """å•ä¸ªy_predçš„MCTSè§„åˆ™å‘ç°ï¼ˆç”¨äºå¹¶è¡Œè®¡ç®—ï¼‰"""
    if len(args) == 5:
        # å…¼å®¹æ—§ç‰ˆæœ¬è°ƒç”¨
        df, predicates, y_pred, max_depth, n_iter = args
        use_policy = False
        policy_model = None
        c_param = 1.4
        alpha = 1.0
    elif len(args) == 8:
        # æ–°ç‰ˆæœ¬è°ƒç”¨ï¼Œæ”¯æŒç­–ç•¥æ¨¡å‹
        df, predicates, y_pred, max_depth, n_iter, use_policy, policy_model, c_param = args
        alpha = 1.0
    else:
        # æœ€æ–°ç‰ˆæœ¬è°ƒç”¨ï¼Œæ”¯æŒalphaå‚æ•°
        df, predicates, y_pred, max_depth, n_iter, use_policy, policy_model, c_param, alpha = args
    
    feature_predicates = [p for p in predicates if p not in [y_pred]]
    
    # è·å–æ•°æ®ç»Ÿè®¡ä¿¡æ¯
    df_stats = get_df_stats(df, predicates)
    
    root = MCTSNode([y_pred])
    best_support, best_confidence = 0, 0.0
    best_rule = []
    
    for _ in range(n_iter):
        node = root
        # Selection
        while node.children:
            node = node.best_child(c_param, alpha)
            if node is None:
                break
        
        # Expansion
        if not node.is_terminal(max_depth, feature_predicates):
            node.expand(feature_predicates)
            if node.children:
                # ä¸ºå­èŠ‚ç‚¹è®¾ç½®å…ˆéªŒä»·å€¼
                if policy_model and policy_model.is_trained:
                    for child in node.children:
                        child.prior_value = policy_model.get_value(child.predicates, df_stats)
                
                # é€‰æ‹©æ–°æ‰©å±•çš„èŠ‚ç‚¹
                if use_policy and policy_model and policy_model.is_trained:
                    # åŸºäºç­–ç•¥é€‰æ‹©
                    probs = policy_model.get_policy_probs(node.predicates, 
                                                        [c.predicates[-1] for c in node.children], df_stats)
                    node = np.random.choice(node.children, p=probs)
                else:
                    # éšæœºé€‰æ‹©
                    node = random.choice(node.children)
        
        # Simulation (Policy-based rollout)
        sim_preds = policy_based_rollout(node, feature_predicates, max_depth, df, policy_model, df_stats)
        
        # è¯„ä¼°ï¼šå‰æ=sim_preds[1:], ç»“è®º=sim_preds[0]
        support, confidence = evaluate_rule(df, [sim_preds[0]] + sim_preds[1:])
        reward = support * confidence
        
        # Backpropagation
        tmp_node = node
        while tmp_node:
            tmp_node.visits += 1
            tmp_node.value += reward
            tmp_node = tmp_node.parent
        
        if reward > best_support * best_confidence:
            best_support, best_confidence = support, confidence
            best_rule = list(sim_preds)
    
    return (y_pred, best_rule, best_support, best_confidence)

def mcts_rule_discovery_yroot(df, predicates, enum_predicates, max_depth=3, n_iter=100, n_workers=None, use_parallel=True, 
                             use_policy=True, policy_model=None, c_param=1.4, alpha=1.0):
    """
    åˆ†å¸ƒå¼MCTSè§„åˆ™å‘ç°ï¼ˆæ”¯æŒç­–ç•¥æ¨¡å‹ï¼‰
    
    Args:
        df: æ•°æ®æ¡†
        predicates: æ‰€æœ‰è°“è¯
        enum_predicates: æšä¸¾å‹è°“è¯ï¼ˆä½œä¸ºç»“è®ºï¼‰
        max_depth: æœ€å¤§è§„åˆ™æ·±åº¦
        n_iter: æ¯ä¸ªy_predçš„è¿­ä»£æ¬¡æ•°
        n_workers: å¹¶è¡Œå·¥ä½œè¿›ç¨‹æ•°ï¼ŒNoneè¡¨ç¤ºä½¿ç”¨CPUæ ¸å¿ƒæ•°
        use_policy: æ˜¯å¦ä½¿ç”¨ç­–ç•¥æ¨¡å‹
        policy_model: ç­–ç•¥æ¨¡å‹å®ä¾‹
        c_param: MCTSæ¢ç´¢å‚æ•°
    """
    if not use_parallel or len(enum_predicates) < 5:
        # å•çº¿ç¨‹æ¨¡å¼ï¼ˆç”¨äºè°ƒè¯•æˆ–å°è§„æ¨¡æ•°æ®ï¼‰
        print(f"ğŸ”„ ä½¿ç”¨å•çº¿ç¨‹MCTS: {len(enum_predicates)}ä¸ªy_pred")
        if use_policy and policy_model and policy_model.is_trained:
            print(f"ğŸ¯ ä½¿ç”¨ç­–ç•¥æ¨¡å‹è¿›è¡Œrollout")
        start_time = time.time()
        results = []
        for i, y_pred in enumerate(tqdm(enum_predicates, desc="å•çº¿ç¨‹MCTS", unit="y_pred")):
            args = (df, predicates, y_pred, max_depth, n_iter, use_policy, policy_model, c_param, alpha)
            result = mcts_rule_discovery_single_y_pred(args)
            results.append(result)
        return results
    
    # å¤šçº¿ç¨‹æ¨¡å¼
    if n_workers is None:
        n_workers = min(mp.cpu_count(), len(enum_predicates))
    
    print(f"ğŸš€ å¯åŠ¨åˆ†å¸ƒå¼MCTSè§„åˆ™å‘ç°: {len(enum_predicates)}ä¸ªy_pred, {n_workers}ä¸ªå·¥ä½œè¿›ç¨‹")
    if use_policy and policy_model and policy_model.is_trained:
        print(f"ğŸ¯ ä½¿ç”¨å¯å‘å¼ç­–ç•¥æ¨¡å‹è¿›è¡Œrollout")
    start_time = time.time()
    
    # å‡†å¤‡å‚æ•°ï¼ˆä½¿ç”¨ç®€åŒ–çš„å¯å‘å¼ç­–ç•¥ï¼‰
    args_list = [(df, predicates, y_pred, max_depth, n_iter, use_policy, policy_model, c_param, alpha) 
                 for y_pred in enum_predicates]
    
    # ä½¿ç”¨è¿›ç¨‹æ± è¿›è¡Œå¹¶è¡Œè®¡ç®—
    results = []
    with ProcessPoolExecutor(max_workers=n_workers) as executor:
        # æäº¤æ‰€æœ‰ä»»åŠ¡
        future_to_y_pred = {executor.submit(mcts_rule_discovery_single_y_pred, args): args[2] 
                           for args in args_list}
        
        # æ”¶é›†ç»“æœ
        completed = 0
        with tqdm(total=len(enum_predicates), desc="åˆ†å¸ƒå¼MCTS", unit="y_pred") as pbar:
            for future in as_completed(future_to_y_pred):
                y_pred = future_to_y_pred[future]
                try:
                    # æ·»åŠ è¶…æ—¶æœºåˆ¶ï¼Œé¿å…æ— é™ç­‰å¾…
                    result = future.result(timeout=300)  # 5åˆ†é’Ÿè¶…æ—¶
                    results.append(result)
                    completed += 1
                    pbar.update(1)
                except Exception as e:
                    print(f"âŒ y_pred {y_pred} å¤„ç†å¤±è´¥: {e}")
                    # æ·»åŠ é»˜è®¤ç»“æœ
                    results.append((y_pred, [], 0, 0.0))
                    completed += 1
                    pbar.update(1)
    
    elapsed_time = time.time() - start_time
    print(f"âœ… åˆ†å¸ƒå¼MCTSå®Œæˆ: {len(results)}ä¸ªè§„åˆ™, è€—æ—¶ {elapsed_time:.2f}ç§’")
    
    # æ‰“å°æ€§èƒ½ç»Ÿè®¡
    print_performance_stats(start_time, len(enum_predicates), n_workers)
    
    return results


def get_distributed_config():
    """è·å–åˆ†å¸ƒå¼è®¡ç®—é…ç½®"""
    config = {
        'n_workers': None,  # Noneè¡¨ç¤ºä½¿ç”¨CPUæ ¸å¿ƒæ•°
        'use_multiprocessing': True,  # æ˜¯å¦ä½¿ç”¨å¤šè¿›ç¨‹
        'chunk_size': 10,  # æ¯ä¸ªè¿›ç¨‹å¤„ç†çš„y_predæ•°é‡
        'max_workers': mp.cpu_count(),  # æœ€å¤§å·¥ä½œè¿›ç¨‹æ•°
    }
    
    # å¯ä»¥æ ¹æ®æ•°æ®è§„æ¨¡è°ƒæ•´
    if config['max_workers'] > 8:
        config['n_workers'] = 8  # é™åˆ¶æœ€å¤§è¿›ç¨‹æ•°
    else:
        config['n_workers'] = config['max_workers']
    
    return config

def print_performance_stats(start_time, n_y_preds, n_workers):
    """æ‰“å°æ€§èƒ½ç»Ÿè®¡ä¿¡æ¯"""
    elapsed_time = time.time() - start_time
    avg_time_per_pred = elapsed_time / n_y_preds if n_y_preds > 0 else 0
    theoretical_speedup = n_workers if n_workers > 1 else 1
    
    print(f"ğŸ“Š æ€§èƒ½ç»Ÿè®¡:")
    print(f"   æ€»è€—æ—¶: {elapsed_time:.2f}ç§’")
    print(f"   å¹³å‡æ¯ä¸ªy_predè€—æ—¶: {avg_time_per_pred:.2f}ç§’")
    print(f"   å·¥ä½œè¿›ç¨‹æ•°: {n_workers}")
    print(f"   ç†è®ºåŠ é€Ÿæ¯”: {theoretical_speedup}x")
    print(f"   å®é™…åŠ é€Ÿæ¯”: {elapsed_time / (avg_time_per_pred * n_y_preds / n_workers) if n_workers > 1 else 1:.2f}x")

def train_policy_model(df, predicates, enum_predicates, max_depth=3, n_samples=1000, model_path=None):
    """è®­ç»ƒç­–ç•¥æ¨¡å‹
    
    Args:
        df: æ•°æ®æ¡†
        predicates: æ‰€æœ‰è°“è¯
        enum_predicates: æšä¸¾å‹è°“è¯
        max_depth: æœ€å¤§è§„åˆ™æ·±åº¦
        n_samples: è®­ç»ƒæ ·æœ¬æ•°é‡
        model_path: æ¨¡å‹ä¿å­˜è·¯å¾„
    
    Returns:
        ValuePolicyModel: è®­ç»ƒå¥½çš„ç­–ç•¥æ¨¡å‹
    """
    print(f"ğŸ¯ å¼€å§‹è®­ç»ƒç­–ç•¥æ¨¡å‹ï¼Œç”Ÿæˆ {n_samples} ä¸ªè®­ç»ƒæ ·æœ¬...")
    
    policy_model = ValuePolicyModel()
    training_data = []
    feature_predicates = [p for p in predicates if p not in enum_predicates]
    df_stats = get_df_stats(df, predicates)
    
    # ç”Ÿæˆè®­ç»ƒæ•°æ®
    print(f"ğŸ”„ ç”Ÿæˆè®­ç»ƒæ ·æœ¬...")
    for _ in tqdm(range(n_samples), desc="ç”Ÿæˆè®­ç»ƒæ•°æ®", unit="æ ·æœ¬"):
        # éšæœºé€‰æ‹©ä¸€ä¸ªy_pred
        y_pred = random.choice(enum_predicates)
        
        # éšæœºç”Ÿæˆä¸€ä¸ªè§„åˆ™
        current_preds = [y_pred]
        unused = list(feature_predicates)
        
        # éšæœºæ·»åŠ è°“è¯
        rule_length = random.randint(1, max_depth)
        for _ in range(rule_length - 1):
            if not unused:
                break
            pred = random.choice(unused)
            current_preds.append(pred)
            unused.remove(pred)
        
        # è¯„ä¼°è§„åˆ™
        support, confidence = evaluate_rule(df, current_preds)
        reward = support * confidence
        
        # åˆ›å»ºç‰¹å¾
        state_features = policy_model._create_state_features(current_preds, df_stats)
        
        # ä¸ºæ¯ä¸ªå¯èƒ½çš„åŠ¨ä½œåˆ›å»ºç‰¹å¾
        for action_pred in unused[:min(10, len(unused))]:  # é™åˆ¶åŠ¨ä½œæ•°é‡
            action_features = policy_model._create_feature_vector(current_preds, unused, df_stats)
            next_state_features = policy_model._create_state_features(current_preds + [action_pred], df_stats)
            
            training_data.append((state_features, action_features, reward, next_state_features))
    
    # è®­ç»ƒæ¨¡å‹
    if training_data:
        policy_model.train(training_data)
        
        # ä¿å­˜æ¨¡å‹
        if model_path:
            policy_model.save_model(model_path)
        
        print(f"âœ… ç­–ç•¥æ¨¡å‹è®­ç»ƒå®Œæˆï¼Œä½¿ç”¨ {len(training_data)} ä¸ªæ ·æœ¬")
    else:
        print("âš ï¸ æ²¡æœ‰ç”Ÿæˆæœ‰æ•ˆçš„è®­ç»ƒæ•°æ®")
    
    return policy_model

def run_single_experiment(mcts_df, filtered_predicates, enum_predicates, dist_config, 
                         n_iter, c_param, support_threshold, confidence_threshold, 
                         data_dir, experiment_name, use_policy=True, policy_model=None, alpha=1.0):
    """è¿è¡Œå•ä¸ªå‚æ•°ç»„åˆçš„å®éªŒ"""
    print(f"\n{'='*60}")
    print(f"ğŸ”¬ å®éªŒ: {experiment_name}")
    print(f"   å‚æ•°: n_iter={n_iter}, c_param={c_param}, alpha={alpha}, support_threshold={support_threshold}, confidence_threshold={confidence_threshold}")
    print(f"   ç­–ç•¥æ¨¡å‹: {'å¯ç”¨' if use_policy else 'ç¦ç”¨'}")
    print(f"{'='*60}")
    
    # ä¸´æ—¶ä¿®æ”¹MCTSNodeçš„best_childæ–¹æ³•
    original_best_child = MCTSNode.best_child
    
    def custom_best_child(self, c_param=c_param, alpha=alpha):
        import numpy as np
        if not self.children:
            return None
            
        # ä½¿ç”¨UCBè¿›è¡ŒèŠ‚ç‚¹é€‰æ‹©ï¼Œalphaå‚æ•°å½±å“æ¢ç´¢é¡¹
        choices_weights = []
        for child in self.children:
            # UCBå…¬å¼ï¼šexploitation + alpha * exploration
            exploitation = child.value / (child.visits + 1e-6)
            exploration = c_param * math.sqrt(math.log(self.visits + 1) / (child.visits + 1e-6))
            ucb_score = exploitation + alpha * exploration
            choices_weights.append(ucb_score)
        
        return self.children[np.argmax(choices_weights)]
    
    MCTSNode.best_child = custom_best_child
    
    try:
        # ç­–ç•¥1: å…¨å±€è§„åˆ™å‘ç°
        print(f"ğŸ” ç­–ç•¥1: å…¨å±€è§„åˆ™å‘ç°...")
        mcts_results = mcts_rule_discovery_yroot(
            mcts_df, 
            filtered_predicates, 
            enum_predicates, 
            max_depth=6,
            n_iter=n_iter,  # ä½¿ç”¨å®éªŒå‚æ•°
            n_workers=dist_config['n_workers'],
            use_parallel=dist_config['use_multiprocessing'],
            use_policy=use_policy,  # ä½¿ç”¨ç­–ç•¥æ¨¡å‹
            policy_model=policy_model,
            c_param=c_param,  # ä½¿ç”¨å®éªŒå‚æ•°
            alpha=alpha  # ä½¿ç”¨å®éªŒå‚æ•°
        )
        
        # ç­–ç•¥2: é’ˆå¯¹ç‰¹å®šåˆ—çš„ä¸“é—¨è§„åˆ™å‘ç°
        print(f"ğŸ” ç­–ç•¥2: é’ˆå¯¹ç‰¹å®šåˆ—çš„ä¸“é—¨è§„åˆ™å‘ç°...")
        target_columns = ['genre', 'language', 'format', 'publisher', 'rating', 'availability']
        specialized_results = []
        
        for target_col in target_columns:
            if target_col in mcts_df.columns:
                print(f"  ğŸ¯ ä¸º{target_col}åˆ—å‘ç°ä¸“é—¨è§„åˆ™...")
                col_vals = mcts_df[target_col].dropna().unique()
                col_predicates = [f'{target_col} = "{val}"' for val in col_vals if pd.notna(val) and str(val).strip() != '']
                
                if col_predicates:
                    col_predicates = col_predicates[:15]
                    print(f"    {target_col}åˆ—è°“è¯æ•°: {len(col_predicates)}")
                    
                    col_results = mcts_rule_discovery_yroot(
                        mcts_df,
                        filtered_predicates,
                        col_predicates,
                        max_depth=6,
                        n_iter=n_iter // 2,  # ä¸“é—¨è§„åˆ™ä½¿ç”¨è¾ƒå°‘çš„è¿­ä»£æ¬¡æ•°
                        n_workers=dist_config['n_workers'],
                        use_parallel=dist_config['use_multiprocessing'],
                        use_policy=use_policy,  # ä½¿ç”¨ç­–ç•¥æ¨¡å‹
                        policy_model=policy_model,
                        c_param=c_param,  # ä½¿ç”¨å®éªŒå‚æ•°
                        alpha=alpha  # ä½¿ç”¨å®éªŒå‚æ•°
                    )
                    specialized_results.extend(col_results)
                    print(f"    âœ… {target_col}åˆ—å‘ç° {len(col_results)} ä¸ªè§„åˆ™")
        
        # åˆå¹¶æ‰€æœ‰ç»“æœ
        all_results = mcts_results + specialized_results
        print(f"âœ… æ€»è§„åˆ™æ•°: {len(all_results)} (å…¨å±€: {len(mcts_results)}, ä¸“é—¨: {len(specialized_results)})")
        
        # ä½¿ç”¨å®éªŒå‚æ•°è¿›è¡Œè´¨é‡è¿‡æ»¤
        rules_data = []
        for y_pred, rule, support, confidence in all_results:
            if support >= support_threshold and confidence >= confidence_threshold:
                rule_complexity = len(rule) - 1
                if rule_complexity >= 1:
                    rules_data.append({
                        'y_pred': y_pred,
                        'best_rule': ' ^ '.join(rule[1:]),
                        'support': support,
                        'confidence': confidence
                    })
        
        print(f'ğŸ” è´¨é‡è¿‡æ»¤åä¿ç•™ {len(rules_data)} ä¸ªè§„åˆ™ (æ€»å‘ç° {len(all_results)} ä¸ª)')
        rules_df = pd.DataFrame(rules_data)
        
        if not rules_df.empty:
            # ä¿å­˜å®éªŒç‰¹å®šçš„è§„åˆ™æ–‡ä»¶
            experiment_rules_file = os.path.join(data_dir, f'dcr_mcts_rule_{experiment_name}.csv')
            rules_df.to_csv(experiment_rules_file, index=False)
            print(f'âœ… å‘ç° {len(rules_df)} ä¸ªæœ‰æ•ˆè§„åˆ™ï¼Œä¿å­˜åˆ°: {experiment_rules_file}')
            
            # è§„åˆ™åˆ†å¸ƒç»Ÿè®¡
            print(f'ğŸ“Š è§„åˆ™åˆ†å¸ƒ:')
            for col in ['genre', 'language', 'format', 'publisher', 'rating', 'availability']:
                col_rules = rules_df[rules_df['y_pred'].str.contains(col, na=False)]
                print(f'  {col}: {len(col_rules)} ä¸ªè§„åˆ™')
        else:
            rules_df = pd.DataFrame(columns=['y_pred', 'best_rule', 'support', 'confidence'])
            experiment_rules_file = os.path.join(data_dir, f'dcr_mcts_rule_{experiment_name}.csv')
            rules_df.to_csv(experiment_rules_file, index=False)
            print('âš ï¸ æœªå‘ç°ä»»ä½•æœ‰æ•ˆè§„åˆ™')
        
        # è§„åˆ™æŸ¥é”™å’Œè¯„ä¼°
        test_dirty = pd.read_csv(os.path.join(data_dir, 'test_dirty_extend.csv'))
        test_clean = pd.read_csv(os.path.join(data_dir, 'test_extend.csv'))
        
        results = []
        def extract_col_from_predicate(pred):
            import re
            m = re.match(r'(\w+)\s*[=!<>]+\s*.+', pred)
            if m:
                return m.group(1)
            return None
        
        exclude_cols = [col for col in test_dirty.columns if "embed_cluster" in col or "img_category" in col]
        
        for idx, row in tqdm(rules_df.iterrows(), total=len(rules_df), desc="è§„åˆ™æŸ¥é”™", unit="è§„åˆ™"):
            y_pred = row['y_pred']
            best_rule = row['best_rule']
            premise_preds = [p.strip() for p in best_rule.split('^') if p.strip()]
            conclusion_pred = y_pred
            conclusion_col = extract_col_from_predicate(conclusion_pred)
            if conclusion_col is None or conclusion_col in exclude_cols:
                continue
            
            mask = np.ones(len(test_dirty), dtype=bool)
            for pred in premise_preds:
                mask = mask & predicate_mask(test_dirty, pred)
            mask_conclusion = predicate_mask(test_dirty, conclusion_pred)
            error_mask = mask & (~mask_conclusion)
            
            for i in range(len(test_dirty)):
                if error_mask[i]:
                    clean_val = test_clean.iloc[i][conclusion_col] if i < len(test_clean) else None
                    dirty_val = test_dirty.iloc[i][conclusion_col]
                    if pd.isna(clean_val) and pd.isna(dirty_val):
                        error_mask[i] = False
                    elif not pd.isna(clean_val) and not pd.isna(dirty_val):
                        if str(clean_val).strip() == str(dirty_val).strip():
                            error_mask[i] = False
            
            error_positions = [i for i, is_error in enumerate(error_mask) if is_error]
            error_cells = [(i, conclusion_col) for i in error_positions]
            if error_cells:
                results.append({
                    'rule_id': idx,
                    'y_pred': y_pred,
                    'best_rule': best_rule,
                    'error_cell': json.dumps(error_cells),
                    'error_count': len(error_cells)
                })
        
        error_df = pd.DataFrame(results)
        experiment_error_file = os.path.join(data_dir, f'dcr_rule_error_detect_{experiment_name}.csv')
        if not error_df.empty:
            error_df.to_csv(experiment_error_file, index=False)
        else:
            error_df = pd.DataFrame(columns=['rule_id', 'y_pred', 'best_rule', 'error_cell', 'error_count'])
            error_df.to_csv(experiment_error_file, index=False)
        
        # è®¡ç®—è¯„ä¼°æŒ‡æ ‡
        pred_cells = set()
        for cells in error_df['error_cell']:
            for cell in json.loads(cells):
                row_idx, col_name = cell
                if row_idx >= len(test_clean) or col_name not in test_clean.columns:
                    continue
                pred_cells.add(tuple(cell))
        
        real_cells = set()
        exclude_cols = [col for col in test_clean.columns if "embed_cluster" in col or "img_category" in col]
        
        print(f"ğŸ” æ£€æµ‹å®é™…é”™è¯¯...")
        for i in tqdm(range(len(test_clean)), desc="æ£€æµ‹å®é™…é”™è¯¯", unit="è¡Œ"):
            for col in test_clean.columns:
                # è·³è¿‡ä¸åº”è¯¥æœ‰é”™è¯¯çš„åˆ—
                if col in exclude_cols:
                    continue
                clean_val = test_clean.at[i, col]
                dirty_val = test_dirty.at[i, col]
                # æ­£ç¡®å¤„ç†NaNå€¼æ¯”è¾ƒ
                if pd.isna(clean_val) and pd.isna(dirty_val):
                    continue  # ä¸¤ä¸ªéƒ½æ˜¯NaNï¼Œä¸ç®—é”™è¯¯
                elif pd.isna(clean_val) or pd.isna(dirty_val):
                    real_cells.add((i, col))  # ä¸€ä¸ªNaNä¸€ä¸ªéNaNï¼Œç®—é”™è¯¯
                elif str(clean_val).strip() != str(dirty_val).strip():
                    real_cells.add((i, col))  # å­—ç¬¦ä¸²æ¯”è¾ƒï¼Œå»é™¤ç©ºæ ¼
        
        # è®¡ç®—æŒ‡æ ‡
        TP = len(pred_cells & real_cells)
        FP = len(pred_cells - real_cells)
        FN = len(real_cells - pred_cells)
        total_cells = test_clean.shape[0] * test_clean.shape[1]
        TN = total_cells - TP - FP - FN
        precision = TP / (TP + FP) if (TP + FP) > 0 else 0
        recall = TP / (TP + FN) if (TP + FN) > 0 else 0
        f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0
        accuracy = (TP + TN) / total_cells
        
        # ä¿å­˜å®éªŒç»“æœ
        experiment_metrics_file = os.path.join(data_dir, f'dcr_rule_error_metrics_{experiment_name}.txt')
        with open(experiment_metrics_file, 'w', encoding='utf-8') as f:
            f.write(f'Precision: {precision:.4f}\n')
            f.write(f'Recall: {recall:.4f}\n')
            f.write(f'F1: {f1:.4f}\n')
            f.write(f'Accuracy: {accuracy:.4f}\n')
            f.write(f'\né”™è¯¯ç»Ÿè®¡:\n')
            f.write(f'æ€»å•å…ƒæ ¼æ•°: {total_cells}\n')
            f.write(f'å®é™…é”™è¯¯æ•°: {len(real_cells)}\n')
            f.write(f'é¢„æµ‹é”™è¯¯æ•°: {len(pred_cells)}\n')
            f.write(f'çœŸé˜³æ€§(TP): {TP}\n')
            f.write(f'å‡é˜³æ€§(FP): {FP}\n')
            f.write(f'å‡é˜´æ€§(FN): {FN}\n')
            f.write(f'çœŸé˜´æ€§(TN): {TN}\n')
        
        print(f'âœ… å®éªŒ {experiment_name} å®Œæˆ:')
        print(f'   Precision: {precision:.4f}')
        print(f'   Recall: {recall:.4f}')
        print(f'   F1: {f1:.4f}')
        print(f'   Accuracy: {accuracy:.4f}')
        
        return {
            'experiment_name': experiment_name,
            'n_iter': n_iter,
            'c_param': c_param,
            'alpha': alpha,
            'precision': precision,
            'recall': recall,
            'f1': f1,
            'accuracy': accuracy,
            'rules_count': len(rules_df),
            'errors_detected': len(pred_cells),
            'actual_errors': len(real_cells)
        }
        
    finally:
        # æ¢å¤åŸå§‹çš„best_childæ–¹æ³•
        MCTSNode.best_child = original_best_child

def run_parameter_experiments(mcts_df, filtered_predicates, enum_predicates, dist_config, data_dir, use_policy=True, policy_model=None):
    """è¿è¡Œå‚æ•°å®éªŒ"""
    print(f"\n{'='*80}")
    print(f"ğŸ”¬ å¼€å§‹å‚æ•°æ•æ„Ÿæ€§å®éªŒ")
    print(f"   ç­–ç•¥æ¨¡å‹: {'å¯ç”¨' if use_policy else 'ç¦ç”¨'}")
    print(f"{'='*80}")
    
    # å®éªŒå‚æ•°é…ç½®
    experiments = []
    
    # å®éªŒ1: ä¸åŒè¿­ä»£æ¬¡æ•° (ğ¼max)
    n_iter_values = [100, 500, 1000, 5000, 10000]
    for n_iter in n_iter_values:
        experiments.append({
            'name': f'iter_{n_iter}',
            'n_iter': n_iter,
            'c_param': 1.4,  # å›ºå®šc_param
            'support_threshold': 0.2,
            'confidence_threshold': 0.65
        })
    
    # å®éªŒ2: ä¸åŒc_paramå€¼
    c_param_values = [0.1, 0.5, 1.0, 1.4, 2.0, 3.0]
    for c_param in c_param_values:
        experiments.append({
            'name': f'cparam_{c_param}',
            'n_iter': 10000,  # å›ºå®šn_iter
            'c_param': c_param,
            'support_threshold': 0.2,
            'confidence_threshold': 0.65
        })
    
    # å®éªŒ3: ä¸åŒalphaå€¼ (æ¢ç´¢ä¸åˆ©ç”¨å¹³è¡¡å‚æ•°)
    alpha_values = [0.1, 0.5, 1.0, 1.5, 2.0, 3.0]
    for alpha in alpha_values:
        experiments.append({
            'name': f'alpha_{alpha}',
            'n_iter': 10000,  # å›ºå®šn_iter
            'c_param': 1.4,   # å›ºå®šc_param
            'alpha': alpha,
            'support_threshold': 0.2,
            'confidence_threshold': 0.65
        })
    
    # è¿è¡Œæ‰€æœ‰å®éªŒ
    results = []
    for i, exp_config in enumerate(experiments):
        print(f"\nğŸ“Š è¿›åº¦: {i+1}/{len(experiments)} ({((i+1)/len(experiments)*100):.1f}%)")
        
        try:
            result = run_single_experiment(
                mcts_df, filtered_predicates, enum_predicates, dist_config,
                exp_config['n_iter'], exp_config['c_param'], exp_config['support_threshold'], exp_config['confidence_threshold'],
                data_dir, exp_config['name'], use_policy, policy_model, exp_config.get('alpha', 1.0)
            )
            results.append(result)
        except Exception as e:
            print(f"âŒ å®éªŒ {exp_config['name']} å¤±è´¥: {e}")
            results.append({
                'experiment_name': exp_config['name'],
                'n_iter': exp_config['n_iter'],
                'c_param': exp_config['c_param'],
                'alpha': exp_config.get('alpha', 1.0),
                'precision': 0.0,
                'recall': 0.0,
                'f1': 0.0,
                'accuracy': 0.0,
                'rules_count': 0,
                'errors_detected': 0,
                'actual_errors': 0
            })
    
    # ä¿å­˜å®éªŒç»“æœæ±‡æ€»
    results_df = pd.DataFrame(results)
    results_summary_file = os.path.join(data_dir, 'parameter_experiments_summary.csv')
    results_df.to_csv(results_summary_file, index=False)
    print(f"\nâœ… æ‰€æœ‰å®éªŒå®Œæˆï¼Œç»“æœä¿å­˜åˆ°: {results_summary_file}")
    
    # æ‰“å°å®éªŒç»“æœåˆ†æ
    print(f"\n{'='*80}")
    print(f"ğŸ“Š å‚æ•°å®éªŒç»“æœåˆ†æ")
    print(f"{'='*80}")
    
    # åˆ†æè¿­ä»£æ¬¡æ•°å½±å“
    iter_results = results_df[results_df['experiment_name'].str.startswith('iter_')]
    if not iter_results.empty:
        print(f"\nğŸ” è¿­ä»£æ¬¡æ•° (ğ¼max) å½±å“åˆ†æ:")
        for _, row in iter_results.iterrows():
            print(f"  n_iter={row['experiment_name'].split('_')[1]}: Precision={row['precision']:.4f}, Recall={row['recall']:.4f}, F1={row['f1']:.4f}, Accuracy={row['accuracy']:.4f}")
    
    # åˆ†æc_paramå½±å“
    cparam_results = results_df[results_df['experiment_name'].str.startswith('cparam_')]
    if not cparam_results.empty:
        print(f"\nğŸ” c_param å½±å“åˆ†æ:")
        for _, row in cparam_results.iterrows():
            print(f"  c_param={row['experiment_name'].split('_')[1]}: Precision={row['precision']:.4f}, Recall={row['recall']:.4f}, F1={row['f1']:.4f}, Accuracy={row['accuracy']:.4f}")
    
    # åˆ†æalphaå½±å“
    alpha_results = results_df[results_df['experiment_name'].str.startswith('alpha_')]
    if not alpha_results.empty:
        print(f"\nğŸ” alpha å½±å“åˆ†æ:")
        for _, row in alpha_results.iterrows():
            print(f"  alpha={row['experiment_name'].split('_')[1]}: Precision={row['precision']:.4f}, Recall={row['recall']:.4f}, F1={row['f1']:.4f}, Accuracy={row['accuracy']:.4f}")
    
    # æ‰¾å‡ºæœ€ä½³å‚æ•°ç»„åˆ
    best_result = results_df.loc[results_df['f1'].idxmax()]
    print(f"\nğŸ† æœ€ä½³å‚æ•°ç»„åˆ (åŸºäºF1åˆ†æ•°):")
    print(f"  å®éªŒåç§°: {best_result['experiment_name']}")
    print(f"  n_iter: {best_result.get('n_iter', 'N/A')}")
    print(f"  c_param: {best_result.get('c_param', 'N/A')}")
    print(f"  alpha: {best_result.get('alpha', 'N/A')}")
    print(f"  F1: {best_result['f1']:.4f}")
    print(f"  Precision: {best_result['precision']:.4f}")
    print(f"  Recall: {best_result['recall']:.4f}")
    print(f"  Accuracy: {best_result['accuracy']:.4f}")
    print(f"  è§„åˆ™æ•°é‡: {best_result['rules_count']}")
    print(f"  æ£€æµ‹é”™è¯¯æ•°: {best_result['errors_detected']}")
    print(f"  å®é™…é”™è¯¯æ•°: {best_result['actual_errors']}")
    
    return results_df

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸš€ Starting Multimodal DCRLearner Pipeline")
    
    # è·å–åˆ†å¸ƒå¼é…ç½®
    dist_config = get_distributed_config()
    print(f"ğŸ”§ åˆ†å¸ƒå¼é…ç½®: {dist_config}")
    
    # ä½¿ç”¨æ–°çš„æ•°æ®è·¯å¾„
    data_dir = "/data_nas/DCR/split_addnoise/goodreads_test"
    
    # æ·»åŠ å®éªŒæ¨¡å¼é€‰æ‹©
    import sys
    if len(sys.argv) > 1 and sys.argv[1] == '--experiment':
        print("ğŸ”¬ å¯ç”¨å‚æ•°å®éªŒæ¨¡å¼")
        run_experiment_mode = True
    else:
        print("ğŸ“Š å¯ç”¨æ ‡å‡†æ¨¡å¼")
        run_experiment_mode = False
    
    print(f"ğŸ“ Using data directory: {data_dir}")
    
    # æ£€æŸ¥æ•°æ®ç›®å½•æ˜¯å¦å­˜åœ¨
    if not os.path.exists(data_dir):
        print(f"âŒ Data directory not found: {data_dir}")
        return
    
    # æ£€æŸ¥å¿…è¦çš„æ–‡ä»¶æ˜¯å¦å­˜åœ¨
    required_files = ['train_clean.csv', 'test_clean.csv', 'test_dirty.csv']
    for file in required_files:
        file_path = os.path.join(data_dir, file)
        if not os.path.exists(file_path):
            print(f"âŒ Required file not found: {file_path}")
            return
        else:
            print(f"âœ… Found {file}")

    # ========== æ–°å¢ï¼šæ•°æ®ç±»å‹è¯†åˆ«ä¸embeddingé›†æˆ ==========
    # train_csv = os.path.join(data_dir, 'train_clean.csv')
    # test_csv = os.path.join(data_dir, 'test_clean.csv')
    # imgs_dir = os.path.join(data_dir, 'imgs')
    # if not os.path.exists(train_csv) or not os.path.exists(test_csv):
    #     print(f"âŒ train_clean.csv æˆ– test_clean.csv ä¸å­˜åœ¨")
    #     return
    # df_train = pd.read_csv(train_csv)
    # df_test = pd.read_csv(test_csv)
    # df_all = pd.concat([df_train, df_test], ignore_index=True)
    # print(f"ğŸ“Š åˆå¹¶åæ•°æ® shape: {df_all.shape}")

    # # åˆå§‹åŒ–æ¨¡å‹
    # model = MultimodalModels(device='cuda')

    # # è‡ªåŠ¨è¯†åˆ«åˆ—ç±»å‹
    # col_types = {}
    # for col in df_all.columns:
    #     if "img_path" in col:
    #         col_types[col] = "img_path"
    #     elif df_all[col].dtype in ['float64', 'int64']:
    #         col_types[col] = "numeric"
    #     elif df_all[col].dtype == 'object':
    #         unique_count = df_all[col].nunique()
    #         avg_len = df_all[col].astype(str).apply(len).mean()
    #         if unique_count < 20:
    #             col_types[col] = "enum"
    #         elif avg_len > 20:
    #             col_types[col] = "text"
    #         else:
    #             col_types[col] = "enum"
    # print(f"ğŸ” åˆ—ç±»å‹è¯†åˆ«: {col_types}")

    # # å›¾ç‰‡ embedding
    # if "img_path" in col_types.values():
    #     img_col = [col for col, typ in col_types.items() if typ == "img_path"][0]
    #     img_paths = df_all[img_col].apply(lambda x: os.path.join(imgs_dir, str(x)))
    #     img_embeds = []
    #     img_categories = []
    #     for path in img_paths:
    #         img_feat = model.extract_image_features(path)
    #         img_category = model.extract_image_category(path)
    #         if img_feat is not None:
    #             img_embeds.append(img_feat.cpu().numpy())
    #         else:
    #             img_embeds.append(None)
    #         img_categories.append(img_category)
    #     df_all["img_embedding"] = img_embeds
    #     df_all["img_category"] = img_categories
    #     print(f"âœ… å›¾ç‰‡embeddingå’Œç±»åˆ«å·²æ·»åŠ ")

    #     # å±‚æ¬¡èšç±»å‰PCAé™ç»´
    #     from sklearn.decomposition import PCA
    #     from scipy.cluster.hierarchy import linkage, fcluster
    #     import numpy as np
    #     valid_indices = [i for i, emb in enumerate(img_embeds) if emb is not None]
    #     valid_embeds = np.array([img_embeds[i].flatten() for i in valid_indices])
    #     if len(valid_embeds) > 1:
    #         pca = PCA(n_components=64)
    #         reduced_embeds = pca.fit_transform(valid_embeds)
    #         Z = linkage(reduced_embeds, method='ward')
    #         max_clusters = min(10, len(valid_embeds))
    #         clusters = fcluster(Z, max_clusters, criterion='maxclust')
    #         img_embed_cluster = [None] * len(img_embeds)
    #         for idx, c in zip(valid_indices, clusters):
    #             img_embed_cluster[idx] = int(c)
    #         df_all[f"img_embed_cluster"] = img_embed_cluster
    #         print(f"âœ… å·²å®Œæˆå›¾ç‰‡embeddingçš„PCAé™ç»´+å±‚æ¬¡èšç±»ï¼Œèšç±»æ•°: {max(clusters)}")
    #     else:
    #         df_all["img_embed_cluster"] = [None] * len(img_embeds)
    #         print(f"âš ï¸ æœ‰æ•ˆå›¾ç‰‡embeddingæ•°é‡ä¸è¶³ï¼Œæœªèšç±»")

    # # æ–‡æœ¬ embedding
    # for col, typ in col_types.items():
    #     if typ == "text":
    #         text_embeds = []
    #         for text in df_all[col].astype(str):
    #             text_feat = model.extract_text_features(text)
    #             text_embeds.append(text_feat.cpu().numpy())
    #         df_all[f"text_embedding_{col}"] = text_embeds
    #         print(f"âœ… æ–‡æœ¬embeddingå·²æ·»åŠ : {col}")

    #         # æ–‡æœ¬embeddingå±‚æ¬¡èšç±»å‰PCAé™ç»´
    #         from sklearn.decomposition import PCA
    #         from scipy.cluster.hierarchy import linkage, fcluster
    #         import numpy as np
    #         valid_indices = [i for i, emb in enumerate(text_embeds) if emb is not None]
    #         valid_embeds = np.array([text_embeds[i].flatten() for i in valid_indices])
    #         if len(valid_embeds) > 1:
    #             pca = PCA(n_components=64)
    #             reduced_embeds = pca.fit_transform(valid_embeds)
    #             Z = linkage(reduced_embeds, method='ward')
    #             max_clusters = min(10, len(valid_embeds))
    #             clusters = fcluster(Z, max_clusters, criterion='maxclust')
    #             text_embed_cluster = [None] * len(text_embeds)
    #             for idx, c in zip(valid_indices, clusters):
    #                 text_embed_cluster[idx] = int(c)
    #             df_all[f"text_{col}_embed_cluster"] = text_embed_cluster
    #             print(f"âœ… å·²å®Œæˆæ–‡æœ¬embeddingçš„PCAé™ç»´+å±‚æ¬¡èšç±»: {col}ï¼Œèšç±»æ•°: {max(clusters)}")
    #         else:
    #             df_all[f"text_{col}_embed_cluster"] = [None] * len(text_embeds)
    #             print(f"âš ï¸ æœ‰æ•ˆæ–‡æœ¬embeddingæ•°é‡ä¸è¶³ï¼Œæœªèšç±»: {col}")

    # # ä¿å­˜ä¸ºpkl
    # out_pkl = os.path.join(data_dir, "train_with_embeddings.pkl")
    # df_all.to_pickle(out_pkl)
    # print(f"âœ… å·²ä¿å­˜å¸¦embeddingçš„æ•°æ®: {out_pkl}")

    # # ä¿å­˜ä¸ºcsvï¼Œåˆ†åˆ«æ‹†åˆ†train/test
    # n_train = len(df_train)
    # df_train_extend = df_all.iloc[:n_train].copy()
    # df_test_extend = df_all.iloc[n_train:].copy()
    # # åªä¿å­˜å¯åºåˆ—åŒ–çš„åˆ—ï¼ˆå»é™¤é«˜ç»´embeddingåˆ—ï¼‰
    # drop_cols = [col for col in df_all.columns if isinstance(df_all[col].iloc[0], (np.ndarray, list, dict, torch.Tensor))]
    # df_train_csv = df_train_extend.drop(columns=drop_cols)
    # df_test_csv = df_test_extend.drop(columns=drop_cols)
    # df_train_csv.to_csv(os.path.join(data_dir, "train_extend.csv"), index=False)
    # df_test_csv.to_csv(os.path.join(data_dir, "test_extend.csv"), index=False)
    # print(f"âœ… å·²ä¿å­˜æ‰©å±•ç‰¹å¾çš„csv: train_extend.csv, test_extend.csv")

    
    df_train_csv=pd.read_csv(os.path.join(data_dir, "train_extend.csv"))
    df_test_csv=pd.read_csv(os.path.join(data_dir, "test_extend.csv"))
    test_dirty=pd.read_csv(os.path.join(data_dir, "test_dirty.csv"))
    
    # test_dirtyèšç±»æ ‡ç­¾ç›´æ¥ç”¨test_cleançš„ï¼ˆå‡è®¾ä¸€ä¸€å¯¹åº”ï¼‰
    for col in df_test_csv.columns:
        if ("embed_cluster" in col or "img_category" in col) and col not in test_dirty.columns:
            # ç¡®ä¿æ•°æ®ç±»å‹ä¸€è‡´ï¼Œé¿å…æµ®ç‚¹æ•°vsæ•´æ•°çš„æ¯”è¾ƒé—®é¢˜
            test_dirty[col] = df_test_csv[col].astype(str).values
    test_dirty.to_csv(os.path.join(data_dir, "test_dirty_extend.csv"), index=False)
    print(f"âœ… å·²ä¿å­˜æ‰©å±•ç‰¹å¾çš„csv: test_dirty_extend.csv")


    # åç»­pipelineç”¨train_extend.csvã€test_dirty_extend.csv
    out_csv = os.path.join(data_dir, "train_extend.csv")

    # æ„é€ è°“è¯å¹¶ä¿å­˜
    pc = PredicateConstructor(out_csv)
    predicates = pc.construct_predicates()
    with open(os.path.join(data_dir, "predicates.txt"), "w", encoding="utf-8") as f:
        for p in predicates:
            f.write(p + "\n")
    print(f"âœ… å·²ä¿å­˜æ‰€æœ‰æ„é€ è°“è¯åˆ°: {os.path.join(data_dir, 'predicates.txt')}")

    # MCTSè§„åˆ™å‘ç°ï¼ˆä»¥æšä¸¾å‹è°“è¯ä¸ºY predicateï¼‰
    mcts_df = pd.read_csv(out_csv)
    with open(os.path.join(data_dir, 'predicates.txt'), 'r', encoding='utf-8') as f:
        mcts_predicates = [line.strip() for line in f if line.strip()]
    # æ”¯æŒåº¦ç­›é€‰ï¼Œå‡å°‘MCTSæœç´¢ç©ºé—´
    support_filter_threshold = 0.05  # é™ä½é˜ˆå€¼ï¼Œåªä¿ç•™æ”¯æŒåº¦å¤§äº0.5%çš„è°“è¯
    max_predicates = 1000  # é™åˆ¶æœ€å¤§è°“è¯æ•°é‡
    filtered_predicates = []
    for p in mcts_predicates:
        mask = predicate_mask(mcts_df, p)
        support = mask.sum() / len(mcts_df) if len(mcts_df) > 0 else 0
        if support >= support_filter_threshold:
            filtered_predicates.append(p)
        if len(filtered_predicates) >= max_predicates:  # è¾¾åˆ°ä¸Šé™å°±åœæ­¢
            break
    print(f'âœ… æ”¯æŒåº¦ç­›é€‰åè°“è¯æ•°: {len(filtered_predicates)} (åŸå§‹: {len(mcts_predicates)})')
    # æšä¸¾å‹è°“è¯ç­›é€‰ï¼ˆå¦‚ t0.col = ... where colä¸ºæšä¸¾å‹ï¼‰
    enum_predicates = [p for p in filtered_predicates if re.search(r'=\s*"', p)]
    print(f'âœ… æšä¸¾å‹è°“è¯æ•°: {len(enum_predicates)}')
    
    # ä¸ºGoodreadsæ•°æ®é›†æ„å»ºä¸“é—¨çš„è°“è¯
    
    # ä¸ºgenreåˆ—æ·»åŠ ä¸“é—¨çš„è°“è¯ï¼ˆå›¾ä¹¦ç±»å‹ï¼‰
    if 'genre' in mcts_df.columns:
        genre_vals = mcts_df['genre'].dropna().unique()
        # åªå–å‰15ä¸ªæœ€å¸¸è§çš„ç±»å‹ï¼Œé¿å…è°“è¯çˆ†ç‚¸
        genre_vals = genre_vals[:15] if len(genre_vals) > 15 else genre_vals
        for val in genre_vals:
            if pd.notna(val) and str(val).strip() != '':
                enum_predicates.append(f'genre = "{val}"')
        print(f'âœ… ä¸ºgenreåˆ—æ·»åŠ äº† {len(genre_vals)} ä¸ªè°“è¯')
        
        # æ·»åŠ genreçš„é”™è¯¯æ¨¡å¼è°“è¯ï¼ˆåªæ·»åŠ æœ€é‡è¦çš„å‡ ä¸ªï¼‰
        important_genres = [
            'genre = "Fiction"',      # æ£€æµ‹å°è¯´ç›¸å…³çš„é”™è¯¯
            'genre = "Non-Fiction"',  # æ£€æµ‹éå°è¯´ç›¸å…³çš„é”™è¯¯
            'genre = "Mystery"',      # æ£€æµ‹æ‚¬ç–‘ç›¸å…³çš„é”™è¯¯
            'genre = "Romance"',      # æ£€æµ‹è¨€æƒ…ç›¸å…³çš„é”™è¯¯
            'genre = "Science Fiction"',  # æ£€æµ‹ç§‘å¹»ç›¸å…³çš„é”™è¯¯
        ]
        for pred in important_genres:
            if pred not in enum_predicates:
                enum_predicates.append(pred)
        print(f'âœ… æ·»åŠ äº†genreé‡è¦é”™è¯¯æ¨¡å¼è°“è¯')
    
    # ä¸ºlanguageåˆ—æ·»åŠ ä¸“é—¨çš„è°“è¯ï¼ˆè¯­è¨€ï¼‰
    if 'language' in mcts_df.columns:
        language_vals = mcts_df['language'].dropna().unique()
        for val in language_vals:
            if pd.notna(val) and str(val).strip() != '':
                enum_predicates.append(f'language = "{val}"')
        print(f'âœ… ä¸ºlanguageåˆ—æ·»åŠ äº† {len(language_vals)} ä¸ªè°“è¯')
        
        # æ·»åŠ languageçš„é”™è¯¯æ¨¡å¼è°“è¯
        important_languages = [
            'language = "English"',   # æ£€æµ‹è‹±è¯­ç›¸å…³çš„é”™è¯¯
            'language = "Spanish"',   # æ£€æµ‹è¥¿ç­ç‰™è¯­ç›¸å…³çš„é”™è¯¯
            'language = "French"',    # æ£€æµ‹æ³•è¯­ç›¸å…³çš„é”™è¯¯
            'language = "German"',    # æ£€æµ‹å¾·è¯­ç›¸å…³çš„é”™è¯¯
        ]
        for pred in important_languages:
            if pred not in enum_predicates:
                enum_predicates.append(pred)
        print(f'âœ… æ·»åŠ äº†languageé‡è¦é”™è¯¯æ¨¡å¼è°“è¯')
    
    # ä¸ºformatåˆ—æ·»åŠ ä¸“é—¨çš„è°“è¯ï¼ˆå›¾ä¹¦æ ¼å¼ï¼‰
    if 'format' in mcts_df.columns:
        format_vals = mcts_df['format'].dropna().unique()
        for val in format_vals:
            if pd.notna(val) and str(val).strip() != '':
                enum_predicates.append(f'format = "{val}"')
        print(f'âœ… ä¸ºformatåˆ—æ·»åŠ äº† {len(format_vals)} ä¸ªè°“è¯')
        
        # æ·»åŠ formatçš„é”™è¯¯æ¨¡å¼è°“è¯
        important_formats = [
            'format = "Paperback"',   # æ£€æµ‹å¹³è£…æœ¬ç›¸å…³çš„é”™è¯¯
            'format = "Hardcover"',   # æ£€æµ‹ç²¾è£…æœ¬ç›¸å…³çš„é”™è¯¯
            'format = "Ebook"',       # æ£€æµ‹ç”µå­ä¹¦ç›¸å…³çš„é”™è¯¯
            'format = "Audiobook"',   # æ£€æµ‹æœ‰å£°ä¹¦ç›¸å…³çš„é”™è¯¯
        ]
        for pred in important_formats:
            if pred not in enum_predicates:
                enum_predicates.append(pred)
        print(f'âœ… æ·»åŠ äº†formaté‡è¦é”™è¯¯æ¨¡å¼è°“è¯')
    
    # ä¸ºpublisheråˆ—æ·»åŠ ä¸“é—¨çš„è°“è¯ï¼ˆå‡ºç‰ˆç¤¾ï¼‰
    if 'publisher' in mcts_df.columns:
        publisher_vals = mcts_df['publisher'].dropna().unique()
        # åªå–å‰10ä¸ªæœ€å¸¸è§çš„å‡ºç‰ˆç¤¾ï¼Œé¿å…è°“è¯çˆ†ç‚¸
        publisher_vals = publisher_vals[:10] if len(publisher_vals) > 10 else publisher_vals
        for val in publisher_vals:
            if pd.notna(val) and str(val).strip() != '':
                enum_predicates.append(f'publisher = "{val}"')
        print(f'âœ… ä¸ºpublisheråˆ—æ·»åŠ äº† {len(publisher_vals)} ä¸ªè°“è¯')
        
        # æ·»åŠ publisherçš„é”™è¯¯æ¨¡å¼è°“è¯
        important_publishers = [
            'publisher = "Penguin"',      # æ£€æµ‹ä¼é¹…å‡ºç‰ˆç¤¾ç›¸å…³çš„é”™è¯¯
            'publisher = "Random House"', # æ£€æµ‹å…°ç™»ä¹¦å±‹ç›¸å…³çš„é”™è¯¯
            'publisher = "HarperCollins"', # æ£€æµ‹å“ˆç€æŸ¯æ—æ–¯ç›¸å…³çš„é”™è¯¯
        ]
        for pred in important_publishers:
            if pred not in enum_predicates:
                enum_predicates.append(pred)
        print(f'âœ… æ·»åŠ äº†publisheré‡è¦é”™è¯¯æ¨¡å¼è°“è¯')
    
    # ä¸ºratingåˆ—æ·»åŠ ä¸“é—¨çš„è°“è¯ï¼ˆè¯„åˆ†ï¼‰
    if 'rating' in mcts_df.columns:
        rating_vals = mcts_df['rating'].dropna().unique()
        for val in rating_vals:
            if pd.notna(val):
                enum_predicates.append(f'rating = "{val}"')
        print(f'âœ… ä¸ºratingåˆ—æ·»åŠ äº† {len(rating_vals)} ä¸ªè°“è¯')
        
        # æ·»åŠ ratingçš„é”™è¯¯æ¨¡å¼è°“è¯
        important_ratings = [
            'rating = "5"',  # æ£€æµ‹5æ˜Ÿè¯„åˆ†ç›¸å…³çš„é”™è¯¯
            'rating = "4"',  # æ£€æµ‹4æ˜Ÿè¯„åˆ†ç›¸å…³çš„é”™è¯¯
            'rating = "3"',  # æ£€æµ‹3æ˜Ÿè¯„åˆ†ç›¸å…³çš„é”™è¯¯
        ]
        for pred in important_ratings:
            if pred not in enum_predicates:
                enum_predicates.append(pred)
        print(f'âœ… æ·»åŠ äº†ratingé‡è¦é”™è¯¯æ¨¡å¼è°“è¯')
    
    # ä¸ºavailabilityåˆ—æ·»åŠ ä¸“é—¨çš„è°“è¯ï¼ˆå¯ç”¨æ€§ï¼‰
    if 'availability' in mcts_df.columns:
        availability_vals = mcts_df['availability'].dropna().unique()
        for val in availability_vals:
            if pd.notna(val):
                enum_predicates.append(f'availability = "{val}"')
        print(f'âœ… ä¸ºavailabilityåˆ—æ·»åŠ äº† {len(availability_vals)} ä¸ªè°“è¯')
        
        # æ·»åŠ availabilityçš„é”™è¯¯æ¨¡å¼è°“è¯
        availability_errors = [
            'availability = "Available"',   # æ£€æµ‹å¯ç”¨ç›¸å…³çš„é”™è¯¯
            'availability = "Out of Stock"', # æ£€æµ‹ç¼ºè´§ç›¸å…³çš„é”™è¯¯
        ]
        for pred in availability_errors:
            if pred not in enum_predicates:
                enum_predicates.append(pred)
        print(f'âœ… æ·»åŠ äº†availabilityé”™è¯¯æ¨¡å¼è°“è¯')
    
    print(f'âœ… æœ€ç»ˆæšä¸¾å‹è°“è¯æ•°: {len(enum_predicates)}')
    # é™åˆ¶æšä¸¾å‹è°“è¯æ•°é‡ï¼Œé¿å…MCTSæœç´¢è¿‡æ…¢
    # if len(enum_predicates) > 50:
    #     enum_predicates = enum_predicates[:50]
    #     print(f'âœ… é™åˆ¶æšä¸¾å‹è°“è¯æ•°ä¸º: {len(enum_predicates)}')
    # ä½¿ç”¨åˆ†å¸ƒå¼MCTSè§„åˆ™å‘ç°
    print(f"ğŸ¯ å¼€å§‹åˆ†å¸ƒå¼MCTSè§„åˆ™å‘ç°...")
    print(f"   æ•°æ®è§„æ¨¡: {len(mcts_df)}è¡Œ, {len(filtered_predicates)}ä¸ªè°“è¯, {len(enum_predicates)}ä¸ªy_pred")
    
    # è®­ç»ƒæˆ–åŠ è½½ç­–ç•¥æ¨¡å‹
    policy_model = None
    model_path = os.path.join(data_dir, 'mcts_policy_model.pkl')
    
    # ç”¨æˆ·å¯ä»¥é€‰æ‹©æ˜¯å¦ä½¿ç”¨ç­–ç•¥æ¨¡å‹
    # å¦‚æœé‡åˆ°å¡ä½é—®é¢˜ï¼Œå¯ä»¥è®¾ç½®ä¸ºFalseå¿«é€Ÿåˆ‡æ¢åˆ°ä¼ ç»ŸMCTS
    use_policy_model = True  # å¯ä»¥é€šè¿‡ç¯å¢ƒå˜é‡æˆ–é…ç½®æ–‡ä»¶æ§åˆ¶
    
    # å¿«é€Ÿä¿®å¤ï¼šå¦‚æœé‡åˆ°åˆ†å¸ƒå¼ç­–ç•¥æ¨¡å‹é—®é¢˜ï¼Œå¯ä»¥è®¾ç½®ä¸ºFalse
    if os.environ.get('DISABLE_POLICY_MODEL', 'false').lower() == 'true':
        use_policy_model = False
        print(f"âš ï¸ æ£€æµ‹åˆ°ç¯å¢ƒå˜é‡DISABLE_POLICY_MODEL=trueï¼Œç¦ç”¨ç­–ç•¥æ¨¡å‹")
    
    if use_policy_model:
        if os.path.exists(model_path):
            print(f"ğŸ“¥ åŠ è½½å·²æœ‰ç­–ç•¥æ¨¡å‹: {model_path}")
            policy_model = ValuePolicyModel()
            policy_model.load_model(model_path)
        else:
            print(f"ğŸ¯ è®­ç»ƒæ–°çš„ç­–ç•¥æ¨¡å‹...")
            policy_model = train_policy_model(
                mcts_df, 
                filtered_predicates, 
                enum_predicates, 
                max_depth=6, 
                n_samples=2000,  # å¢åŠ è®­ç»ƒæ ·æœ¬æ•°é‡
                model_path=model_path
            )
    else:
        print(f"âš ï¸ è·³è¿‡ç­–ç•¥æ¨¡å‹è®­ç»ƒï¼Œä½¿ç”¨ä¼ ç»ŸMCTS")
    
    # ç­–ç•¥1: å…¨å±€è§„åˆ™å‘ç°ï¼ˆä½¿ç”¨ç­–ç•¥æ¨¡å‹ï¼‰
    print(f"ğŸ” ç­–ç•¥1: å…¨å±€è§„åˆ™å‘ç°ï¼ˆä½¿ç”¨ç­–ç•¥æ¨¡å‹ï¼‰...")
    
    # ç»Ÿä¸€ä½¿ç”¨å¤šçº¿ç¨‹æ¨¡å¼ï¼Œç®€åŒ–ç­–ç•¥æ¨¡å‹
    print(f"ğŸ¯ ä½¿ç”¨å¤šçº¿ç¨‹MCTSæ¨¡å¼ï¼Œç®€åŒ–ç­–ç•¥æ¨¡å‹")
    use_parallel_for_policy = dist_config['use_multiprocessing']
    n_workers_for_policy = dist_config['n_workers']
    
    # ç®€åŒ–ç­–ç•¥æ¨¡å‹ä½¿ç”¨ï¼šåªä½¿ç”¨å¯å‘å¼ç­–ç•¥ï¼Œä¸ä½¿ç”¨æœºå™¨å­¦ä¹ æ¨¡å‹
    if use_policy_model and policy_model and policy_model.is_trained:
        use_policy_flag = True
        print(f"âœ… ä½¿ç”¨å¯å‘å¼ç­–ç•¥æ¨¡å‹")
    else:
        use_policy_flag = False
        policy_model = None
        print(f"âš ï¸ ä½¿ç”¨ä¼ ç»Ÿéšæœºç­–ç•¥")
    
    mcts_results = mcts_rule_discovery_yroot(
        mcts_df, 
        filtered_predicates, 
        enum_predicates, 
        max_depth=6,  # è¿›ä¸€æ­¥å¢åŠ æœç´¢æ·±åº¦ä»¥å‘ç°æ›´å¤æ‚çš„è§„åˆ™
        n_iter=10000,  # å¤§å¹…å¢åŠ è¿­ä»£æ¬¡æ•°ä»¥æé«˜å¬å›ç‡
        n_workers=n_workers_for_policy,
        use_parallel=use_parallel_for_policy,
        use_policy=use_policy_flag,  # æ ¹æ®é…ç½®å†³å®šæ˜¯å¦å¯ç”¨ç­–ç•¥æ¨¡å‹
        policy_model=policy_model,
        c_param=1.4,  # MCTSæ¢ç´¢å‚æ•°
        alpha=1.0  # æ¢ç´¢ä¸åˆ©ç”¨å¹³è¡¡å‚æ•°
    )
    
    # ç­–ç•¥2: é’ˆå¯¹ç‰¹å®šåˆ—çš„ä¸“é—¨è§„åˆ™å‘ç°
    print(f"ğŸ” ç­–ç•¥2: é’ˆå¯¹ç‰¹å®šåˆ—çš„ä¸“é—¨è§„åˆ™å‘ç°...")
    target_columns = ['genre', 'language', 'format', 'publisher', 'rating', 'availability']
    specialized_results = []
    
    for target_col in tqdm(target_columns, desc="ä¸“é—¨è§„åˆ™å‘ç°", unit="åˆ—"):
        if target_col in mcts_df.columns:
            print(f"  ğŸ¯ ä¸º{target_col}åˆ—å‘ç°ä¸“é—¨è§„åˆ™...")
            # ä¸ºè¯¥åˆ—åˆ›å»ºä¸“é—¨çš„y_pred
            col_vals = mcts_df[target_col].dropna().unique()
            col_predicates = [f'{target_col} = "{val}"' for val in col_vals if pd.notna(val) and str(val).strip() != '']
            
            if col_predicates:
                # é™åˆ¶è°“è¯æ•°é‡ä»¥é¿å…æœç´¢è¿‡æ…¢
                col_predicates = col_predicates[:15]  # å¢åŠ è°“è¯æ•°é‡ä»¥æ•è·æ›´å¤šæ¨¡å¼
                print(f"    {target_col}åˆ—è°“è¯æ•°: {len(col_predicates)}")
                
                # ä¸ºè¯¥åˆ—è¿›è¡Œä¸“é—¨çš„MCTSæœç´¢ï¼ˆä½¿ç”¨ç­–ç•¥æ¨¡å‹ï¼‰
                col_results = mcts_rule_discovery_yroot(
                    mcts_df,
                    filtered_predicates,
                    col_predicates,
                    max_depth=6,  # è¿›ä¸€æ­¥å¢åŠ æ·±åº¦ä»¥å‘ç°æ›´å¤æ‚çš„è§„åˆ™
                    n_iter=5000,  # å¢åŠ è¿­ä»£æ¬¡æ•°ä»¥æé«˜å‘ç°æ¦‚ç‡
                    n_workers=n_workers_for_policy,  # ä½¿ç”¨ä¸å…¨å±€è§„åˆ™å‘ç°ç›¸åŒçš„è®¾ç½®
                    use_parallel=use_parallel_for_policy,  # ä½¿ç”¨ä¸å…¨å±€è§„åˆ™å‘ç°ç›¸åŒçš„è®¾ç½®
                    use_policy=use_policy_flag,  # æ ¹æ®é…ç½®å†³å®šæ˜¯å¦å¯ç”¨ç­–ç•¥æ¨¡å‹
                    policy_model=policy_model,
                    c_param=1.4,  # MCTSæ¢ç´¢å‚æ•°
                    alpha=1.0  # æ¢ç´¢ä¸åˆ©ç”¨å¹³è¡¡å‚æ•°
                )
                specialized_results.extend(col_results)
                print(f"    âœ… {target_col}åˆ—å‘ç° {len(col_results)} ä¸ªè§„åˆ™")
    
    # åˆå¹¶æ‰€æœ‰ç»“æœ
    all_results = mcts_results + specialized_results
    print(f"âœ… æ€»è§„åˆ™æ•°: {len(all_results)} (å…¨å±€: {len(mcts_results)}, ä¸“é—¨: {len(specialized_results)})")
    # é˜ˆå€¼è®¾ç½® - é€‚åº¦ä¼˜åŒ–ä»¥æé«˜å¬å›ç‡
    support_threshold = 0.2  # é€‚åº¦é™ä½æ”¯æŒåº¦é˜ˆå€¼ä»¥å‘ç°æ›´å¤šè§„åˆ™
    confidence_threshold = 0.65  # é€‚åº¦é™ä½ç½®ä¿¡åº¦é˜ˆå€¼ä»¥æé«˜å¬å›ç‡
    # ä¿å­˜ä¸ºç»“æ„åŒ–csv
    rules_data = []
    for y_pred, rule, support, confidence in all_results:
        if support >= support_threshold and confidence >= confidence_threshold:
            # é¢å¤–çš„è´¨é‡è¿‡æ»¤ï¼šé¿å…è¿‡äºå®½æ³›çš„è§„åˆ™
            rule_complexity = len(rule) - 1  # å‰ææ¡ä»¶çš„æ•°é‡
            if rule_complexity >= 1:  # è‡³å°‘éœ€è¦1ä¸ªå‰ææ¡ä»¶ï¼ˆæ”¾å®½è¦æ±‚ï¼‰
                rules_data.append({
                    'y_pred': y_pred,
                    'best_rule': ' ^ '.join(rule[1:]),
                    'support': support,
                    'confidence': confidence
                })
    
    print(f'ğŸ” è´¨é‡è¿‡æ»¤åä¿ç•™ {len(rules_data)} ä¸ªè§„åˆ™ (æ€»å‘ç° {len(all_results)} ä¸ª)')
    rules_df = pd.DataFrame(rules_data)
    if not rules_df.empty:
        rules_df.to_csv(os.path.join(data_dir, 'dcr_mcts_rule.csv'), index=False)
        print(f'âœ… å‘ç° {len(rules_df)} ä¸ªæœ‰æ•ˆè§„åˆ™')
        print(f'ğŸ“Š è§„åˆ™åˆ†å¸ƒ:')
        for col in ['genre', 'language', 'format', 'publisher', 'rating', 'availability']:
            col_rules = rules_df[rules_df['y_pred'].str.contains(col, na=False)]
            print(f'  {col}: {len(col_rules)} ä¸ªè§„åˆ™')
    else:
        rules_df = pd.DataFrame(columns=['y_pred', 'best_rule', 'support', 'confidence'])
        rules_df.to_csv(os.path.join(data_dir, 'dcr_mcts_rule.csv'), index=False)
        print('âš ï¸ æœªå‘ç°ä»»ä½•æœ‰æ•ˆè§„åˆ™')
    print(f'âœ… å·²ä¿å­˜ç»“æ„åŒ–è§„åˆ™è¡¨åˆ°: {os.path.join(data_dir, "dcr_mcts_rule.csv")}')

    # è§„åˆ™æŸ¥é”™ï¼šè¾“å‡ºerror_cell (è¡Œå·, åˆ—å)
    rules_df = pd.read_csv(os.path.join(data_dir, 'dcr_mcts_rule.csv'))
    test_dirty = pd.read_csv(os.path.join(data_dir, 'test_dirty_extend.csv'))
    test_clean = pd.read_csv(os.path.join(data_dir, 'test_extend.csv'))

    results = []
    def extract_col_from_predicate(pred):
        import re
        m = re.match(r'(\w+)\s*[=!<>]+\s*.+', pred)
        if m:
            return m.group(1)
        return None
    # æ’é™¤ä¸åº”è¯¥æœ‰é”™è¯¯çš„åˆ—
    exclude_cols = [col for col in test_dirty.columns if "embed_cluster" in col or "img_category" in col]
    
    for idx, row in tqdm(rules_df.iterrows(), total=len(rules_df), desc="è§„åˆ™æŸ¥é”™", unit="è§„åˆ™"):
        y_pred = row['y_pred']
        best_rule = row['best_rule']
        # è§£æå‰æè°“è¯
        premise_preds = [p.strip() for p in best_rule.split('^') if p.strip()]
        # ç»“è®ºè°“è¯
        conclusion_pred = y_pred
        conclusion_col = extract_col_from_predicate(conclusion_pred)
        if conclusion_col is None or conclusion_col in exclude_cols:
            continue  # è·³è¿‡æ— æ³•æå–åˆ—åçš„è§„åˆ™æˆ–ä¸åº”è¯¥æœ‰é”™è¯¯çš„åˆ—
        # å‰æmask
        mask = np.ones(len(test_dirty), dtype=bool)
        for pred in premise_preds:
            mask = mask & predicate_mask(test_dirty, pred)
        # ç»“è®ºmask
        mask_conclusion = predicate_mask(test_dirty, conclusion_pred)
        # æŸ¥é”™ï¼šå‰ææˆç«‹ä½†ç»“è®ºä¸æˆç«‹çš„æ ·æœ¬
        error_mask = mask & (~mask_conclusion)
        
        # è¿‡æ»¤æ‰NaNå€¼å’Œè¯¯æŠ¥
        for i in range(len(test_dirty)):
            if error_mask[i]:
                clean_val = test_clean.iloc[i][conclusion_col] if i < len(test_clean) else None
                dirty_val = test_dirty.iloc[i][conclusion_col]
                # å¦‚æœä¸¤ä¸ªå€¼éƒ½æ˜¯NaNï¼Œä¸ç®—é”™è¯¯
                if pd.isna(clean_val) and pd.isna(dirty_val):
                    error_mask[i] = False
                # å¦‚æœä¸¤ä¸ªå€¼ç›¸åŒï¼Œä¸ç®—é”™è¯¯ï¼ˆé¿å…è¯¯æŠ¥ï¼‰
                elif not pd.isna(clean_val) and not pd.isna(dirty_val):
                    if str(clean_val).strip() == str(dirty_val).strip():
                        error_mask[i] = False
        
        # ä½¿ç”¨ä½ç½®ç´¢å¼•è€Œä¸æ˜¯DataFrameç´¢å¼•ï¼Œç¡®ä¿ä¸€è‡´æ€§
        error_positions = [i for i, is_error in enumerate(error_mask) if is_error]
        # è¾“å‡ºerror_cell
        error_cells = [(i, conclusion_col) for i in error_positions]
        if error_cells:
            results.append({
                'rule_id': idx,
                'y_pred': y_pred,
                'best_rule': best_rule,
                'error_cell': json.dumps(error_cells),
                'error_count': len(error_cells)
            })
    error_df = pd.DataFrame(results)
    if not error_df.empty:
        error_df.to_csv(os.path.join(data_dir, 'dcr_rule_error_detect.csv'), index=False)
    else:
        # æ˜ç¡®æŒ‡å®šåˆ—åï¼Œå†™å…¥è¡¨å¤´
        error_df = pd.DataFrame(columns=['rule_id', 'y_pred', 'best_rule', 'error_cell', 'error_count'])
        error_df.to_csv(os.path.join(data_dir, 'dcr_rule_error_detect.csv'), index=False)
    print('âœ… å·²ä¿å­˜è§„åˆ™æŸ¥é”™ç»“æœåˆ° dcr_rule_error_detect.csv')

    # è§„åˆ™æŸ¥é”™è¯„ä¼°ï¼šä¸test_clean.csvå¯¹æ¯”ï¼Œè®¡ç®—F1/recall/precision/accuracy
    import ast
    error_file = os.path.join(data_dir, 'dcr_rule_error_detect.csv')
    # åˆ¤æ–­æ–‡ä»¶æ˜¯å¦ä¸ºç©ºæˆ–æ— åˆ—
    if os.path.getsize(error_file) == 0 or pd.read_csv(error_file).shape[1] == 0:
        print("âš ï¸ æŸ¥é”™ç»“æœæ–‡ä»¶ä¸ºç©ºæˆ–æ— åˆ—ï¼Œè·³è¿‡è¯„ä¼°ã€‚")
        with open(os.path.join(data_dir, 'dcr_rule_error_metrics.txt'), 'w', encoding='utf-8') as f:
            f.write('Precision: 0.0000\nRecall: 0.0000\nF1: 0.0000\nAccuracy: 0.0000\n')
        return

    test_clean = pd.read_csv(os.path.join(data_dir, 'test_extend.csv'))    
    test_dirty = pd.read_csv(os.path.join(data_dir, 'test_dirty_extend.csv'))
    
    # æ‰“å°æ•°æ®æ¡†å¤§å°ä¿¡æ¯
    print(f"ğŸ“Š æ•°æ®æ¡†å¤§å°ä¿¡æ¯:")
    print(f"   test_clean.shape: {test_clean.shape}")
    print(f"   test_dirty.shape: {test_dirty.shape}")
    print(f"   test_clean.columns: {list(test_clean.columns)}")
    print(f"   test_dirty.columns: {list(test_dirty.columns)}") 
    
    # é¢„æµ‹ä¸ºæ­£çš„cellé›†åˆ
    pred_cells = set()
    for cells in error_df['error_cell']:
        for cell in json.loads(cells):
            row_idx, col_name = cell
            # éªŒè¯ç´¢å¼•åœ¨æœ‰æ•ˆèŒƒå›´å†…
            if row_idx >= len(test_clean):
                print(f"âš ï¸ è­¦å‘Šï¼šé¢„æµ‹é”™è¯¯ç´¢å¼•è¶…å‡ºèŒƒå›´: è¡Œ{row_idx}, åˆ—{col_name} (test_clean.shape={test_clean.shape})")
                continue
            if col_name not in test_clean.columns:
                print(f"âš ï¸ è­¦å‘Šï¼šé¢„æµ‹é”™è¯¯åˆ—ä¸å­˜åœ¨: è¡Œ{row_idx}, åˆ—{col_name}")
                continue
            pred_cells.add(tuple(cell))
    
    print(f"ğŸ“Š æœ‰æ•ˆé¢„æµ‹é”™è¯¯æ•°: {len(pred_cells)}")
    
    # å®é™…ä¸ºæ­£çš„cellé›†åˆ
    real_cells = set()
    # æ’é™¤ä¸åº”è¯¥æœ‰é”™è¯¯çš„åˆ—ï¼ˆembeddingèšç±»åˆ—å’Œå›¾ç‰‡ç±»åˆ«åˆ—ï¼‰
    exclude_cols = [col for col in test_clean.columns if "embed_cluster" in col or "img_category" in col]
    print(f"ğŸ” æ’é™¤çš„åˆ—ï¼ˆä¸åº”è¯¥æœ‰é”™è¯¯ï¼‰: {exclude_cols}")
    
    print(f"ğŸ” æ£€æµ‹å®é™…é”™è¯¯...")
    for i in tqdm(range(len(test_clean)), desc="æ£€æµ‹å®é™…é”™è¯¯", unit="è¡Œ"):
        for col in test_clean.columns:
            # è·³è¿‡ä¸åº”è¯¥æœ‰é”™è¯¯çš„åˆ—
            if col in exclude_cols:
                continue
            clean_val = test_clean.at[i, col]
            dirty_val = test_dirty.at[i, col]
            # æ­£ç¡®å¤„ç†NaNå€¼æ¯”è¾ƒ
            if pd.isna(clean_val) and pd.isna(dirty_val):
                continue  # ä¸¤ä¸ªéƒ½æ˜¯NaNï¼Œä¸ç®—é”™è¯¯
            elif pd.isna(clean_val) or pd.isna(dirty_val):
                real_cells.add((i, col))  # ä¸€ä¸ªNaNä¸€ä¸ªéNaNï¼Œç®—é”™è¯¯
            elif str(clean_val).strip() != str(dirty_val).strip():
                real_cells.add((i, col))  # å­—ç¬¦ä¸²æ¯”è¾ƒï¼Œå»é™¤ç©ºæ ¼
    
    # è®¡ç®—æŒ‡æ ‡
    TP = len(pred_cells & real_cells)
    FP = len(pred_cells - real_cells)
    FN = len(real_cells - pred_cells)
    total_cells = test_clean.shape[0] * test_clean.shape[1]
    TN = total_cells - TP - FP - FN
    precision = TP / (TP + FP) if (TP + FP) > 0 else 0
    recall = TP / (TP + FN) if (TP + FN) > 0 else 0
    f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0
    accuracy = (TP + TN) / total_cells
    
    print(f'Precision: {precision:.4f}')
    print(f'Recall: {recall:.4f}')
    print(f'F1: {f1:.4f}')
    print(f'Accuracy: {accuracy:.4f}')
    
    # ========== é”™è¯¯ç»Ÿè®¡åˆ†æ ==========
    print(f"\nğŸ“Š é”™è¯¯ç»Ÿè®¡åˆ†æ:")
    print(f"æ€»å•å…ƒæ ¼æ•°: {total_cells}")
    print(f"å®é™…é”™è¯¯æ•°: {len(real_cells)}")
    print(f"é¢„æµ‹é”™è¯¯æ•°: {len(pred_cells)}")
    print(f"çœŸé˜³æ€§(TP): {TP}")
    print(f"å‡é˜³æ€§(FP): {FP}")
    print(f"å‡é˜´æ€§(FN): {FN}")
    print(f"çœŸé˜´æ€§(TN): {TN}")
    
    # åˆ†ææŸ¥åˆ°çš„é”™è¯¯ï¼ˆTPï¼‰
    if TP > 0:
        print(f"\nâœ… æŸ¥åˆ°çš„é”™è¯¯ (TP={TP}):")
        tp_cells = list(pred_cells & real_cells)
        tp_cells.sort()
        for i, (row_idx, col_name) in enumerate(tp_cells[:10]):  # åªæ˜¾ç¤ºå‰10ä¸ª
            # æ£€æŸ¥ç´¢å¼•æ˜¯å¦åœ¨æœ‰æ•ˆèŒƒå›´å†…
            if row_idx < len(test_clean) and col_name in test_clean.columns:
                clean_val = test_clean.at[row_idx, col_name]
                dirty_val = test_dirty.at[row_idx, col_name]
                # æ­£ç¡®å¤„ç†NaNå€¼æ˜¾ç¤º
                clean_str = str(clean_val) if not pd.isna(clean_val) else 'NaN'
                dirty_str = str(dirty_val) if not pd.isna(dirty_val) else 'NaN'
                print(f"  {i+1}. è¡Œ{row_idx}, åˆ—{col_name}: '{clean_str}' -> '{dirty_str}'")
            else:
                print(f"  {i+1}. è¡Œ{row_idx}, åˆ—{col_name}: ç´¢å¼•è¶…å‡ºèŒƒå›´ (test_clean.shape={test_clean.shape})")
        if len(tp_cells) > 10:
            print(f"  ... è¿˜æœ‰ {len(tp_cells)-10} ä¸ªæŸ¥åˆ°çš„é”™è¯¯")
    
    # åˆ†ææœªæŸ¥åˆ°çš„é”™è¯¯ï¼ˆFNï¼‰
    if FN > 0:
        print(f"\nâŒ æœªæŸ¥åˆ°çš„é”™è¯¯ (FN={FN}):")
        fn_cells = list(real_cells - pred_cells)
        fn_cells.sort()
        for i, (row_idx, col_name) in enumerate(fn_cells[:10]):  # åªæ˜¾ç¤ºå‰10ä¸ª
            # æ£€æŸ¥ç´¢å¼•æ˜¯å¦åœ¨æœ‰æ•ˆèŒƒå›´å†…
            if row_idx < len(test_clean) and col_name in test_clean.columns:
                clean_val = test_clean.at[row_idx, col_name]
                dirty_val = test_dirty.at[row_idx, col_name]
                # æ­£ç¡®å¤„ç†NaNå€¼æ˜¾ç¤º
                clean_str = str(clean_val) if not pd.isna(clean_val) else 'NaN'
                dirty_str = str(dirty_val) if not pd.isna(dirty_val) else 'NaN'
                print(f"  {i+1}. è¡Œ{row_idx}, åˆ—{col_name}: '{clean_str}' -> '{dirty_str}'")
            else:
                print(f"  {i+1}. è¡Œ{row_idx}, åˆ—{col_name}: ç´¢å¼•è¶…å‡ºèŒƒå›´ (test_clean.shape={test_clean.shape})")
        if len(fn_cells) > 10:
            print(f"  ... è¿˜æœ‰ {len(fn_cells)-10} ä¸ªæœªæŸ¥åˆ°çš„é”™è¯¯")
    
    # åˆ†æè¯¯æŠ¥çš„é”™è¯¯ï¼ˆFPï¼‰
    if FP > 0:
        print(f"\nâš ï¸ è¯¯æŠ¥çš„é”™è¯¯ (FP={FP}):")
        fp_cells = list(pred_cells - real_cells)
        fp_cells.sort()
        
        # å…ˆæ˜¾ç¤ºæ‰€æœ‰è§„åˆ™ï¼Œå¸®åŠ©ç†è§£è¯¯æŠ¥åŸå› 
        print(f"ğŸ” å½“å‰æ‰€æœ‰è§„åˆ™:")
        print(f"error_df columns: {error_df.columns.tolist()}")
        print(f"rules_df columns: {rules_df.columns.tolist()}")
        for idx, row in error_df.iterrows():
            rule_id = row['rule_id']
            rule_info = f"  Rule_{rule_id}: {row['y_pred']} <- {row['best_rule']}"
            # ä»rules_dfæŸ¥support/confidence
            if rule_id < len(rules_df):
                rule_row = rules_df.iloc[rule_id]
                rule_info += f" (support={rule_row['support']:.3f}, confidence={rule_row['confidence']:.3f})"
            print(rule_info)

        # åˆ†æè¯¯æŠ¥çš„è§„åˆ™æ¥æº
        print(f"\nğŸ” è¯¯æŠ¥åˆ†æ - æ£€æŸ¥å“ªäº›è§„åˆ™å¯¼è‡´äº†è¯¯æŠ¥:")
        fp_rule_counts = {}
        
        # ä¸ºæ¯ä¸ªè¯¯æŠ¥cellæ‰¾åˆ°å¯¹åº”çš„è§„åˆ™
        for fp_cell in fp_cells:
            cell_found = False
            for idx, row in error_df.iterrows():
                rule_cells = json.loads(row['error_cell'])
                if fp_cell in rule_cells:
                    rule_info = f"Rule_{row['rule_id']}: {row['y_pred']} <- {row['best_rule']}"
                    fp_rule_counts[rule_info] = fp_rule_counts.get(rule_info, 0) + 1
                    cell_found = True
                    break  # æ‰¾åˆ°ç¬¬ä¸€ä¸ªåŒ¹é…çš„è§„åˆ™å°±åœæ­¢
        
        # æ˜¾ç¤ºå¯¼è‡´è¯¯æŠ¥æœ€å¤šçš„è§„åˆ™
        if fp_rule_counts:
            sorted_rules = sorted(fp_rule_counts.items(), key=lambda x: x[1], reverse=True)
            print(f"ğŸ“‹ å¯¼è‡´è¯¯æŠ¥æœ€å¤šçš„è§„åˆ™:")
            for rule_info, count in sorted_rules[:5]:
                print(f"  {rule_info}: {count}ä¸ªè¯¯æŠ¥")
            
            # æ˜¾ç¤ºè¯¯æŠ¥åˆ†å¸ƒç»Ÿè®¡
            print(f"\nğŸ“Š è¯¯æŠ¥åˆ†å¸ƒç»Ÿè®¡:")
            total_fp = len(fp_cells)
            covered_fp = sum(fp_rule_counts.values())
            print(f"  æ€»è¯¯æŠ¥æ•°: {total_fp}")
            print(f"  è¢«è§„åˆ™è¦†ç›–çš„è¯¯æŠ¥æ•°: {covered_fp}")
            print(f"  æœªæ‰¾åˆ°æ¥æºçš„è¯¯æŠ¥æ•°: {total_fp - covered_fp}")
        else:
            print("  âš ï¸ æ— æ³•ç¡®å®šè¯¯æŠ¥æ¥æº")
            
        # æ˜¾ç¤ºæ¯ä¸ªè¯¯æŠ¥cellå¯¹åº”çš„è§„åˆ™
        print(f"\nğŸ” è¯¯æŠ¥cellä¸è§„åˆ™å¯¹åº”å…³ç³»:")
        for i, fp_cell in enumerate(fp_cells[:10]):  # åªæ˜¾ç¤ºå‰10ä¸ª
            cell_found = False
            for idx, row in error_df.iterrows():
                rule_cells = json.loads(row['error_cell'])
                if fp_cell in rule_cells:
                    rule_info = f"Rule_{row['rule_id']}: {row['y_pred']}"
                    print(f"  {i+1}. è¡Œ{fp_cell[0]}, åˆ—{fp_cell[1]} -> {rule_info}")
                    cell_found = True
                    break
            if not cell_found:
                print(f"  {i+1}. è¡Œ{fp_cell[0]}, åˆ—{fp_cell[1]} -> æœªæ‰¾åˆ°å¯¹åº”è§„åˆ™")
        if len(fp_cells) > 10:
            print(f"  ... è¿˜æœ‰ {len(fp_cells)-10} ä¸ªè¯¯æŠ¥cell")
        
        # æ˜¾ç¤ºè¯¯æŠ¥è¯¦æƒ…
        print(f"\nğŸ“‹ è¯¯æŠ¥è¯¦æƒ…:")
        for i, (row_idx, col_name) in enumerate(fp_cells[:10]):  # åªæ˜¾ç¤ºå‰10ä¸ª
            # æ£€æŸ¥ç´¢å¼•æ˜¯å¦åœ¨æœ‰æ•ˆèŒƒå›´å†…
            if row_idx < len(test_clean) and col_name in test_clean.columns:
                clean_val = test_clean.at[row_idx, col_name]
                dirty_val = test_dirty.at[row_idx, col_name]
                # æ­£ç¡®å¤„ç†NaNå€¼æ˜¾ç¤º
                clean_str = str(clean_val) if not pd.isna(clean_val) else 'NaN'
                dirty_str = str(dirty_val) if not pd.isna(dirty_val) else 'NaN'
                print(f"  {i+1}. è¡Œ{row_idx}, åˆ—{col_name}: '{clean_str}' == '{dirty_str}' (å®é™…æ— é”™è¯¯)")
                
                # åˆ†æè¯¯æŠ¥åŸå› ï¼šæ£€æŸ¥è¯¥è¡Œæ˜¯å¦æ»¡è¶³ä»»ä½•è§„åˆ™çš„å‰ææ¡ä»¶
                print(f"     ğŸ” è¯¯æŠ¥åŸå› åˆ†æ:")
                for idx, row in error_df.iterrows():
                    rule_id = row['rule_id']
                    y_pred = row['y_pred']
                    best_rule = row['best_rule']
                    
                    # æ£€æŸ¥è¿™ä¸ªè¯¯æŠ¥æ˜¯å¦ç”±è¿™ä¸ªè§„åˆ™å¼•èµ·
                    rule_cells = json.loads(row['error_cell'])
                    if (row_idx, col_name) in rule_cells:
                        print(f"       - ç”±Rule_{rule_id}å¼•èµ·: {y_pred} <- {best_rule}")
                        
                        # åˆ†æè§„åˆ™å‰ææ¡ä»¶
                        if best_rule:
                            premises = best_rule.split(' ^ ')
                            print(f"         å‰ææ¡ä»¶:")
                            for premise in premises:
                                premise = premise.strip()
                                if premise:
                                    # æ£€æŸ¥è¯¥è¡Œæ˜¯å¦æ»¡è¶³è¿™ä¸ªå‰ææ¡ä»¶
                                    try:
                                        mask = predicate_mask(test_clean, premise)
                                        if mask.iloc[row_idx]:
                                            print(f"           âœ“ {premise} (æ»¡è¶³)")
                                        else:
                                            print(f"           âœ— {premise} (ä¸æ»¡è¶³)")
                                    except:
                                        print(f"           ? {premise} (æ— æ³•è¯„ä¼°)")
                        break
            else:
                print(f"  {i+1}. è¡Œ{row_idx}, åˆ—{col_name}: ç´¢å¼•è¶…å‡ºèŒƒå›´ (test_clean.shape={test_clean.shape})")
        if len(fp_cells) > 10:
            print(f"  ... è¿˜æœ‰ {len(fp_cells)-10} ä¸ªè¯¯æŠ¥")
    
    # æŒ‰åˆ—ç»Ÿè®¡é”™è¯¯åˆ†å¸ƒ
    print(f"\nğŸ“ˆ æŒ‰åˆ—ç»Ÿè®¡é”™è¯¯åˆ†å¸ƒ:")
    col_error_stats = {}
    for row_idx, col_name in real_cells:
        if col_name not in col_error_stats:
            col_error_stats[col_name] = {'total': 0, 'detected': 0, 'missed': 0}
        col_error_stats[col_name]['total'] += 1
        if (row_idx, col_name) in pred_cells:
            col_error_stats[col_name]['detected'] += 1
        else:
            col_error_stats[col_name]['missed'] += 1
    
    # æŒ‰æ£€æµ‹ç‡æ’åº
    sorted_cols = sorted(col_error_stats.items(), 
                        key=lambda x: x[1]['detected']/x[1]['total'] if x[1]['total'] > 0 else 0, 
                        reverse=True)
    
    for col_name, stats in sorted_cols[:10]:  # æ˜¾ç¤ºå‰10åˆ—
        detection_rate = stats['detected'] / stats['total'] if stats['total'] > 0 else 0
        print(f"  {col_name}: {stats['detected']}/{stats['total']} ({detection_rate:.2%})")
    
    # è§„åˆ™è´¨é‡è¯„ä¼°
    print(f"\nğŸ” è§„åˆ™è´¨é‡è¯„ä¼°:")
    for idx, row in error_df.iterrows():
        rule_id = row['rule_id']
        y_pred = row['y_pred']
        best_rule = row['best_rule']
        error_cells = json.loads(row['error_cell'])
        
        # å°†error_cellsè½¬æ¢ä¸ºå…ƒç»„é›†åˆï¼Œä½¿å…¶å¯å“ˆå¸Œ
        error_cells_set = set(tuple(cell) for cell in error_cells)
        
        # è®¡ç®—è¯¥è§„åˆ™çš„ç²¾ç¡®ç‡
        rule_tp = len(error_cells_set & real_cells)
        rule_fp = len(error_cells_set - real_cells)
        rule_precision = rule_tp / (rule_tp + rule_fp) if (rule_tp + rule_fp) > 0 else 0
        
        # è®¡ç®—è¯¥è§„åˆ™çš„å¬å›ç‡
        rule_fn = len(real_cells - error_cells_set)
        rule_recall = rule_tp / (rule_tp + rule_fn) if (rule_tp + rule_fn) > 0 else 0
        
        print(f"  Rule_{rule_id}: {y_pred} <- {best_rule}")
        print(f"    ç²¾ç¡®ç‡: {rule_precision:.3f} (TP={rule_tp}, FP={rule_fp})")
        print(f"    å¬å›ç‡: {rule_recall:.3f} (TP={rule_tp}, FN={rule_fn})")
        print(f"    è¦†ç›–å•å…ƒæ ¼æ•°: {len(error_cells)}")
        print()
    
    with open(os.path.join(data_dir, 'dcr_rule_error_metrics.txt'), 'w', encoding='utf-8') as f:
        f.write(f'Precision: {precision:.4f}\n')
        f.write(f'Recall: {recall:.4f}\n')
        f.write(f'F1: {f1:.4f}\n')
        f.write(f'Accuracy: {accuracy:.4f}\n')
        f.write(f'\né”™è¯¯ç»Ÿè®¡:\n')
        f.write(f'æ€»å•å…ƒæ ¼æ•°: {total_cells}\n')
        f.write(f'å®é™…é”™è¯¯æ•°: {len(real_cells)}\n')
        f.write(f'é¢„æµ‹é”™è¯¯æ•°: {len(pred_cells)}\n')
        f.write(f'çœŸé˜³æ€§(TP): {TP}\n')
        f.write(f'å‡é˜³æ€§(FP): {FP}\n')
        f.write(f'å‡é˜´æ€§(FN): {FN}\n')
        f.write(f'çœŸé˜´æ€§(TN): {TN}\n')
    print('âœ… å·²ä¿å­˜æŸ¥é”™è¯„ä¼°æŒ‡æ ‡åˆ° dcr_rule_error_metrics.txt')
    
    # å¦‚æœå¯ç”¨å®éªŒæ¨¡å¼ï¼Œè¿è¡Œå‚æ•°å®éªŒ
    if run_experiment_mode:
        print(f"\n{'='*80}")
        print(f"ğŸ”¬ å¼€å§‹å‚æ•°æ•æ„Ÿæ€§å®éªŒ")
        print(f"{'='*80}")
        
        # è¿è¡Œå‚æ•°å®éªŒ
        experiment_results = run_parameter_experiments(
            mcts_df, filtered_predicates, enum_predicates, dist_config, data_dir, use_policy_flag, policy_model
        )
        
        print(f"\nâœ… å‚æ•°å®éªŒå®Œæˆï¼")
        print(f"ğŸ“Š å®éªŒç»“æœå·²ä¿å­˜åˆ°: {os.path.join(data_dir, 'parameter_experiments_summary.csv')}")
        print(f"ğŸ” è¯¦ç»†åˆ†æè¯·æŸ¥çœ‹æ§åˆ¶å°è¾“å‡ºå’Œå„ä¸ªå®éªŒæ–‡ä»¶")


if __name__ == '__main__':
    main() 